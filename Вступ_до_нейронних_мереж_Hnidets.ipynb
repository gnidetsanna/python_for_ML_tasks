{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnidetsanna/python_for_ML_tasks/blob/main/%D0%92%D1%81%D1%82%D1%83%D0%BF_%D0%B4%D0%BE_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B8%D1%85_%D0%BC%D0%B5%D1%80%D0%B5%D0%B6_Hnidets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_tensor = torch.from_numpy(inputs)\n",
        "targets_tensor = torch.from_numpy(targets)\n",
        "\n",
        "# Вивід результату на екран\n",
        "print(\"Вхідні дані як тензор:\")\n",
        "print(inputs_tensor)\n",
        "\n",
        "print(\"\\nТаргети як тензор:\")\n",
        "print(targets_tensor)"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63151019-d9ec-4d14-e231-c2513265bfe4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вхідні дані як тензор:\n",
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "\n",
            "Таргети як тензор:\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ee7575-8a2c-4b8a-d232-b08268d0c469"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f47756c71b0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 3\n",
        "\n",
        "\n",
        "\n",
        "w = torch.randn(1, 3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "\n",
        "print(\"Ваги w:\")\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78824333-9eff-45bf-e438-4a70e403b336"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ваги w:\n",
            "tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "tensor([0.6213], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x, w, b):\n",
        "    q = x @ w.t() + b\n",
        "    print(q)\n",
        "    return 1 / (1 + torch.exp(-q))\n",
        "\n",
        "\n",
        "predictions = model(inputs_tensor, w, b)\n",
        "print(\"Прогнози ймовірностей:\")\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c87100-159f-4af6-c5d6-6dda9377d07a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[69.4361],\n",
            "        [88.2410],\n",
            "        [97.5041],\n",
            "        [81.8390],\n",
            "        [76.1967]], grad_fn=<AddBackward0>)\n",
            "Прогнози ймовірностей:\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Висновок\n",
        "\n",
        "Значення близькі до 1 це може вказувати на проблеми з початковою ініціалізацією ваг або неправильним масштабуванням даних."
      ],
      "metadata": {
        "id": "H-qJd8sNOj8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "    predicted_probs = torch.clamp(predicted_probs, min=1e-5, max=1 - 1e-5)\n",
        "    loss = - (true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
        "\n",
        "    return torch.mean(loss)\n",
        "print(\"Середня втрата (binary cross-entropy):\", loss.item())"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c873dd03-dba5-437c-b0f3-57139a53d780"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Середня втрата (binary cross-entropy): 0.2602730393409729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss = binary_cross_entropy(predictions, targets_tensor)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5wPOWNUTXAc",
        "outputId": "c4806107-2f65-4aeb-bb24-8cb99d00323a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.6046, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(w.grad)\n",
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewg0_FCCTtYz",
        "outputId": "a4c370e9-db30-49f6-c4a0-d431835ef320"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "tensor([[0., 0., 0.]])\n",
            "tensor([0.6213], requires_grad=True)\n",
            "tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Висновок\n",
        "\n",
        "Усі градієнти для ваг і зміщення є нульовими, що може свідчити про одну з наступних проблем:\n",
        "\n",
        "\n",
        "1) Насичення сигмоїдної функції.\n",
        "\n",
        "\n",
        "2)  Модель може знаходитись у точці плато, де похідні (градієнти) дуже малі або дорівнюють нулю, що означає, що параметри не змінюються, і модель не покращується.\n",
        "\n",
        "\n",
        "3) Неправильне масштабування вхідних даних"
      ],
      "metadata": {
        "id": "GUQoSiazUQGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(inputs_tensor, w, b)\n",
        "print(predictions)\n",
        "\n",
        "loss = binary_cross_entropy(predictions, targets_tensor)\n",
        "print(loss)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print(w)\n",
        "print(w.grad)\n",
        "print(b)\n",
        "print(b.grad)\n",
        ""
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f49c5e9-1eca-49e3-a5b4-ff8a648e2355"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0694],\n",
            "        [0.0882],\n",
            "        [0.0975],\n",
            "        [0.0818],\n",
            "        [0.0762]], grad_fn=<AddBackward0>)\n",
            "tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n",
            "tensor(0.6829, grad_fn=<MeanBackward0>)\n",
            "tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True)\n",
            "tensor([[ -5.4417, -18.9853, -10.0682]])\n",
            "tensor([0.0006], requires_grad=True)\n",
            "tensor([-0.0794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(model, inputs, targets, learning_rate, epochs):\n",
        "  w = torch.randn(1, 3, requires_grad=True)\n",
        "  b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "  w.data = w.data / 1000\n",
        "  b.data = b.data / 1000\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    loss = binary_cross_entropy(model(inputs, w, b), targets)\n",
        "    loss.backward()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "      w.data -= learning_rate * w.grad\n",
        "      b.data -= learning_rate * b.grad\n",
        "      w.grad.zero_()\n",
        "      b.grad.zero_()\n",
        "\n",
        "  return model(inputs, w, b)\n",
        "\n",
        "final_preds = gradient_descent(model, inputs_tensor, targets_tensor, 0.001, 1000)"
      ],
      "metadata": {
        "id": "mObHPyE06qsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b30aebaa-5bb5-4dfe-b945-3e5df4d0f983"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        [-7.1031],\n",
            "        [ 5.2806]], grad_fn=<AddBackward0>)\n",
            "Epoch 168/1000, Loss: 0.1723877191543579\n",
            "tensor([[-0.4168],\n",
            "        [ 0.9076],\n",
            "        [ 4.6801],\n",
            "        [-7.1203],\n",
            "        [ 5.2933]], grad_fn=<AddBackward0>)\n",
            "Epoch 169/1000, Loss: 0.1720665544271469\n",
            "tensor([[-0.4193],\n",
            "        [ 0.9093],\n",
            "        [ 4.6839],\n",
            "        [-7.1374],\n",
            "        [ 5.3061]], grad_fn=<AddBackward0>)\n",
            "Epoch 170/1000, Loss: 0.17174668610095978\n",
            "tensor([[-0.4218],\n",
            "        [ 0.9111],\n",
            "        [ 4.6878],\n",
            "        [-7.1544],\n",
            "        [ 5.3188]], grad_fn=<AddBackward0>)\n",
            "Epoch 171/1000, Loss: 0.17142809927463531\n",
            "tensor([[-0.4242],\n",
            "        [ 0.9128],\n",
            "        [ 4.6916],\n",
            "        [-7.1715],\n",
            "        [ 5.3315]], grad_fn=<AddBackward0>)\n",
            "Epoch 172/1000, Loss: 0.1711108386516571\n",
            "tensor([[-0.4267],\n",
            "        [ 0.9146],\n",
            "        [ 4.6955],\n",
            "        [-7.1884],\n",
            "        [ 5.3442]], grad_fn=<AddBackward0>)\n",
            "Epoch 173/1000, Loss: 0.17079493403434753\n",
            "tensor([[-0.4291],\n",
            "        [ 0.9163],\n",
            "        [ 4.6993],\n",
            "        [-7.2053],\n",
            "        [ 5.3568]], grad_fn=<AddBackward0>)\n",
            "Epoch 174/1000, Loss: 0.17048019170761108\n",
            "tensor([[-0.4316],\n",
            "        [ 0.9181],\n",
            "        [ 4.7030],\n",
            "        [-7.2222],\n",
            "        [ 5.3694]], grad_fn=<AddBackward0>)\n",
            "Epoch 175/1000, Loss: 0.17016670107841492\n",
            "tensor([[-0.4340],\n",
            "        [ 0.9198],\n",
            "        [ 4.7068],\n",
            "        [-7.2390],\n",
            "        [ 5.3820]], grad_fn=<AddBackward0>)\n",
            "Epoch 176/1000, Loss: 0.1698545217514038\n",
            "tensor([[-0.4364],\n",
            "        [ 0.9216],\n",
            "        [ 4.7105],\n",
            "        [-7.2558],\n",
            "        [ 5.3946]], grad_fn=<AddBackward0>)\n",
            "Epoch 177/1000, Loss: 0.16954359412193298\n",
            "tensor([[-0.4388],\n",
            "        [ 0.9233],\n",
            "        [ 4.7143],\n",
            "        [-7.2726],\n",
            "        [ 5.4071]], grad_fn=<AddBackward0>)\n",
            "Epoch 178/1000, Loss: 0.1692337691783905\n",
            "tensor([[-0.4413],\n",
            "        [ 0.9251],\n",
            "        [ 4.7180],\n",
            "        [-7.2893],\n",
            "        [ 5.4196]], grad_fn=<AddBackward0>)\n",
            "Epoch 179/1000, Loss: 0.16892527043819427\n",
            "tensor([[-0.4437],\n",
            "        [ 0.9268],\n",
            "        [ 4.7217],\n",
            "        [-7.3059],\n",
            "        [ 5.4321]], grad_fn=<AddBackward0>)\n",
            "Epoch 180/1000, Loss: 0.16861791908740997\n",
            "tensor([[-0.4461],\n",
            "        [ 0.9286],\n",
            "        [ 4.7254],\n",
            "        [-7.3225],\n",
            "        [ 5.4445]], grad_fn=<AddBackward0>)\n",
            "Epoch 181/1000, Loss: 0.16831177473068237\n",
            "tensor([[-0.4485],\n",
            "        [ 0.9303],\n",
            "        [ 4.7290],\n",
            "        [-7.3391],\n",
            "        [ 5.4570]], grad_fn=<AddBackward0>)\n",
            "Epoch 182/1000, Loss: 0.16800685226917267\n",
            "tensor([[-0.4509],\n",
            "        [ 0.9321],\n",
            "        [ 4.7326],\n",
            "        [-7.3556],\n",
            "        [ 5.4694]], grad_fn=<AddBackward0>)\n",
            "Epoch 183/1000, Loss: 0.1677030622959137\n",
            "tensor([[-0.4532],\n",
            "        [ 0.9338],\n",
            "        [ 4.7363],\n",
            "        [-7.3721],\n",
            "        [ 5.4818]], grad_fn=<AddBackward0>)\n",
            "Epoch 184/1000, Loss: 0.16740036010742188\n",
            "tensor([[-0.4556],\n",
            "        [ 0.9356],\n",
            "        [ 4.7399],\n",
            "        [-7.3885],\n",
            "        [ 5.4941]], grad_fn=<AddBackward0>)\n",
            "Epoch 185/1000, Loss: 0.16709890961647034\n",
            "tensor([[-0.4580],\n",
            "        [ 0.9373],\n",
            "        [ 4.7435],\n",
            "        [-7.4049],\n",
            "        [ 5.5064]], grad_fn=<AddBackward0>)\n",
            "Epoch 186/1000, Loss: 0.16679857671260834\n",
            "tensor([[-0.4604],\n",
            "        [ 0.9390],\n",
            "        [ 4.7470],\n",
            "        [-7.4213],\n",
            "        [ 5.5187]], grad_fn=<AddBackward0>)\n",
            "Epoch 187/1000, Loss: 0.1664993017911911\n",
            "tensor([[-0.4627],\n",
            "        [ 0.9408],\n",
            "        [ 4.7506],\n",
            "        [-7.4376],\n",
            "        [ 5.5310]], grad_fn=<AddBackward0>)\n",
            "Epoch 188/1000, Loss: 0.16620126366615295\n",
            "tensor([[-0.4651],\n",
            "        [ 0.9425],\n",
            "        [ 4.7541],\n",
            "        [-7.4539],\n",
            "        [ 5.5433]], grad_fn=<AddBackward0>)\n",
            "Epoch 189/1000, Loss: 0.16590432822704315\n",
            "tensor([[-0.4674],\n",
            "        [ 0.9443],\n",
            "        [ 4.7576],\n",
            "        [-7.4701],\n",
            "        [ 5.5555]], grad_fn=<AddBackward0>)\n",
            "Epoch 190/1000, Loss: 0.1656084954738617\n",
            "tensor([[-0.4698],\n",
            "        [ 0.9460],\n",
            "        [ 4.7612],\n",
            "        [-7.4863],\n",
            "        [ 5.5677]], grad_fn=<AddBackward0>)\n",
            "Epoch 191/1000, Loss: 0.16531367599964142\n",
            "tensor([[-0.4721],\n",
            "        [ 0.9477],\n",
            "        [ 4.7646],\n",
            "        [-7.5025],\n",
            "        [ 5.5799]], grad_fn=<AddBackward0>)\n",
            "Epoch 192/1000, Loss: 0.16502003371715546\n",
            "tensor([[-0.4744],\n",
            "        [ 0.9495],\n",
            "        [ 4.7681],\n",
            "        [-7.5186],\n",
            "        [ 5.5920]], grad_fn=<AddBackward0>)\n",
            "Epoch 193/1000, Loss: 0.16472743451595306\n",
            "tensor([[-0.4768],\n",
            "        [ 0.9512],\n",
            "        [ 4.7716],\n",
            "        [-7.5347],\n",
            "        [ 5.6041]], grad_fn=<AddBackward0>)\n",
            "Epoch 194/1000, Loss: 0.16443590819835663\n",
            "tensor([[-0.4791],\n",
            "        [ 0.9529],\n",
            "        [ 4.7750],\n",
            "        [-7.5507],\n",
            "        [ 5.6163]], grad_fn=<AddBackward0>)\n",
            "Epoch 195/1000, Loss: 0.16414548456668854\n",
            "tensor([[-0.4814],\n",
            "        [ 0.9547],\n",
            "        [ 4.7785],\n",
            "        [-7.5667],\n",
            "        [ 5.6283]], grad_fn=<AddBackward0>)\n",
            "Epoch 196/1000, Loss: 0.16385601460933685\n",
            "tensor([[-0.4837],\n",
            "        [ 0.9564],\n",
            "        [ 4.7819],\n",
            "        [-7.5827],\n",
            "        [ 5.6404]], grad_fn=<AddBackward0>)\n",
            "Epoch 197/1000, Loss: 0.1635676771402359\n",
            "tensor([[-0.4860],\n",
            "        [ 0.9581],\n",
            "        [ 4.7853],\n",
            "        [-7.5986],\n",
            "        [ 5.6524]], grad_fn=<AddBackward0>)\n",
            "Epoch 198/1000, Loss: 0.16328033804893494\n",
            "tensor([[-0.4883],\n",
            "        [ 0.9598],\n",
            "        [ 4.7887],\n",
            "        [-7.6145],\n",
            "        [ 5.6644]], grad_fn=<AddBackward0>)\n",
            "Epoch 199/1000, Loss: 0.16299408674240112\n",
            "tensor([[-0.4906],\n",
            "        [ 0.9616],\n",
            "        [ 4.7920],\n",
            "        [-7.6303],\n",
            "        [ 5.6764]], grad_fn=<AddBackward0>)\n",
            "Epoch 200/1000, Loss: 0.16270874440670013\n",
            "tensor([[-0.4929],\n",
            "        [ 0.9633],\n",
            "        [ 4.7954],\n",
            "        [-7.6462],\n",
            "        [ 5.6884]], grad_fn=<AddBackward0>)\n",
            "Epoch 201/1000, Loss: 0.1624244898557663\n",
            "tensor([[-0.4952],\n",
            "        [ 0.9650],\n",
            "        [ 4.7987],\n",
            "        [-7.6619],\n",
            "        [ 5.7003]], grad_fn=<AddBackward0>)\n",
            "Epoch 202/1000, Loss: 0.16214129328727722\n",
            "tensor([[-0.4975],\n",
            "        [ 0.9667],\n",
            "        [ 4.8021],\n",
            "        [-7.6777],\n",
            "        [ 5.7123]], grad_fn=<AddBackward0>)\n",
            "Epoch 203/1000, Loss: 0.16185902059078217\n",
            "tensor([[-0.4997],\n",
            "        [ 0.9685],\n",
            "        [ 4.8054],\n",
            "        [-7.6934],\n",
            "        [ 5.7242]], grad_fn=<AddBackward0>)\n",
            "Epoch 204/1000, Loss: 0.1615777313709259\n",
            "tensor([[-0.5020],\n",
            "        [ 0.9702],\n",
            "        [ 4.8087],\n",
            "        [-7.7090],\n",
            "        [ 5.7360]], grad_fn=<AddBackward0>)\n",
            "Epoch 205/1000, Loss: 0.16129745543003082\n",
            "tensor([[-0.5043],\n",
            "        [ 0.9719],\n",
            "        [ 4.8120],\n",
            "        [-7.7247],\n",
            "        [ 5.7479]], grad_fn=<AddBackward0>)\n",
            "Epoch 206/1000, Loss: 0.16101817786693573\n",
            "tensor([[-0.5065],\n",
            "        [ 0.9736],\n",
            "        [ 4.8153],\n",
            "        [-7.7403],\n",
            "        [ 5.7597]], grad_fn=<AddBackward0>)\n",
            "Epoch 207/1000, Loss: 0.16073989868164062\n",
            "tensor([[-0.5088],\n",
            "        [ 0.9753],\n",
            "        [ 4.8185],\n",
            "        [-7.7558],\n",
            "        [ 5.7715]], grad_fn=<AddBackward0>)\n",
            "Epoch 208/1000, Loss: 0.16046252846717834\n",
            "tensor([[-0.5110],\n",
            "        [ 0.9771],\n",
            "        [ 4.8218],\n",
            "        [-7.7714],\n",
            "        [ 5.7833]], grad_fn=<AddBackward0>)\n",
            "Epoch 209/1000, Loss: 0.16018611192703247\n",
            "tensor([[-0.5133],\n",
            "        [ 0.9788],\n",
            "        [ 4.8250],\n",
            "        [-7.7869],\n",
            "        [ 5.7951]], grad_fn=<AddBackward0>)\n",
            "Epoch 210/1000, Loss: 0.1599106788635254\n",
            "tensor([[-0.5155],\n",
            "        [ 0.9805],\n",
            "        [ 4.8282],\n",
            "        [-7.8023],\n",
            "        [ 5.8068]], grad_fn=<AddBackward0>)\n",
            "Epoch 211/1000, Loss: 0.15963616967201233\n",
            "tensor([[-0.5177],\n",
            "        [ 0.9822],\n",
            "        [ 4.8314],\n",
            "        [-7.8178],\n",
            "        [ 5.8186]], grad_fn=<AddBackward0>)\n",
            "Epoch 212/1000, Loss: 0.1593625545501709\n",
            "tensor([[-0.5200],\n",
            "        [ 0.9839],\n",
            "        [ 4.8346],\n",
            "        [-7.8331],\n",
            "        [ 5.8303]], grad_fn=<AddBackward0>)\n",
            "Epoch 213/1000, Loss: 0.15908993780612946\n",
            "tensor([[-0.5222],\n",
            "        [ 0.9856],\n",
            "        [ 4.8378],\n",
            "        [-7.8485],\n",
            "        [ 5.8419]], grad_fn=<AddBackward0>)\n",
            "Epoch 214/1000, Loss: 0.15881821513175964\n",
            "tensor([[-0.5244],\n",
            "        [ 0.9873],\n",
            "        [ 4.8410],\n",
            "        [-7.8638],\n",
            "        [ 5.8536]], grad_fn=<AddBackward0>)\n",
            "Epoch 215/1000, Loss: 0.15854746103286743\n",
            "tensor([[-0.5266],\n",
            "        [ 0.9890],\n",
            "        [ 4.8442],\n",
            "        [-7.8791],\n",
            "        [ 5.8652]], grad_fn=<AddBackward0>)\n",
            "Epoch 216/1000, Loss: 0.15827754139900208\n",
            "tensor([[-0.5288],\n",
            "        [ 0.9907],\n",
            "        [ 4.8473],\n",
            "        [-7.8944],\n",
            "        [ 5.8769]], grad_fn=<AddBackward0>)\n",
            "Epoch 217/1000, Loss: 0.1580086052417755\n",
            "tensor([[-0.5310],\n",
            "        [ 0.9925],\n",
            "        [ 4.8504],\n",
            "        [-7.9096],\n",
            "        [ 5.8885]], grad_fn=<AddBackward0>)\n",
            "Epoch 218/1000, Loss: 0.1577405035495758\n",
            "tensor([[-0.5332],\n",
            "        [ 0.9942],\n",
            "        [ 4.8536],\n",
            "        [-7.9248],\n",
            "        [ 5.9000]], grad_fn=<AddBackward0>)\n",
            "Epoch 219/1000, Loss: 0.15747329592704773\n",
            "tensor([[-0.5354],\n",
            "        [ 0.9959],\n",
            "        [ 4.8567],\n",
            "        [-7.9400],\n",
            "        [ 5.9116]], grad_fn=<AddBackward0>)\n",
            "Epoch 220/1000, Loss: 0.15720705687999725\n",
            "tensor([[-0.5376],\n",
            "        [ 0.9976],\n",
            "        [ 4.8598],\n",
            "        [-7.9551],\n",
            "        [ 5.9231]], grad_fn=<AddBackward0>)\n",
            "Epoch 221/1000, Loss: 0.15694166719913483\n",
            "tensor([[-0.5398],\n",
            "        [ 0.9993],\n",
            "        [ 4.8629],\n",
            "        [-7.9702],\n",
            "        [ 5.9346]], grad_fn=<AddBackward0>)\n",
            "Epoch 222/1000, Loss: 0.15667715668678284\n",
            "tensor([[-0.5420],\n",
            "        [ 1.0010],\n",
            "        [ 4.8659],\n",
            "        [-7.9853],\n",
            "        [ 5.9461]], grad_fn=<AddBackward0>)\n",
            "Epoch 223/1000, Loss: 0.15641340613365173\n",
            "tensor([[-0.5442],\n",
            "        [ 1.0027],\n",
            "        [ 4.8690],\n",
            "        [-8.0003],\n",
            "        [ 5.9576]], grad_fn=<AddBackward0>)\n",
            "Epoch 224/1000, Loss: 0.1561506986618042\n",
            "tensor([[-0.5463],\n",
            "        [ 1.0044],\n",
            "        [ 4.8721],\n",
            "        [-8.0153],\n",
            "        [ 5.9691]], grad_fn=<AddBackward0>)\n",
            "Epoch 225/1000, Loss: 0.15588872134685516\n",
            "tensor([[-0.5485],\n",
            "        [ 1.0060],\n",
            "        [ 4.8751],\n",
            "        [-8.0303],\n",
            "        [ 5.9805]], grad_fn=<AddBackward0>)\n",
            "Epoch 226/1000, Loss: 0.15562766790390015\n",
            "tensor([[-0.5507],\n",
            "        [ 1.0077],\n",
            "        [ 4.8781],\n",
            "        [-8.0452],\n",
            "        [ 5.9919]], grad_fn=<AddBackward0>)\n",
            "Epoch 227/1000, Loss: 0.15536746382713318\n",
            "tensor([[-0.5528],\n",
            "        [ 1.0094],\n",
            "        [ 4.8812],\n",
            "        [-8.0601],\n",
            "        [ 6.0033]], grad_fn=<AddBackward0>)\n",
            "Epoch 228/1000, Loss: 0.1551080048084259\n",
            "tensor([[-0.5550],\n",
            "        [ 1.0111],\n",
            "        [ 4.8842],\n",
            "        [-8.0750],\n",
            "        [ 6.0147]], grad_fn=<AddBackward0>)\n",
            "Epoch 229/1000, Loss: 0.15484954416751862\n",
            "tensor([[-0.5571],\n",
            "        [ 1.0128],\n",
            "        [ 4.8872],\n",
            "        [-8.0899],\n",
            "        [ 6.0260]], grad_fn=<AddBackward0>)\n",
            "Epoch 230/1000, Loss: 0.15459176898002625\n",
            "tensor([[-0.5593],\n",
            "        [ 1.0145],\n",
            "        [ 4.8902],\n",
            "        [-8.1047],\n",
            "        [ 6.0374]], grad_fn=<AddBackward0>)\n",
            "Epoch 231/1000, Loss: 0.1543348729610443\n",
            "tensor([[-0.5614],\n",
            "        [ 1.0162],\n",
            "        [ 4.8932],\n",
            "        [-8.1195],\n",
            "        [ 6.0487]], grad_fn=<AddBackward0>)\n",
            "Epoch 232/1000, Loss: 0.154078871011734\n",
            "tensor([[-0.5635],\n",
            "        [ 1.0179],\n",
            "        [ 4.8961],\n",
            "        [-8.1342],\n",
            "        [ 6.0600]], grad_fn=<AddBackward0>)\n",
            "Epoch 233/1000, Loss: 0.15382356941699982\n",
            "tensor([[-0.5657],\n",
            "        [ 1.0196],\n",
            "        [ 4.8991],\n",
            "        [-8.1490],\n",
            "        [ 6.0713]], grad_fn=<AddBackward0>)\n",
            "Epoch 234/1000, Loss: 0.15356919169425964\n",
            "tensor([[-0.5678],\n",
            "        [ 1.0212],\n",
            "        [ 4.9020],\n",
            "        [-8.1637],\n",
            "        [ 6.0825]], grad_fn=<AddBackward0>)\n",
            "Epoch 235/1000, Loss: 0.15331551432609558\n",
            "tensor([[-0.5699],\n",
            "        [ 1.0229],\n",
            "        [ 4.9050],\n",
            "        [-8.1783],\n",
            "        [ 6.0938]], grad_fn=<AddBackward0>)\n",
            "Epoch 236/1000, Loss: 0.15306270122528076\n",
            "tensor([[-0.5721],\n",
            "        [ 1.0246],\n",
            "        [ 4.9079],\n",
            "        [-8.1930],\n",
            "        [ 6.1050]], grad_fn=<AddBackward0>)\n",
            "Epoch 237/1000, Loss: 0.15281064808368683\n",
            "tensor([[-0.5742],\n",
            "        [ 1.0263],\n",
            "        [ 4.9108],\n",
            "        [-8.2076],\n",
            "        [ 6.1162]], grad_fn=<AddBackward0>)\n",
            "Epoch 238/1000, Loss: 0.15255944430828094\n",
            "tensor([[-0.5763],\n",
            "        [ 1.0279],\n",
            "        [ 4.9137],\n",
            "        [-8.2222],\n",
            "        [ 6.1274]], grad_fn=<AddBackward0>)\n",
            "Epoch 239/1000, Loss: 0.15230901539325714\n",
            "tensor([[-0.5784],\n",
            "        [ 1.0296],\n",
            "        [ 4.9166],\n",
            "        [-8.2368],\n",
            "        [ 6.1385]], grad_fn=<AddBackward0>)\n",
            "Epoch 240/1000, Loss: 0.1520593762397766\n",
            "tensor([[-0.5805],\n",
            "        [ 1.0313],\n",
            "        [ 4.9195],\n",
            "        [-8.2513],\n",
            "        [ 6.1497]], grad_fn=<AddBackward0>)\n",
            "Epoch 241/1000, Loss: 0.1518104374408722\n",
            "tensor([[-0.5826],\n",
            "        [ 1.0330],\n",
            "        [ 4.9224],\n",
            "        [-8.2658],\n",
            "        [ 6.1608]], grad_fn=<AddBackward0>)\n",
            "Epoch 242/1000, Loss: 0.15156236290931702\n",
            "tensor([[-0.5847],\n",
            "        [ 1.0346],\n",
            "        [ 4.9253],\n",
            "        [-8.2803],\n",
            "        [ 6.1719]], grad_fn=<AddBackward0>)\n",
            "Epoch 243/1000, Loss: 0.15131501853466034\n",
            "tensor([[-0.5868],\n",
            "        [ 1.0363],\n",
            "        [ 4.9281],\n",
            "        [-8.2947],\n",
            "        [ 6.1830]], grad_fn=<AddBackward0>)\n",
            "Epoch 244/1000, Loss: 0.15106847882270813\n",
            "tensor([[-0.5889],\n",
            "        [ 1.0380],\n",
            "        [ 4.9310],\n",
            "        [-8.3091],\n",
            "        [ 6.1941]], grad_fn=<AddBackward0>)\n",
            "Epoch 245/1000, Loss: 0.15082265436649323\n",
            "tensor([[-0.5910],\n",
            "        [ 1.0396],\n",
            "        [ 4.9338],\n",
            "        [-8.3235],\n",
            "        [ 6.2052]], grad_fn=<AddBackward0>)\n",
            "Epoch 246/1000, Loss: 0.1505776345729828\n",
            "tensor([[-0.5930],\n",
            "        [ 1.0413],\n",
            "        [ 4.9366],\n",
            "        [-8.3379],\n",
            "        [ 6.2162]], grad_fn=<AddBackward0>)\n",
            "Epoch 247/1000, Loss: 0.15033334493637085\n",
            "tensor([[-0.5951],\n",
            "        [ 1.0430],\n",
            "        [ 4.9395],\n",
            "        [-8.3522],\n",
            "        [ 6.2272]], grad_fn=<AddBackward0>)\n",
            "Epoch 248/1000, Loss: 0.15008988976478577\n",
            "tensor([[-0.5972],\n",
            "        [ 1.0446],\n",
            "        [ 4.9423],\n",
            "        [-8.3665],\n",
            "        [ 6.2382]], grad_fn=<AddBackward0>)\n",
            "Epoch 249/1000, Loss: 0.14984707534313202\n",
            "tensor([[-0.5993],\n",
            "        [ 1.0463],\n",
            "        [ 4.9451],\n",
            "        [-8.3808],\n",
            "        [ 6.2492]], grad_fn=<AddBackward0>)\n",
            "Epoch 250/1000, Loss: 0.1496049463748932\n",
            "tensor([[-0.6013],\n",
            "        [ 1.0480],\n",
            "        [ 4.9479],\n",
            "        [-8.3951],\n",
            "        [ 6.2602]], grad_fn=<AddBackward0>)\n",
            "Epoch 251/1000, Loss: 0.14936363697052002\n",
            "tensor([[-0.6034],\n",
            "        [ 1.0496],\n",
            "        [ 4.9507],\n",
            "        [-8.4093],\n",
            "        [ 6.2711]], grad_fn=<AddBackward0>)\n",
            "Epoch 252/1000, Loss: 0.14912307262420654\n",
            "tensor([[-0.6054],\n",
            "        [ 1.0513],\n",
            "        [ 4.9534],\n",
            "        [-8.4235],\n",
            "        [ 6.2820]], grad_fn=<AddBackward0>)\n",
            "Epoch 253/1000, Loss: 0.14888322353363037\n",
            "tensor([[-0.6075],\n",
            "        [ 1.0529],\n",
            "        [ 4.9562],\n",
            "        [-8.4377],\n",
            "        [ 6.2930]], grad_fn=<AddBackward0>)\n",
            "Epoch 254/1000, Loss: 0.14864405989646912\n",
            "tensor([[-0.6095],\n",
            "        [ 1.0546],\n",
            "        [ 4.9590],\n",
            "        [-8.4518],\n",
            "        [ 6.3039]], grad_fn=<AddBackward0>)\n",
            "Epoch 255/1000, Loss: 0.1484057456254959\n",
            "tensor([[-0.6116],\n",
            "        [ 1.0562],\n",
            "        [ 4.9617],\n",
            "        [-8.4659],\n",
            "        [ 6.3147]], grad_fn=<AddBackward0>)\n",
            "Epoch 256/1000, Loss: 0.14816799759864807\n",
            "tensor([[-0.6136],\n",
            "        [ 1.0579],\n",
            "        [ 4.9645],\n",
            "        [-8.4800],\n",
            "        [ 6.3256]], grad_fn=<AddBackward0>)\n",
            "Epoch 257/1000, Loss: 0.14793099462985992\n",
            "tensor([[-0.6157],\n",
            "        [ 1.0595],\n",
            "        [ 4.9672],\n",
            "        [-8.4941],\n",
            "        [ 6.3364]], grad_fn=<AddBackward0>)\n",
            "Epoch 258/1000, Loss: 0.14769470691680908\n",
            "tensor([[-0.6177],\n",
            "        [ 1.0612],\n",
            "        [ 4.9699],\n",
            "        [-8.5082],\n",
            "        [ 6.3473]], grad_fn=<AddBackward0>)\n",
            "Epoch 259/1000, Loss: 0.14745916426181793\n",
            "tensor([[-0.6197],\n",
            "        [ 1.0628],\n",
            "        [ 4.9726],\n",
            "        [-8.5222],\n",
            "        [ 6.3581]], grad_fn=<AddBackward0>)\n",
            "Epoch 260/1000, Loss: 0.1472243070602417\n",
            "tensor([[-0.6218],\n",
            "        [ 1.0645],\n",
            "        [ 4.9753],\n",
            "        [-8.5362],\n",
            "        [ 6.3689]], grad_fn=<AddBackward0>)\n",
            "Epoch 261/1000, Loss: 0.14699013531208038\n",
            "tensor([[-0.6238],\n",
            "        [ 1.0661],\n",
            "        [ 4.9780],\n",
            "        [-8.5501],\n",
            "        [ 6.3797]], grad_fn=<AddBackward0>)\n",
            "Epoch 262/1000, Loss: 0.14675672352313995\n",
            "tensor([[-0.6258],\n",
            "        [ 1.0677],\n",
            "        [ 4.9807],\n",
            "        [-8.5641],\n",
            "        [ 6.3904]], grad_fn=<AddBackward0>)\n",
            "Epoch 263/1000, Loss: 0.14652390778064728\n",
            "tensor([[-0.6278],\n",
            "        [ 1.0694],\n",
            "        [ 4.9834],\n",
            "        [-8.5780],\n",
            "        [ 6.4012]], grad_fn=<AddBackward0>)\n",
            "Epoch 264/1000, Loss: 0.14629177749156952\n",
            "tensor([[-0.6298],\n",
            "        [ 1.0710],\n",
            "        [ 4.9861],\n",
            "        [-8.5919],\n",
            "        [ 6.4119]], grad_fn=<AddBackward0>)\n",
            "Epoch 265/1000, Loss: 0.14606036245822906\n",
            "tensor([[-0.6319],\n",
            "        [ 1.0727],\n",
            "        [ 4.9888],\n",
            "        [-8.6058],\n",
            "        [ 6.4226]], grad_fn=<AddBackward0>)\n",
            "Epoch 266/1000, Loss: 0.1458296924829483\n",
            "tensor([[-0.6339],\n",
            "        [ 1.0743],\n",
            "        [ 4.9914],\n",
            "        [-8.6196],\n",
            "        [ 6.4333]], grad_fn=<AddBackward0>)\n",
            "Epoch 267/1000, Loss: 0.1455996334552765\n",
            "tensor([[-0.6359],\n",
            "        [ 1.0759],\n",
            "        [ 4.9941],\n",
            "        [-8.6334],\n",
            "        [ 6.4440]], grad_fn=<AddBackward0>)\n",
            "Epoch 268/1000, Loss: 0.14537033438682556\n",
            "tensor([[-0.6379],\n",
            "        [ 1.0776],\n",
            "        [ 4.9967],\n",
            "        [-8.6472],\n",
            "        [ 6.4546]], grad_fn=<AddBackward0>)\n",
            "Epoch 269/1000, Loss: 0.14514155685901642\n",
            "tensor([[-0.6399],\n",
            "        [ 1.0792],\n",
            "        [ 4.9993],\n",
            "        [-8.6610],\n",
            "        [ 6.4653]], grad_fn=<AddBackward0>)\n",
            "Epoch 270/1000, Loss: 0.14491353929042816\n",
            "tensor([[-0.6418],\n",
            "        [ 1.0808],\n",
            "        [ 5.0020],\n",
            "        [-8.6748],\n",
            "        [ 6.4759]], grad_fn=<AddBackward0>)\n",
            "Epoch 271/1000, Loss: 0.14468610286712646\n",
            "tensor([[-0.6438],\n",
            "        [ 1.0825],\n",
            "        [ 5.0046],\n",
            "        [-8.6885],\n",
            "        [ 6.4865]], grad_fn=<AddBackward0>)\n",
            "Epoch 272/1000, Loss: 0.14445945620536804\n",
            "tensor([[-0.6458],\n",
            "        [ 1.0841],\n",
            "        [ 5.0072],\n",
            "        [-8.7022],\n",
            "        [ 6.4971]], grad_fn=<AddBackward0>)\n",
            "Epoch 273/1000, Loss: 0.14423337578773499\n",
            "tensor([[-0.6478],\n",
            "        [ 1.0857],\n",
            "        [ 5.0098],\n",
            "        [-8.7159],\n",
            "        [ 6.5077]], grad_fn=<AddBackward0>)\n",
            "Epoch 274/1000, Loss: 0.14400801062583923\n",
            "tensor([[-0.6498],\n",
            "        [ 1.0873],\n",
            "        [ 5.0124],\n",
            "        [-8.7295],\n",
            "        [ 6.5182]], grad_fn=<AddBackward0>)\n",
            "Epoch 275/1000, Loss: 0.14378315210342407\n",
            "tensor([[-0.6518],\n",
            "        [ 1.0889],\n",
            "        [ 5.0150],\n",
            "        [-8.7432],\n",
            "        [ 6.5288]], grad_fn=<AddBackward0>)\n",
            "Epoch 276/1000, Loss: 0.143559068441391\n",
            "tensor([[-0.6537],\n",
            "        [ 1.0906],\n",
            "        [ 5.0176],\n",
            "        [-8.7568],\n",
            "        [ 6.5393]], grad_fn=<AddBackward0>)\n",
            "Epoch 277/1000, Loss: 0.14333561062812805\n",
            "tensor([[-0.6557],\n",
            "        [ 1.0922],\n",
            "        [ 5.0202],\n",
            "        [-8.7703],\n",
            "        [ 6.5498]], grad_fn=<AddBackward0>)\n",
            "Epoch 278/1000, Loss: 0.14311273396015167\n",
            "tensor([[-0.6577],\n",
            "        [ 1.0938],\n",
            "        [ 5.0227],\n",
            "        [-8.7839],\n",
            "        [ 6.5603]], grad_fn=<AddBackward0>)\n",
            "Epoch 279/1000, Loss: 0.14289061725139618\n",
            "tensor([[-0.6596],\n",
            "        [ 1.0954],\n",
            "        [ 5.0253],\n",
            "        [-8.7974],\n",
            "        [ 6.5708]], grad_fn=<AddBackward0>)\n",
            "Epoch 280/1000, Loss: 0.14266903698444366\n",
            "tensor([[-0.6616],\n",
            "        [ 1.0970],\n",
            "        [ 5.0278],\n",
            "        [-8.8110],\n",
            "        [ 6.5813]], grad_fn=<AddBackward0>)\n",
            "Epoch 281/1000, Loss: 0.14244811236858368\n",
            "tensor([[-0.6635],\n",
            "        [ 1.0986],\n",
            "        [ 5.0304],\n",
            "        [-8.8244],\n",
            "        [ 6.5917]], grad_fn=<AddBackward0>)\n",
            "Epoch 282/1000, Loss: 0.14222776889801025\n",
            "tensor([[-0.6655],\n",
            "        [ 1.1003],\n",
            "        [ 5.0329],\n",
            "        [-8.8379],\n",
            "        [ 6.6022]], grad_fn=<AddBackward0>)\n",
            "Epoch 283/1000, Loss: 0.14200811088085175\n",
            "tensor([[-0.6674],\n",
            "        [ 1.1019],\n",
            "        [ 5.0355],\n",
            "        [-8.8514],\n",
            "        [ 6.6126]], grad_fn=<AddBackward0>)\n",
            "Epoch 284/1000, Loss: 0.14178909361362457\n",
            "tensor([[-0.6694],\n",
            "        [ 1.1035],\n",
            "        [ 5.0380],\n",
            "        [-8.8648],\n",
            "        [ 6.6230]], grad_fn=<AddBackward0>)\n",
            "Epoch 285/1000, Loss: 0.14157061278820038\n",
            "tensor([[-0.6713],\n",
            "        [ 1.1051],\n",
            "        [ 5.0405],\n",
            "        [-8.8782],\n",
            "        [ 6.6334]], grad_fn=<AddBackward0>)\n",
            "Epoch 286/1000, Loss: 0.14135292172431946\n",
            "tensor([[-0.6733],\n",
            "        [ 1.1067],\n",
            "        [ 5.0430],\n",
            "        [-8.8916],\n",
            "        [ 6.6438]], grad_fn=<AddBackward0>)\n",
            "Epoch 287/1000, Loss: 0.14113560318946838\n",
            "tensor([[-0.6752],\n",
            "        [ 1.1083],\n",
            "        [ 5.0455],\n",
            "        [-8.9049],\n",
            "        [ 6.6541]], grad_fn=<AddBackward0>)\n",
            "Epoch 288/1000, Loss: 0.14091899991035461\n",
            "tensor([[-0.6771],\n",
            "        [ 1.1099],\n",
            "        [ 5.0480],\n",
            "        [-8.9182],\n",
            "        [ 6.6645]], grad_fn=<AddBackward0>)\n",
            "Epoch 289/1000, Loss: 0.140703022480011\n",
            "tensor([[-0.6791],\n",
            "        [ 1.1115],\n",
            "        [ 5.0505],\n",
            "        [-8.9316],\n",
            "        [ 6.6748]], grad_fn=<AddBackward0>)\n",
            "Epoch 290/1000, Loss: 0.1404876708984375\n",
            "tensor([[-0.6810],\n",
            "        [ 1.1131],\n",
            "        [ 5.0530],\n",
            "        [-8.9448],\n",
            "        [ 6.6851]], grad_fn=<AddBackward0>)\n",
            "Epoch 291/1000, Loss: 0.14027287065982819\n",
            "tensor([[-0.6829],\n",
            "        [ 1.1147],\n",
            "        [ 5.0555],\n",
            "        [-8.9581],\n",
            "        [ 6.6954]], grad_fn=<AddBackward0>)\n",
            "Epoch 292/1000, Loss: 0.14005863666534424\n",
            "tensor([[-0.6848],\n",
            "        [ 1.1163],\n",
            "        [ 5.0579],\n",
            "        [-8.9714],\n",
            "        [ 6.7057]], grad_fn=<AddBackward0>)\n",
            "Epoch 293/1000, Loss: 0.13984502851963043\n",
            "tensor([[-0.6867],\n",
            "        [ 1.1179],\n",
            "        [ 5.0604],\n",
            "        [-8.9846],\n",
            "        [ 6.7160]], grad_fn=<AddBackward0>)\n",
            "Epoch 294/1000, Loss: 0.13963207602500916\n",
            "tensor([[-0.6887],\n",
            "        [ 1.1195],\n",
            "        [ 5.0629],\n",
            "        [-8.9978],\n",
            "        [ 6.7262]], grad_fn=<AddBackward0>)\n",
            "Epoch 295/1000, Loss: 0.13941970467567444\n",
            "tensor([[-0.6906],\n",
            "        [ 1.1211],\n",
            "        [ 5.0653],\n",
            "        [-9.0110],\n",
            "        [ 6.7365]], grad_fn=<AddBackward0>)\n",
            "Epoch 296/1000, Loss: 0.13920782506465912\n",
            "tensor([[-0.6925],\n",
            "        [ 1.1227],\n",
            "        [ 5.0678],\n",
            "        [-9.0241],\n",
            "        [ 6.7467]], grad_fn=<AddBackward0>)\n",
            "Epoch 297/1000, Loss: 0.13899663090705872\n",
            "tensor([[-0.6944],\n",
            "        [ 1.1243],\n",
            "        [ 5.0702],\n",
            "        [-9.0372],\n",
            "        [ 6.7569]], grad_fn=<AddBackward0>)\n",
            "Epoch 298/1000, Loss: 0.1387859731912613\n",
            "tensor([[-0.6963],\n",
            "        [ 1.1258],\n",
            "        [ 5.0726],\n",
            "        [-9.0504],\n",
            "        [ 6.7671]], grad_fn=<AddBackward0>)\n",
            "Epoch 299/1000, Loss: 0.13857600092887878\n",
            "tensor([[-0.6982],\n",
            "        [ 1.1274],\n",
            "        [ 5.0751],\n",
            "        [-9.0635],\n",
            "        [ 6.7773]], grad_fn=<AddBackward0>)\n",
            "Epoch 300/1000, Loss: 0.13836640119552612\n",
            "tensor([[-0.7001],\n",
            "        [ 1.1290],\n",
            "        [ 5.0775],\n",
            "        [-9.0765],\n",
            "        [ 6.7875]], grad_fn=<AddBackward0>)\n",
            "Epoch 301/1000, Loss: 0.138157457113266\n",
            "tensor([[-0.7020],\n",
            "        [ 1.1306],\n",
            "        [ 5.0799],\n",
            "        [-9.0896],\n",
            "        [ 6.7976]], grad_fn=<AddBackward0>)\n",
            "Epoch 302/1000, Loss: 0.13794906437397003\n",
            "tensor([[-0.7039],\n",
            "        [ 1.1322],\n",
            "        [ 5.0823],\n",
            "        [-9.1026],\n",
            "        [ 6.8077]], grad_fn=<AddBackward0>)\n",
            "Epoch 303/1000, Loss: 0.13774137198925018\n",
            "tensor([[-0.7057],\n",
            "        [ 1.1338],\n",
            "        [ 5.0847],\n",
            "        [-9.1156],\n",
            "        [ 6.8179]], grad_fn=<AddBackward0>)\n",
            "Epoch 304/1000, Loss: 0.13753418624401093\n",
            "tensor([[-0.7076],\n",
            "        [ 1.1353],\n",
            "        [ 5.0871],\n",
            "        [-9.1286],\n",
            "        [ 6.8280]], grad_fn=<AddBackward0>)\n",
            "Epoch 305/1000, Loss: 0.13732749223709106\n",
            "tensor([[-0.7095],\n",
            "        [ 1.1369],\n",
            "        [ 5.0895],\n",
            "        [-9.1416],\n",
            "        [ 6.8381]], grad_fn=<AddBackward0>)\n",
            "Epoch 306/1000, Loss: 0.1371215283870697\n",
            "tensor([[-0.7114],\n",
            "        [ 1.1385],\n",
            "        [ 5.0919],\n",
            "        [-9.1545],\n",
            "        [ 6.8481]], grad_fn=<AddBackward0>)\n",
            "Epoch 307/1000, Loss: 0.13691593706607819\n",
            "tensor([[-0.7133],\n",
            "        [ 1.1401],\n",
            "        [ 5.0942],\n",
            "        [-9.1675],\n",
            "        [ 6.8582]], grad_fn=<AddBackward0>)\n",
            "Epoch 308/1000, Loss: 0.13671094179153442\n",
            "tensor([[-0.7151],\n",
            "        [ 1.1416],\n",
            "        [ 5.0966],\n",
            "        [-9.1804],\n",
            "        [ 6.8683]], grad_fn=<AddBackward0>)\n",
            "Epoch 309/1000, Loss: 0.136506587266922\n",
            "tensor([[-0.7170],\n",
            "        [ 1.1432],\n",
            "        [ 5.0990],\n",
            "        [-9.1933],\n",
            "        [ 6.8783]], grad_fn=<AddBackward0>)\n",
            "Epoch 310/1000, Loss: 0.13630273938179016\n",
            "tensor([[-0.7189],\n",
            "        [ 1.1448],\n",
            "        [ 5.1013],\n",
            "        [-9.2061],\n",
            "        [ 6.8883]], grad_fn=<AddBackward0>)\n",
            "Epoch 311/1000, Loss: 0.1360994279384613\n",
            "tensor([[-0.7207],\n",
            "        [ 1.1463],\n",
            "        [ 5.1037],\n",
            "        [-9.2190],\n",
            "        [ 6.8983]], grad_fn=<AddBackward0>)\n",
            "Epoch 312/1000, Loss: 0.13589659333229065\n",
            "tensor([[-0.7226],\n",
            "        [ 1.1479],\n",
            "        [ 5.1060],\n",
            "        [-9.2318],\n",
            "        [ 6.9083]], grad_fn=<AddBackward0>)\n",
            "Epoch 313/1000, Loss: 0.1356944590806961\n",
            "tensor([[-0.7244],\n",
            "        [ 1.1495],\n",
            "        [ 5.1084],\n",
            "        [-9.2446],\n",
            "        [ 6.9183]], grad_fn=<AddBackward0>)\n",
            "Epoch 314/1000, Loss: 0.135492742061615\n",
            "tensor([[-0.7263],\n",
            "        [ 1.1510],\n",
            "        [ 5.1107],\n",
            "        [-9.2574],\n",
            "        [ 6.9283]], grad_fn=<AddBackward0>)\n",
            "Epoch 315/1000, Loss: 0.13529160618782043\n",
            "tensor([[-0.7282],\n",
            "        [ 1.1526],\n",
            "        [ 5.1131],\n",
            "        [-9.2702],\n",
            "        [ 6.9382]], grad_fn=<AddBackward0>)\n",
            "Epoch 316/1000, Loss: 0.13509097695350647\n",
            "tensor([[-0.7300],\n",
            "        [ 1.1541],\n",
            "        [ 5.1154],\n",
            "        [-9.2829],\n",
            "        [ 6.9482]], grad_fn=<AddBackward0>)\n",
            "Epoch 317/1000, Loss: 0.13489094376564026\n",
            "tensor([[-0.7318],\n",
            "        [ 1.1557],\n",
            "        [ 5.1177],\n",
            "        [-9.2956],\n",
            "        [ 6.9581]], grad_fn=<AddBackward0>)\n",
            "Epoch 318/1000, Loss: 0.13469144701957703\n",
            "tensor([[-0.7337],\n",
            "        [ 1.1573],\n",
            "        [ 5.1200],\n",
            "        [-9.3083],\n",
            "        [ 6.9680]], grad_fn=<AddBackward0>)\n",
            "Epoch 319/1000, Loss: 0.134492427110672\n",
            "tensor([[-0.7355],\n",
            "        [ 1.1588],\n",
            "        [ 5.1223],\n",
            "        [-9.3210],\n",
            "        [ 6.9779]], grad_fn=<AddBackward0>)\n",
            "Epoch 320/1000, Loss: 0.1342940330505371\n",
            "tensor([[-0.7374],\n",
            "        [ 1.1604],\n",
            "        [ 5.1246],\n",
            "        [-9.3337],\n",
            "        [ 6.9878]], grad_fn=<AddBackward0>)\n",
            "Epoch 321/1000, Loss: 0.1340959668159485\n",
            "tensor([[-0.7392],\n",
            "        [ 1.1619],\n",
            "        [ 5.1269],\n",
            "        [-9.3463],\n",
            "        [ 6.9977]], grad_fn=<AddBackward0>)\n",
            "Epoch 322/1000, Loss: 0.1338985711336136\n",
            "tensor([[-0.7410],\n",
            "        [ 1.1635],\n",
            "        [ 5.1292],\n",
            "        [-9.3590],\n",
            "        [ 7.0075]], grad_fn=<AddBackward0>)\n",
            "Epoch 323/1000, Loss: 0.13370166718959808\n",
            "tensor([[-0.7429],\n",
            "        [ 1.1650],\n",
            "        [ 5.1315],\n",
            "        [-9.3716],\n",
            "        [ 7.0174]], grad_fn=<AddBackward0>)\n",
            "Epoch 324/1000, Loss: 0.13350540399551392\n",
            "tensor([[-0.7447],\n",
            "        [ 1.1666],\n",
            "        [ 5.1338],\n",
            "        [-9.3842],\n",
            "        [ 7.0272]], grad_fn=<AddBackward0>)\n",
            "Epoch 325/1000, Loss: 0.1333095133304596\n",
            "tensor([[-0.7465],\n",
            "        [ 1.1681],\n",
            "        [ 5.1360],\n",
            "        [-9.3967],\n",
            "        [ 7.0370]], grad_fn=<AddBackward0>)\n",
            "Epoch 326/1000, Loss: 0.13311420381069183\n",
            "tensor([[-0.7483],\n",
            "        [ 1.1697],\n",
            "        [ 5.1383],\n",
            "        [-9.4093],\n",
            "        [ 7.0468]], grad_fn=<AddBackward0>)\n",
            "Epoch 327/1000, Loss: 0.13291935622692108\n",
            "tensor([[-0.7501],\n",
            "        [ 1.1712],\n",
            "        [ 5.1406],\n",
            "        [-9.4218],\n",
            "        [ 7.0566]], grad_fn=<AddBackward0>)\n",
            "Epoch 328/1000, Loss: 0.13272501528263092\n",
            "tensor([[-0.7520],\n",
            "        [ 1.1727],\n",
            "        [ 5.1428],\n",
            "        [-9.4343],\n",
            "        [ 7.0664]], grad_fn=<AddBackward0>)\n",
            "Epoch 329/1000, Loss: 0.13253115117549896\n",
            "tensor([[-0.7538],\n",
            "        [ 1.1743],\n",
            "        [ 5.1451],\n",
            "        [-9.4468],\n",
            "        [ 7.0761]], grad_fn=<AddBackward0>)\n",
            "Epoch 330/1000, Loss: 0.13233798742294312\n",
            "tensor([[-0.7556],\n",
            "        [ 1.1758],\n",
            "        [ 5.1473],\n",
            "        [-9.4593],\n",
            "        [ 7.0859]], grad_fn=<AddBackward0>)\n",
            "Epoch 331/1000, Loss: 0.13214519619941711\n",
            "tensor([[-0.7574],\n",
            "        [ 1.1774],\n",
            "        [ 5.1496],\n",
            "        [-9.4718],\n",
            "        [ 7.0956]], grad_fn=<AddBackward0>)\n",
            "Epoch 332/1000, Loss: 0.13195285201072693\n",
            "tensor([[-0.7592],\n",
            "        [ 1.1789],\n",
            "        [ 5.1518],\n",
            "        [-9.4842],\n",
            "        [ 7.1053]], grad_fn=<AddBackward0>)\n",
            "Epoch 333/1000, Loss: 0.1317610740661621\n",
            "tensor([[-0.7610],\n",
            "        [ 1.1804],\n",
            "        [ 5.1540],\n",
            "        [-9.4966],\n",
            "        [ 7.1150]], grad_fn=<AddBackward0>)\n",
            "Epoch 334/1000, Loss: 0.1315697729587555\n",
            "tensor([[-0.7628],\n",
            "        [ 1.1820],\n",
            "        [ 5.1563],\n",
            "        [-9.5090],\n",
            "        [ 7.1247]], grad_fn=<AddBackward0>)\n",
            "Epoch 335/1000, Loss: 0.13137903809547424\n",
            "tensor([[-0.7646],\n",
            "        [ 1.1835],\n",
            "        [ 5.1585],\n",
            "        [-9.5214],\n",
            "        [ 7.1344]], grad_fn=<AddBackward0>)\n",
            "Epoch 336/1000, Loss: 0.1311887502670288\n",
            "tensor([[-0.7664],\n",
            "        [ 1.1850],\n",
            "        [ 5.1607],\n",
            "        [-9.5338],\n",
            "        [ 7.1441]], grad_fn=<AddBackward0>)\n",
            "Epoch 337/1000, Loss: 0.1309989094734192\n",
            "tensor([[-0.7682],\n",
            "        [ 1.1866],\n",
            "        [ 5.1629],\n",
            "        [-9.5461],\n",
            "        [ 7.1538]], grad_fn=<AddBackward0>)\n",
            "Epoch 338/1000, Loss: 0.13080951571464539\n",
            "tensor([[-0.7700],\n",
            "        [ 1.1881],\n",
            "        [ 5.1651],\n",
            "        [-9.5585],\n",
            "        [ 7.1634]], grad_fn=<AddBackward0>)\n",
            "Epoch 339/1000, Loss: 0.13062076270580292\n",
            "tensor([[-0.7717],\n",
            "        [ 1.1896],\n",
            "        [ 5.1673],\n",
            "        [-9.5708],\n",
            "        [ 7.1730]], grad_fn=<AddBackward0>)\n",
            "Epoch 340/1000, Loss: 0.13043248653411865\n",
            "tensor([[-0.7735],\n",
            "        [ 1.1911],\n",
            "        [ 5.1695],\n",
            "        [-9.5831],\n",
            "        [ 7.1827]], grad_fn=<AddBackward0>)\n",
            "Epoch 341/1000, Loss: 0.13024455308914185\n",
            "tensor([[-0.7753],\n",
            "        [ 1.1927],\n",
            "        [ 5.1717],\n",
            "        [-9.5953],\n",
            "        [ 7.1923]], grad_fn=<AddBackward0>)\n",
            "Epoch 342/1000, Loss: 0.13005727529525757\n",
            "tensor([[-0.7771],\n",
            "        [ 1.1942],\n",
            "        [ 5.1739],\n",
            "        [-9.6076],\n",
            "        [ 7.2019]], grad_fn=<AddBackward0>)\n",
            "Epoch 343/1000, Loss: 0.12987029552459717\n",
            "tensor([[-0.7789],\n",
            "        [ 1.1957],\n",
            "        [ 5.1761],\n",
            "        [-9.6198],\n",
            "        [ 7.2114]], grad_fn=<AddBackward0>)\n",
            "Epoch 344/1000, Loss: 0.1296839565038681\n",
            "tensor([[-0.7806],\n",
            "        [ 1.1972],\n",
            "        [ 5.1783],\n",
            "        [-9.6321],\n",
            "        [ 7.2210]], grad_fn=<AddBackward0>)\n",
            "Epoch 345/1000, Loss: 0.12949809432029724\n",
            "tensor([[-0.7824],\n",
            "        [ 1.1987],\n",
            "        [ 5.1804],\n",
            "        [-9.6443],\n",
            "        [ 7.2306]], grad_fn=<AddBackward0>)\n",
            "Epoch 346/1000, Loss: 0.12931254506111145\n",
            "tensor([[-0.7842],\n",
            "        [ 1.2002],\n",
            "        [ 5.1826],\n",
            "        [-9.6564],\n",
            "        [ 7.2401]], grad_fn=<AddBackward0>)\n",
            "Epoch 347/1000, Loss: 0.12912753224372864\n",
            "tensor([[-0.7859],\n",
            "        [ 1.2018],\n",
            "        [ 5.1848],\n",
            "        [-9.6686],\n",
            "        [ 7.2496]], grad_fn=<AddBackward0>)\n",
            "Epoch 348/1000, Loss: 0.1289430558681488\n",
            "tensor([[-0.7877],\n",
            "        [ 1.2033],\n",
            "        [ 5.1869],\n",
            "        [-9.6808],\n",
            "        [ 7.2591]], grad_fn=<AddBackward0>)\n",
            "Epoch 349/1000, Loss: 0.1287590116262436\n",
            "tensor([[-0.7895],\n",
            "        [ 1.2048],\n",
            "        [ 5.1891],\n",
            "        [-9.6929],\n",
            "        [ 7.2686]], grad_fn=<AddBackward0>)\n",
            "Epoch 350/1000, Loss: 0.12857548892498016\n",
            "tensor([[-0.7912],\n",
            "        [ 1.2063],\n",
            "        [ 5.1912],\n",
            "        [-9.7050],\n",
            "        [ 7.2781]], grad_fn=<AddBackward0>)\n",
            "Epoch 351/1000, Loss: 0.12839233875274658\n",
            "tensor([[-0.7930],\n",
            "        [ 1.2078],\n",
            "        [ 5.1934],\n",
            "        [-9.7171],\n",
            "        [ 7.2876]], grad_fn=<AddBackward0>)\n",
            "Epoch 352/1000, Loss: 0.12820972502231598\n",
            "tensor([[-0.7947],\n",
            "        [ 1.2093],\n",
            "        [ 5.1955],\n",
            "        [-9.7292],\n",
            "        [ 7.2971]], grad_fn=<AddBackward0>)\n",
            "Epoch 353/1000, Loss: 0.12802760303020477\n",
            "tensor([[-0.7965],\n",
            "        [ 1.2108],\n",
            "        [ 5.1976],\n",
            "        [-9.7412],\n",
            "        [ 7.3065]], grad_fn=<AddBackward0>)\n",
            "Epoch 354/1000, Loss: 0.1278458833694458\n",
            "tensor([[-0.7982],\n",
            "        [ 1.2123],\n",
            "        [ 5.1998],\n",
            "        [-9.7533],\n",
            "        [ 7.3160]], grad_fn=<AddBackward0>)\n",
            "Epoch 355/1000, Loss: 0.12766465544700623\n",
            "tensor([[-0.8000],\n",
            "        [ 1.2138],\n",
            "        [ 5.2019],\n",
            "        [-9.7653],\n",
            "        [ 7.3254]], grad_fn=<AddBackward0>)\n",
            "Epoch 356/1000, Loss: 0.1274838149547577\n",
            "tensor([[-0.8017],\n",
            "        [ 1.2153],\n",
            "        [ 5.2040],\n",
            "        [-9.7773],\n",
            "        [ 7.3348]], grad_fn=<AddBackward0>)\n",
            "Epoch 357/1000, Loss: 0.12730351090431213\n",
            "tensor([[-0.8035],\n",
            "        [ 1.2168],\n",
            "        [ 5.2061],\n",
            "        [-9.7893],\n",
            "        [ 7.3442]], grad_fn=<AddBackward0>)\n",
            "Epoch 358/1000, Loss: 0.12712354958057404\n",
            "tensor([[-0.8052],\n",
            "        [ 1.2183],\n",
            "        [ 5.2082],\n",
            "        [-9.8013],\n",
            "        [ 7.3536]], grad_fn=<AddBackward0>)\n",
            "Epoch 359/1000, Loss: 0.1269441694021225\n",
            "tensor([[-0.8069],\n",
            "        [ 1.2198],\n",
            "        [ 5.2104],\n",
            "        [-9.8133],\n",
            "        [ 7.3630]], grad_fn=<AddBackward0>)\n",
            "Epoch 360/1000, Loss: 0.1267651617527008\n",
            "tensor([[-0.8087],\n",
            "        [ 1.2213],\n",
            "        [ 5.2125],\n",
            "        [-9.8252],\n",
            "        [ 7.3724]], grad_fn=<AddBackward0>)\n",
            "Epoch 361/1000, Loss: 0.12658663094043732\n",
            "tensor([[-0.8104],\n",
            "        [ 1.2228],\n",
            "        [ 5.2146],\n",
            "        [-9.8371],\n",
            "        [ 7.3817]], grad_fn=<AddBackward0>)\n",
            "Epoch 362/1000, Loss: 0.12640860676765442\n",
            "tensor([[-0.8121],\n",
            "        [ 1.2243],\n",
            "        [ 5.2166],\n",
            "        [-9.8490],\n",
            "        [ 7.3911]], grad_fn=<AddBackward0>)\n",
            "Epoch 363/1000, Loss: 0.1262308657169342\n",
            "tensor([[-0.8138],\n",
            "        [ 1.2258],\n",
            "        [ 5.2187],\n",
            "        [-9.8609],\n",
            "        [ 7.4004]], grad_fn=<AddBackward0>)\n",
            "Epoch 364/1000, Loss: 0.12605369091033936\n",
            "tensor([[-0.8156],\n",
            "        [ 1.2273],\n",
            "        [ 5.2208],\n",
            "        [-9.8728],\n",
            "        [ 7.4097]], grad_fn=<AddBackward0>)\n",
            "Epoch 365/1000, Loss: 0.1258770078420639\n",
            "tensor([[-0.8173],\n",
            "        [ 1.2287],\n",
            "        [ 5.2229],\n",
            "        [-9.8847],\n",
            "        [ 7.4190]], grad_fn=<AddBackward0>)\n",
            "Epoch 366/1000, Loss: 0.1257006675004959\n",
            "tensor([[-0.8190],\n",
            "        [ 1.2302],\n",
            "        [ 5.2250],\n",
            "        [-9.8965],\n",
            "        [ 7.4283]], grad_fn=<AddBackward0>)\n",
            "Epoch 367/1000, Loss: 0.12552474439144135\n",
            "tensor([[-0.8207],\n",
            "        [ 1.2317],\n",
            "        [ 5.2271],\n",
            "        [-9.9083],\n",
            "        [ 7.4376]], grad_fn=<AddBackward0>)\n",
            "Epoch 368/1000, Loss: 0.12534940242767334\n",
            "tensor([[-0.8224],\n",
            "        [ 1.2332],\n",
            "        [ 5.2291],\n",
            "        [-9.9201],\n",
            "        [ 7.4469]], grad_fn=<AddBackward0>)\n",
            "Epoch 369/1000, Loss: 0.125174418091774\n",
            "tensor([[-0.8241],\n",
            "        [ 1.2347],\n",
            "        [ 5.2312],\n",
            "        [-9.9319],\n",
            "        [ 7.4562]], grad_fn=<AddBackward0>)\n",
            "Epoch 370/1000, Loss: 0.12499977648258209\n",
            "tensor([[-0.8259],\n",
            "        [ 1.2362],\n",
            "        [ 5.2332],\n",
            "        [-9.9437],\n",
            "        [ 7.4654]], grad_fn=<AddBackward0>)\n",
            "Epoch 371/1000, Loss: 0.12482576072216034\n",
            "tensor([[-0.8276],\n",
            "        [ 1.2376],\n",
            "        [ 5.2353],\n",
            "        [-9.9555],\n",
            "        [ 7.4746]], grad_fn=<AddBackward0>)\n",
            "Epoch 372/1000, Loss: 0.12465199083089828\n",
            "tensor([[-0.8293],\n",
            "        [ 1.2391],\n",
            "        [ 5.2373],\n",
            "        [-9.9672],\n",
            "        [ 7.4839]], grad_fn=<AddBackward0>)\n",
            "Epoch 373/1000, Loss: 0.12447866052389145\n",
            "tensor([[-0.8310],\n",
            "        [ 1.2406],\n",
            "        [ 5.2394],\n",
            "        [-9.9789],\n",
            "        [ 7.4931]], grad_fn=<AddBackward0>)\n",
            "Epoch 374/1000, Loss: 0.12430588155984879\n",
            "tensor([[-0.8327],\n",
            "        [ 1.2421],\n",
            "        [ 5.2414],\n",
            "        [-9.9906],\n",
            "        [ 7.5023]], grad_fn=<AddBackward0>)\n",
            "Epoch 375/1000, Loss: 0.12413354963064194\n",
            "tensor([[ -0.8344],\n",
            "        [  1.2435],\n",
            "        [  5.2435],\n",
            "        [-10.0023],\n",
            "        [  7.5115]], grad_fn=<AddBackward0>)\n",
            "Epoch 376/1000, Loss: 0.12396140396595001\n",
            "tensor([[ -0.8361],\n",
            "        [  1.2450],\n",
            "        [  5.2455],\n",
            "        [-10.0140],\n",
            "        [  7.5207]], grad_fn=<AddBackward0>)\n",
            "Epoch 377/1000, Loss: 0.12378986179828644\n",
            "tensor([[ -0.8377],\n",
            "        [  1.2465],\n",
            "        [  5.2475],\n",
            "        [-10.0257],\n",
            "        [  7.5298]], grad_fn=<AddBackward0>)\n",
            "Epoch 378/1000, Loss: 0.1236187070608139\n",
            "tensor([[ -0.8394],\n",
            "        [  1.2479],\n",
            "        [  5.2496],\n",
            "        [-10.0373],\n",
            "        [  7.5390]], grad_fn=<AddBackward0>)\n",
            "Epoch 379/1000, Loss: 0.1234479695558548\n",
            "tensor([[ -0.8411],\n",
            "        [  1.2494],\n",
            "        [  5.2516],\n",
            "        [-10.0490],\n",
            "        [  7.5481]], grad_fn=<AddBackward0>)\n",
            "Epoch 380/1000, Loss: 0.12327772378921509\n",
            "tensor([[ -0.8428],\n",
            "        [  1.2509],\n",
            "        [  5.2536],\n",
            "        [-10.0606],\n",
            "        [  7.5572]], grad_fn=<AddBackward0>)\n",
            "Epoch 381/1000, Loss: 0.12310774624347687\n",
            "tensor([[ -0.8445],\n",
            "        [  1.2523],\n",
            "        [  5.2556],\n",
            "        [-10.0722],\n",
            "        [  7.5664]], grad_fn=<AddBackward0>)\n",
            "Epoch 382/1000, Loss: 0.12293833494186401\n",
            "tensor([[ -0.8462],\n",
            "        [  1.2538],\n",
            "        [  5.2576],\n",
            "        [-10.0838],\n",
            "        [  7.5755]], grad_fn=<AddBackward0>)\n",
            "Epoch 383/1000, Loss: 0.12276919931173325\n",
            "tensor([[ -0.8479],\n",
            "        [  1.2553],\n",
            "        [  5.2596],\n",
            "        [-10.0953],\n",
            "        [  7.5846]], grad_fn=<AddBackward0>)\n",
            "Epoch 384/1000, Loss: 0.12260055541992188\n",
            "tensor([[ -0.8495],\n",
            "        [  1.2567],\n",
            "        [  5.2616],\n",
            "        [-10.1069],\n",
            "        [  7.5937]], grad_fn=<AddBackward0>)\n",
            "Epoch 385/1000, Loss: 0.12243236601352692\n",
            "tensor([[ -0.8512],\n",
            "        [  1.2582],\n",
            "        [  5.2636],\n",
            "        [-10.1184],\n",
            "        [  7.6027]], grad_fn=<AddBackward0>)\n",
            "Epoch 386/1000, Loss: 0.122264564037323\n",
            "tensor([[ -0.8529],\n",
            "        [  1.2596],\n",
            "        [  5.2656],\n",
            "        [-10.1299],\n",
            "        [  7.6118]], grad_fn=<AddBackward0>)\n",
            "Epoch 387/1000, Loss: 0.12209706008434296\n",
            "tensor([[ -0.8545],\n",
            "        [  1.2611],\n",
            "        [  5.2676],\n",
            "        [-10.1414],\n",
            "        [  7.6209]], grad_fn=<AddBackward0>)\n",
            "Epoch 388/1000, Loss: 0.1219300776720047\n",
            "tensor([[ -0.8562],\n",
            "        [  1.2625],\n",
            "        [  5.2696],\n",
            "        [-10.1529],\n",
            "        [  7.6299]], grad_fn=<AddBackward0>)\n",
            "Epoch 389/1000, Loss: 0.1217634305357933\n",
            "tensor([[ -0.8579],\n",
            "        [  1.2640],\n",
            "        [  5.2716],\n",
            "        [-10.1644],\n",
            "        [  7.6389]], grad_fn=<AddBackward0>)\n",
            "Epoch 390/1000, Loss: 0.12159721553325653\n",
            "tensor([[ -0.8595],\n",
            "        [  1.2654],\n",
            "        [  5.2736],\n",
            "        [-10.1759],\n",
            "        [  7.6479]], grad_fn=<AddBackward0>)\n",
            "Epoch 391/1000, Loss: 0.12143142521381378\n",
            "tensor([[ -0.8612],\n",
            "        [  1.2669],\n",
            "        [  5.2755],\n",
            "        [-10.1873],\n",
            "        [  7.6569]], grad_fn=<AddBackward0>)\n",
            "Epoch 392/1000, Loss: 0.12126604467630386\n",
            "tensor([[ -0.8629],\n",
            "        [  1.2683],\n",
            "        [  5.2775],\n",
            "        [-10.1987],\n",
            "        [  7.6659]], grad_fn=<AddBackward0>)\n",
            "Epoch 393/1000, Loss: 0.1211010217666626\n",
            "tensor([[ -0.8645],\n",
            "        [  1.2698],\n",
            "        [  5.2795],\n",
            "        [-10.2101],\n",
            "        [  7.6749]], grad_fn=<AddBackward0>)\n",
            "Epoch 394/1000, Loss: 0.12093637138605118\n",
            "tensor([[ -0.8662],\n",
            "        [  1.2712],\n",
            "        [  5.2814],\n",
            "        [-10.2215],\n",
            "        [  7.6839]], grad_fn=<AddBackward0>)\n",
            "Epoch 395/1000, Loss: 0.12077218294143677\n",
            "tensor([[ -0.8678],\n",
            "        [  1.2727],\n",
            "        [  5.2834],\n",
            "        [-10.2329],\n",
            "        [  7.6929]], grad_fn=<AddBackward0>)\n",
            "Epoch 396/1000, Loss: 0.12060830742120743\n",
            "tensor([[ -0.8695],\n",
            "        [  1.2741],\n",
            "        [  5.2854],\n",
            "        [-10.2443],\n",
            "        [  7.7018]], grad_fn=<AddBackward0>)\n",
            "Epoch 397/1000, Loss: 0.12044493854045868\n",
            "tensor([[ -0.8711],\n",
            "        [  1.2756],\n",
            "        [  5.2873],\n",
            "        [-10.2556],\n",
            "        [  7.7108]], grad_fn=<AddBackward0>)\n",
            "Epoch 398/1000, Loss: 0.12028186023235321\n",
            "tensor([[ -0.8728],\n",
            "        [  1.2770],\n",
            "        [  5.2893],\n",
            "        [-10.2670],\n",
            "        [  7.7197]], grad_fn=<AddBackward0>)\n",
            "Epoch 399/1000, Loss: 0.12011925876140594\n",
            "tensor([[ -0.8744],\n",
            "        [  1.2784],\n",
            "        [  5.2912],\n",
            "        [-10.2783],\n",
            "        [  7.7286]], grad_fn=<AddBackward0>)\n",
            "Epoch 400/1000, Loss: 0.11995694786310196\n",
            "tensor([[ -0.8760],\n",
            "        [  1.2799],\n",
            "        [  5.2931],\n",
            "        [-10.2896],\n",
            "        [  7.7375]], grad_fn=<AddBackward0>)\n",
            "Epoch 401/1000, Loss: 0.11979514360427856\n",
            "tensor([[ -0.8777],\n",
            "        [  1.2813],\n",
            "        [  5.2951],\n",
            "        [-10.3009],\n",
            "        [  7.7464]], grad_fn=<AddBackward0>)\n",
            "Epoch 402/1000, Loss: 0.11963361501693726\n",
            "tensor([[ -0.8793],\n",
            "        [  1.2827],\n",
            "        [  5.2970],\n",
            "        [-10.3122],\n",
            "        [  7.7553]], grad_fn=<AddBackward0>)\n",
            "Epoch 403/1000, Loss: 0.11947252601385117\n",
            "tensor([[ -0.8810],\n",
            "        [  1.2842],\n",
            "        [  5.2989],\n",
            "        [-10.3234],\n",
            "        [  7.7642]], grad_fn=<AddBackward0>)\n",
            "Epoch 404/1000, Loss: 0.11931176483631134\n",
            "tensor([[ -0.8826],\n",
            "        [  1.2856],\n",
            "        [  5.3009],\n",
            "        [-10.3347],\n",
            "        [  7.7730]], grad_fn=<AddBackward0>)\n",
            "Epoch 405/1000, Loss: 0.1191515326499939\n",
            "tensor([[ -0.8842],\n",
            "        [  1.2870],\n",
            "        [  5.3028],\n",
            "        [-10.3459],\n",
            "        [  7.7819]], grad_fn=<AddBackward0>)\n",
            "Epoch 406/1000, Loss: 0.11899153143167496\n",
            "tensor([[ -0.8858],\n",
            "        [  1.2885],\n",
            "        [  5.3047],\n",
            "        [-10.3571],\n",
            "        [  7.7907]], grad_fn=<AddBackward0>)\n",
            "Epoch 407/1000, Loss: 0.11883201450109482\n",
            "tensor([[ -0.8875],\n",
            "        [  1.2899],\n",
            "        [  5.3066],\n",
            "        [-10.3683],\n",
            "        [  7.7996]], grad_fn=<AddBackward0>)\n",
            "Epoch 408/1000, Loss: 0.11867277324199677\n",
            "tensor([[ -0.8891],\n",
            "        [  1.2913],\n",
            "        [  5.3085],\n",
            "        [-10.3795],\n",
            "        [  7.8084]], grad_fn=<AddBackward0>)\n",
            "Epoch 409/1000, Loss: 0.11851402372121811\n",
            "tensor([[ -0.8907],\n",
            "        [  1.2927],\n",
            "        [  5.3104],\n",
            "        [-10.3907],\n",
            "        [  7.8172]], grad_fn=<AddBackward0>)\n",
            "Epoch 410/1000, Loss: 0.11835554987192154\n",
            "tensor([[ -0.8923],\n",
            "        [  1.2941],\n",
            "        [  5.3123],\n",
            "        [-10.4018],\n",
            "        [  7.8260]], grad_fn=<AddBackward0>)\n",
            "Epoch 411/1000, Loss: 0.11819752305746078\n",
            "tensor([[ -0.8940],\n",
            "        [  1.2956],\n",
            "        [  5.3142],\n",
            "        [-10.4130],\n",
            "        [  7.8348]], grad_fn=<AddBackward0>)\n",
            "Epoch 412/1000, Loss: 0.11803983151912689\n",
            "tensor([[ -0.8956],\n",
            "        [  1.2970],\n",
            "        [  5.3161],\n",
            "        [-10.4241],\n",
            "        [  7.8435]], grad_fn=<AddBackward0>)\n",
            "Epoch 413/1000, Loss: 0.11788241565227509\n",
            "tensor([[ -0.8972],\n",
            "        [  1.2984],\n",
            "        [  5.3180],\n",
            "        [-10.4352],\n",
            "        [  7.8523]], grad_fn=<AddBackward0>)\n",
            "Epoch 414/1000, Loss: 0.11772557348012924\n",
            "tensor([[ -0.8988],\n",
            "        [  1.2998],\n",
            "        [  5.3199],\n",
            "        [-10.4463],\n",
            "        [  7.8611]], grad_fn=<AddBackward0>)\n",
            "Epoch 415/1000, Loss: 0.11756891012191772\n",
            "tensor([[ -0.9004],\n",
            "        [  1.3012],\n",
            "        [  5.3218],\n",
            "        [-10.4574],\n",
            "        [  7.8698]], grad_fn=<AddBackward0>)\n",
            "Epoch 416/1000, Loss: 0.11741276830434799\n",
            "tensor([[ -0.9020],\n",
            "        [  1.3027],\n",
            "        [  5.3237],\n",
            "        [-10.4685],\n",
            "        [  7.8786]], grad_fn=<AddBackward0>)\n",
            "Epoch 417/1000, Loss: 0.1172569990158081\n",
            "tensor([[ -0.9036],\n",
            "        [  1.3041],\n",
            "        [  5.3256],\n",
            "        [-10.4795],\n",
            "        [  7.8873]], grad_fn=<AddBackward0>)\n",
            "Epoch 418/1000, Loss: 0.1171015128493309\n",
            "tensor([[ -0.9052],\n",
            "        [  1.3055],\n",
            "        [  5.3275],\n",
            "        [-10.4906],\n",
            "        [  7.8960]], grad_fn=<AddBackward0>)\n",
            "Epoch 419/1000, Loss: 0.11694648116827011\n",
            "tensor([[ -0.9068],\n",
            "        [  1.3069],\n",
            "        [  5.3293],\n",
            "        [-10.5016],\n",
            "        [  7.9047]], grad_fn=<AddBackward0>)\n",
            "Epoch 420/1000, Loss: 0.11679156869649887\n",
            "tensor([[ -0.9084],\n",
            "        [  1.3083],\n",
            "        [  5.3312],\n",
            "        [-10.5126],\n",
            "        [  7.9134]], grad_fn=<AddBackward0>)\n",
            "Epoch 421/1000, Loss: 0.1166372150182724\n",
            "tensor([[ -0.9100],\n",
            "        [  1.3097],\n",
            "        [  5.3331],\n",
            "        [-10.5236],\n",
            "        [  7.9221]], grad_fn=<AddBackward0>)\n",
            "Epoch 422/1000, Loss: 0.1164831891655922\n",
            "tensor([[ -0.9116],\n",
            "        [  1.3111],\n",
            "        [  5.3349],\n",
            "        [-10.5346],\n",
            "        [  7.9307]], grad_fn=<AddBackward0>)\n",
            "Epoch 423/1000, Loss: 0.11632957309484482\n",
            "tensor([[ -0.9132],\n",
            "        [  1.3125],\n",
            "        [  5.3368],\n",
            "        [-10.5456],\n",
            "        [  7.9394]], grad_fn=<AddBackward0>)\n",
            "Epoch 424/1000, Loss: 0.11617622524499893\n",
            "tensor([[ -0.9148],\n",
            "        [  1.3139],\n",
            "        [  5.3386],\n",
            "        [-10.5565],\n",
            "        [  7.9481]], grad_fn=<AddBackward0>)\n",
            "Epoch 425/1000, Loss: 0.11602328717708588\n",
            "tensor([[ -0.9164],\n",
            "        [  1.3153],\n",
            "        [  5.3405],\n",
            "        [-10.5675],\n",
            "        [  7.9567]], grad_fn=<AddBackward0>)\n",
            "Epoch 426/1000, Loss: 0.11587078869342804\n",
            "tensor([[ -0.9180],\n",
            "        [  1.3167],\n",
            "        [  5.3423],\n",
            "        [-10.5784],\n",
            "        [  7.9653]], grad_fn=<AddBackward0>)\n",
            "Epoch 427/1000, Loss: 0.11571848392486572\n",
            "tensor([[ -0.9195],\n",
            "        [  1.3181],\n",
            "        [  5.3442],\n",
            "        [-10.5893],\n",
            "        [  7.9739]], grad_fn=<AddBackward0>)\n",
            "Epoch 428/1000, Loss: 0.11556659638881683\n",
            "tensor([[ -0.9211],\n",
            "        [  1.3195],\n",
            "        [  5.3460],\n",
            "        [-10.6002],\n",
            "        [  7.9826]], grad_fn=<AddBackward0>)\n",
            "Epoch 429/1000, Loss: 0.11541511118412018\n",
            "tensor([[ -0.9227],\n",
            "        [  1.3209],\n",
            "        [  5.3479],\n",
            "        [-10.6111],\n",
            "        [  7.9912]], grad_fn=<AddBackward0>)\n",
            "Epoch 430/1000, Loss: 0.11526386439800262\n",
            "tensor([[ -0.9243],\n",
            "        [  1.3223],\n",
            "        [  5.3497],\n",
            "        [-10.6220],\n",
            "        [  7.9997]], grad_fn=<AddBackward0>)\n",
            "Epoch 431/1000, Loss: 0.1151130422949791\n",
            "tensor([[ -0.9259],\n",
            "        [  1.3237],\n",
            "        [  5.3515],\n",
            "        [-10.6328],\n",
            "        [  8.0083]], grad_fn=<AddBackward0>)\n",
            "Epoch 432/1000, Loss: 0.11496259272098541\n",
            "tensor([[ -0.9274],\n",
            "        [  1.3251],\n",
            "        [  5.3534],\n",
            "        [-10.6437],\n",
            "        [  8.0169]], grad_fn=<AddBackward0>)\n",
            "Epoch 433/1000, Loss: 0.114812470972538\n",
            "tensor([[ -0.9290],\n",
            "        [  1.3265],\n",
            "        [  5.3552],\n",
            "        [-10.6545],\n",
            "        [  8.0255]], grad_fn=<AddBackward0>)\n",
            "Epoch 434/1000, Loss: 0.11466264724731445\n",
            "tensor([[ -0.9306],\n",
            "        [  1.3279],\n",
            "        [  5.3570],\n",
            "        [-10.6653],\n",
            "        [  8.0340]], grad_fn=<AddBackward0>)\n",
            "Epoch 435/1000, Loss: 0.11451327800750732\n",
            "tensor([[ -0.9322],\n",
            "        [  1.3293],\n",
            "        [  5.3588],\n",
            "        [-10.6761],\n",
            "        [  8.0425]], grad_fn=<AddBackward0>)\n",
            "Epoch 436/1000, Loss: 0.11436406522989273\n",
            "tensor([[ -0.9337],\n",
            "        [  1.3306],\n",
            "        [  5.3607],\n",
            "        [-10.6869],\n",
            "        [  8.0511]], grad_fn=<AddBackward0>)\n",
            "Epoch 437/1000, Loss: 0.1142154186964035\n",
            "tensor([[ -0.9353],\n",
            "        [  1.3320],\n",
            "        [  5.3625],\n",
            "        [-10.6977],\n",
            "        [  8.0596]], grad_fn=<AddBackward0>)\n",
            "Epoch 438/1000, Loss: 0.11406701803207397\n",
            "tensor([[ -0.9368],\n",
            "        [  1.3334],\n",
            "        [  5.3643],\n",
            "        [-10.7085],\n",
            "        [  8.0681]], grad_fn=<AddBackward0>)\n",
            "Epoch 439/1000, Loss: 0.11391888558864594\n",
            "tensor([[ -0.9384],\n",
            "        [  1.3348],\n",
            "        [  5.3661],\n",
            "        [-10.7192],\n",
            "        [  8.0766]], grad_fn=<AddBackward0>)\n",
            "Epoch 440/1000, Loss: 0.11377118527889252\n",
            "tensor([[ -0.9400],\n",
            "        [  1.3362],\n",
            "        [  5.3679],\n",
            "        [-10.7300],\n",
            "        [  8.0851]], grad_fn=<AddBackward0>)\n",
            "Epoch 441/1000, Loss: 0.11362382024526596\n",
            "tensor([[ -0.9415],\n",
            "        [  1.3375],\n",
            "        [  5.3697],\n",
            "        [-10.7407],\n",
            "        [  8.0936]], grad_fn=<AddBackward0>)\n",
            "Epoch 442/1000, Loss: 0.11347673088312149\n",
            "tensor([[ -0.9431],\n",
            "        [  1.3389],\n",
            "        [  5.3715],\n",
            "        [-10.7514],\n",
            "        [  8.1020]], grad_fn=<AddBackward0>)\n",
            "Epoch 443/1000, Loss: 0.11332998424768448\n",
            "tensor([[ -0.9446],\n",
            "        [  1.3403],\n",
            "        [  5.3733],\n",
            "        [-10.7621],\n",
            "        [  8.1105]], grad_fn=<AddBackward0>)\n",
            "Epoch 444/1000, Loss: 0.11318367719650269\n",
            "tensor([[ -0.9462],\n",
            "        [  1.3417],\n",
            "        [  5.3751],\n",
            "        [-10.7728],\n",
            "        [  8.1189]], grad_fn=<AddBackward0>)\n",
            "Epoch 445/1000, Loss: 0.11303754895925522\n",
            "tensor([[ -0.9477],\n",
            "        [  1.3431],\n",
            "        [  5.3769],\n",
            "        [-10.7834],\n",
            "        [  8.1274]], grad_fn=<AddBackward0>)\n",
            "Epoch 446/1000, Loss: 0.11289187520742416\n",
            "tensor([[ -0.9493],\n",
            "        [  1.3444],\n",
            "        [  5.3787],\n",
            "        [-10.7941],\n",
            "        [  8.1358]], grad_fn=<AddBackward0>)\n",
            "Epoch 447/1000, Loss: 0.11274649202823639\n",
            "tensor([[ -0.9508],\n",
            "        [  1.3458],\n",
            "        [  5.3804],\n",
            "        [-10.8047],\n",
            "        [  8.1442]], grad_fn=<AddBackward0>)\n",
            "Epoch 448/1000, Loss: 0.11260141432285309\n",
            "tensor([[ -0.9524],\n",
            "        [  1.3472],\n",
            "        [  5.3822],\n",
            "        [-10.8154],\n",
            "        [  8.1526]], grad_fn=<AddBackward0>)\n",
            "Epoch 449/1000, Loss: 0.11245670169591904\n",
            "tensor([[ -0.9539],\n",
            "        [  1.3485],\n",
            "        [  5.3840],\n",
            "        [-10.8260],\n",
            "        [  8.1610]], grad_fn=<AddBackward0>)\n",
            "Epoch 450/1000, Loss: 0.11231233179569244\n",
            "tensor([[ -0.9554],\n",
            "        [  1.3499],\n",
            "        [  5.3858],\n",
            "        [-10.8366],\n",
            "        [  8.1694]], grad_fn=<AddBackward0>)\n",
            "Epoch 451/1000, Loss: 0.1121683269739151\n",
            "tensor([[ -0.9570],\n",
            "        [  1.3513],\n",
            "        [  5.3875],\n",
            "        [-10.8472],\n",
            "        [  8.1778]], grad_fn=<AddBackward0>)\n",
            "Epoch 452/1000, Loss: 0.11202456057071686\n",
            "tensor([[ -0.9585],\n",
            "        [  1.3526],\n",
            "        [  5.3893],\n",
            "        [-10.8578],\n",
            "        [  8.1862]], grad_fn=<AddBackward0>)\n",
            "Epoch 453/1000, Loss: 0.11188113689422607\n",
            "tensor([[ -0.9601],\n",
            "        [  1.3540],\n",
            "        [  5.3911],\n",
            "        [-10.8683],\n",
            "        [  8.1945]], grad_fn=<AddBackward0>)\n",
            "Epoch 454/1000, Loss: 0.1117381826043129\n",
            "tensor([[ -0.9616],\n",
            "        [  1.3554],\n",
            "        [  5.3928],\n",
            "        [-10.8789],\n",
            "        [  8.2029]], grad_fn=<AddBackward0>)\n",
            "Epoch 455/1000, Loss: 0.11159531772136688\n",
            "tensor([[ -0.9631],\n",
            "        [  1.3567],\n",
            "        [  5.3946],\n",
            "        [-10.8894],\n",
            "        [  8.2112]], grad_fn=<AddBackward0>)\n",
            "Epoch 456/1000, Loss: 0.11145280301570892\n",
            "tensor([[ -0.9646],\n",
            "        [  1.3581],\n",
            "        [  5.3964],\n",
            "        [-10.9000],\n",
            "        [  8.2196]], grad_fn=<AddBackward0>)\n",
            "Epoch 457/1000, Loss: 0.11131074279546738\n",
            "tensor([[ -0.9662],\n",
            "        [  1.3594],\n",
            "        [  5.3981],\n",
            "        [-10.9105],\n",
            "        [  8.2279]], grad_fn=<AddBackward0>)\n",
            "Epoch 458/1000, Loss: 0.11116902530193329\n",
            "tensor([[ -0.9677],\n",
            "        [  1.3608],\n",
            "        [  5.3999],\n",
            "        [-10.9210],\n",
            "        [  8.2362]], grad_fn=<AddBackward0>)\n",
            "Epoch 459/1000, Loss: 0.11102745682001114\n",
            "tensor([[ -0.9692],\n",
            "        [  1.3621],\n",
            "        [  5.4016],\n",
            "        [-10.9315],\n",
            "        [  8.2445]], grad_fn=<AddBackward0>)\n",
            "Epoch 460/1000, Loss: 0.110886350274086\n",
            "tensor([[ -0.9707],\n",
            "        [  1.3635],\n",
            "        [  5.4033],\n",
            "        [-10.9419],\n",
            "        [  8.2528]], grad_fn=<AddBackward0>)\n",
            "Epoch 461/1000, Loss: 0.11074544489383698\n",
            "tensor([[ -0.9723],\n",
            "        [  1.3649],\n",
            "        [  5.4051],\n",
            "        [-10.9524],\n",
            "        [  8.2611]], grad_fn=<AddBackward0>)\n",
            "Epoch 462/1000, Loss: 0.11060498654842377\n",
            "tensor([[ -0.9738],\n",
            "        [  1.3662],\n",
            "        [  5.4068],\n",
            "        [-10.9628],\n",
            "        [  8.2694]], grad_fn=<AddBackward0>)\n",
            "Epoch 463/1000, Loss: 0.11046472936868668\n",
            "tensor([[ -0.9753],\n",
            "        [  1.3676],\n",
            "        [  5.4086],\n",
            "        [-10.9733],\n",
            "        [  8.2776]], grad_fn=<AddBackward0>)\n",
            "Epoch 464/1000, Loss: 0.11032487452030182\n",
            "tensor([[ -0.9768],\n",
            "        [  1.3689],\n",
            "        [  5.4103],\n",
            "        [-10.9837],\n",
            "        [  8.2859]], grad_fn=<AddBackward0>)\n",
            "Epoch 465/1000, Loss: 0.11018536239862442\n",
            "tensor([[ -0.9783],\n",
            "        [  1.3703],\n",
            "        [  5.4120],\n",
            "        [-10.9941],\n",
            "        [  8.2941]], grad_fn=<AddBackward0>)\n",
            "Epoch 466/1000, Loss: 0.11004608869552612\n",
            "tensor([[ -0.9798],\n",
            "        [  1.3716],\n",
            "        [  5.4137],\n",
            "        [-11.0045],\n",
            "        [  8.3024]], grad_fn=<AddBackward0>)\n",
            "Epoch 467/1000, Loss: 0.10990717262029648\n",
            "tensor([[ -0.9813],\n",
            "        [  1.3729],\n",
            "        [  5.4155],\n",
            "        [-11.0149],\n",
            "        [  8.3106]], grad_fn=<AddBackward0>)\n",
            "Epoch 468/1000, Loss: 0.10976840555667877\n",
            "tensor([[ -0.9828],\n",
            "        [  1.3743],\n",
            "        [  5.4172],\n",
            "        [-11.0253],\n",
            "        [  8.3188]], grad_fn=<AddBackward0>)\n",
            "Epoch 469/1000, Loss: 0.10963008552789688\n",
            "tensor([[ -0.9843],\n",
            "        [  1.3756],\n",
            "        [  5.4189],\n",
            "        [-11.0356],\n",
            "        [  8.3270]], grad_fn=<AddBackward0>)\n",
            "Epoch 470/1000, Loss: 0.10949206352233887\n",
            "tensor([[ -0.9859],\n",
            "        [  1.3770],\n",
            "        [  5.4206],\n",
            "        [-11.0460],\n",
            "        [  8.3352]], grad_fn=<AddBackward0>)\n",
            "Epoch 471/1000, Loss: 0.10935433208942413\n",
            "tensor([[ -0.9874],\n",
            "        [  1.3783],\n",
            "        [  5.4223],\n",
            "        [-11.0563],\n",
            "        [  8.3434]], grad_fn=<AddBackward0>)\n",
            "Epoch 472/1000, Loss: 0.10921695083379745\n",
            "tensor([[ -0.9889],\n",
            "        [  1.3796],\n",
            "        [  5.4241],\n",
            "        [-11.0666],\n",
            "        [  8.3516]], grad_fn=<AddBackward0>)\n",
            "Epoch 473/1000, Loss: 0.10907985270023346\n",
            "tensor([[ -0.9903],\n",
            "        [  1.3810],\n",
            "        [  5.4258],\n",
            "        [-11.0770],\n",
            "        [  8.3598]], grad_fn=<AddBackward0>)\n",
            "Epoch 474/1000, Loss: 0.10894300788640976\n",
            "tensor([[ -0.9918],\n",
            "        [  1.3823],\n",
            "        [  5.4275],\n",
            "        [-11.0873],\n",
            "        [  8.3679]], grad_fn=<AddBackward0>)\n",
            "Epoch 475/1000, Loss: 0.10880646854639053\n",
            "tensor([[ -0.9933],\n",
            "        [  1.3837],\n",
            "        [  5.4292],\n",
            "        [-11.0975],\n",
            "        [  8.3761]], grad_fn=<AddBackward0>)\n",
            "Epoch 476/1000, Loss: 0.10867029428482056\n",
            "tensor([[ -0.9948],\n",
            "        [  1.3850],\n",
            "        [  5.4309],\n",
            "        [-11.1078],\n",
            "        [  8.3842]], grad_fn=<AddBackward0>)\n",
            "Epoch 477/1000, Loss: 0.10853441059589386\n",
            "tensor([[ -0.9963],\n",
            "        [  1.3863],\n",
            "        [  5.4326],\n",
            "        [-11.1181],\n",
            "        [  8.3924]], grad_fn=<AddBackward0>)\n",
            "Epoch 478/1000, Loss: 0.10839886963367462\n",
            "tensor([[ -0.9978],\n",
            "        [  1.3877],\n",
            "        [  5.4343],\n",
            "        [-11.1283],\n",
            "        [  8.4005]], grad_fn=<AddBackward0>)\n",
            "Epoch 479/1000, Loss: 0.10826344788074493\n",
            "tensor([[ -0.9993],\n",
            "        [  1.3890],\n",
            "        [  5.4359],\n",
            "        [-11.1386],\n",
            "        [  8.4086]], grad_fn=<AddBackward0>)\n",
            "Epoch 480/1000, Loss: 0.10812856256961823\n",
            "tensor([[ -1.0008],\n",
            "        [  1.3903],\n",
            "        [  5.4376],\n",
            "        [-11.1488],\n",
            "        [  8.4167]], grad_fn=<AddBackward0>)\n",
            "Epoch 481/1000, Loss: 0.10799375921487808\n",
            "tensor([[ -1.0023],\n",
            "        [  1.3916],\n",
            "        [  5.4393],\n",
            "        [-11.1590],\n",
            "        [  8.4248]], grad_fn=<AddBackward0>)\n",
            "Epoch 482/1000, Loss: 0.10785941034555435\n",
            "tensor([[ -1.0037],\n",
            "        [  1.3930],\n",
            "        [  5.4410],\n",
            "        [-11.1692],\n",
            "        [  8.4329]], grad_fn=<AddBackward0>)\n",
            "Epoch 483/1000, Loss: 0.10772530734539032\n",
            "tensor([[ -1.0052],\n",
            "        [  1.3943],\n",
            "        [  5.4427],\n",
            "        [-11.1794],\n",
            "        [  8.4410]], grad_fn=<AddBackward0>)\n",
            "Epoch 484/1000, Loss: 0.10759149491786957\n",
            "tensor([[ -1.0067],\n",
            "        [  1.3956],\n",
            "        [  5.4444],\n",
            "        [-11.1896],\n",
            "        [  8.4490]], grad_fn=<AddBackward0>)\n",
            "Epoch 485/1000, Loss: 0.10745792090892792\n",
            "tensor([[ -1.0082],\n",
            "        [  1.3969],\n",
            "        [  5.4460],\n",
            "        [-11.1997],\n",
            "        [  8.4571]], grad_fn=<AddBackward0>)\n",
            "Epoch 486/1000, Loss: 0.10732471942901611\n",
            "tensor([[ -1.0097],\n",
            "        [  1.3983],\n",
            "        [  5.4477],\n",
            "        [-11.2099],\n",
            "        [  8.4652]], grad_fn=<AddBackward0>)\n",
            "Epoch 487/1000, Loss: 0.10719175636768341\n",
            "tensor([[ -1.0111],\n",
            "        [  1.3996],\n",
            "        [  5.4494],\n",
            "        [-11.2200],\n",
            "        [  8.4732]], grad_fn=<AddBackward0>)\n",
            "Epoch 488/1000, Loss: 0.10705913603305817\n",
            "tensor([[ -1.0126],\n",
            "        [  1.4009],\n",
            "        [  5.4510],\n",
            "        [-11.2302],\n",
            "        [  8.4812]], grad_fn=<AddBackward0>)\n",
            "Epoch 489/1000, Loss: 0.10692683607339859\n",
            "tensor([[ -1.0141],\n",
            "        [  1.4022],\n",
            "        [  5.4527],\n",
            "        [-11.2403],\n",
            "        [  8.4893]], grad_fn=<AddBackward0>)\n",
            "Epoch 490/1000, Loss: 0.10679469257593155\n",
            "tensor([[ -1.0155],\n",
            "        [  1.4035],\n",
            "        [  5.4544],\n",
            "        [-11.2504],\n",
            "        [  8.4973]], grad_fn=<AddBackward0>)\n",
            "Epoch 491/1000, Loss: 0.10666298866271973\n",
            "tensor([[ -1.0170],\n",
            "        [  1.4048],\n",
            "        [  5.4560],\n",
            "        [-11.2605],\n",
            "        [  8.5053]], grad_fn=<AddBackward0>)\n",
            "Epoch 492/1000, Loss: 0.1065315380692482\n",
            "tensor([[ -1.0185],\n",
            "        [  1.4062],\n",
            "        [  5.4577],\n",
            "        [-11.2706],\n",
            "        [  8.5133]], grad_fn=<AddBackward0>)\n",
            "Epoch 493/1000, Loss: 0.10640034824609756\n",
            "tensor([[ -1.0199],\n",
            "        [  1.4075],\n",
            "        [  5.4593],\n",
            "        [-11.2806],\n",
            "        [  8.5213]], grad_fn=<AddBackward0>)\n",
            "Epoch 494/1000, Loss: 0.10626931488513947\n",
            "tensor([[ -1.0214],\n",
            "        [  1.4088],\n",
            "        [  5.4610],\n",
            "        [-11.2907],\n",
            "        [  8.5293]], grad_fn=<AddBackward0>)\n",
            "Epoch 495/1000, Loss: 0.10613878816366196\n",
            "tensor([[ -1.0228],\n",
            "        [  1.4101],\n",
            "        [  5.4626],\n",
            "        [-11.3007],\n",
            "        [  8.5372]], grad_fn=<AddBackward0>)\n",
            "Epoch 496/1000, Loss: 0.10600839555263519\n",
            "tensor([[ -1.0243],\n",
            "        [  1.4114],\n",
            "        [  5.4643],\n",
            "        [-11.3108],\n",
            "        [  8.5452]], grad_fn=<AddBackward0>)\n",
            "Epoch 497/1000, Loss: 0.10587833821773529\n",
            "tensor([[ -1.0258],\n",
            "        [  1.4127],\n",
            "        [  5.4659],\n",
            "        [-11.3208],\n",
            "        [  8.5531]], grad_fn=<AddBackward0>)\n",
            "Epoch 498/1000, Loss: 0.10574859380722046\n",
            "tensor([[ -1.0272],\n",
            "        [  1.4140],\n",
            "        [  5.4676],\n",
            "        [-11.3308],\n",
            "        [  8.5611]], grad_fn=<AddBackward0>)\n",
            "Epoch 499/1000, Loss: 0.10561917722225189\n",
            "tensor([[ -1.0287],\n",
            "        [  1.4153],\n",
            "        [  5.4692],\n",
            "        [-11.3408],\n",
            "        [  8.5690]], grad_fn=<AddBackward0>)\n",
            "Epoch 500/1000, Loss: 0.10548990964889526\n",
            "tensor([[ -1.0301],\n",
            "        [  1.4166],\n",
            "        [  5.4709],\n",
            "        [-11.3508],\n",
            "        [  8.5770]], grad_fn=<AddBackward0>)\n",
            "Epoch 501/1000, Loss: 0.10536102950572968\n",
            "tensor([[ -1.0316],\n",
            "        [  1.4179],\n",
            "        [  5.4725],\n",
            "        [-11.3608],\n",
            "        [  8.5849]], grad_fn=<AddBackward0>)\n",
            "Epoch 502/1000, Loss: 0.10523229837417603\n",
            "tensor([[ -1.0330],\n",
            "        [  1.4192],\n",
            "        [  5.4741],\n",
            "        [-11.3707],\n",
            "        [  8.5928]], grad_fn=<AddBackward0>)\n",
            "Epoch 503/1000, Loss: 0.10510393232107162\n",
            "tensor([[ -1.0345],\n",
            "        [  1.4205],\n",
            "        [  5.4757],\n",
            "        [-11.3807],\n",
            "        [  8.6007]], grad_fn=<AddBackward0>)\n",
            "Epoch 504/1000, Loss: 0.10497580468654633\n",
            "tensor([[ -1.0359],\n",
            "        [  1.4218],\n",
            "        [  5.4774],\n",
            "        [-11.3906],\n",
            "        [  8.6086]], grad_fn=<AddBackward0>)\n",
            "Epoch 505/1000, Loss: 0.10484810173511505\n",
            "tensor([[ -1.0373],\n",
            "        [  1.4231],\n",
            "        [  5.4790],\n",
            "        [-11.4005],\n",
            "        [  8.6165]], grad_fn=<AddBackward0>)\n",
            "Epoch 506/1000, Loss: 0.10472047328948975\n",
            "tensor([[ -1.0388],\n",
            "        [  1.4244],\n",
            "        [  5.4806],\n",
            "        [-11.4105],\n",
            "        [  8.6243]], grad_fn=<AddBackward0>)\n",
            "Epoch 507/1000, Loss: 0.10459326207637787\n",
            "tensor([[ -1.0402],\n",
            "        [  1.4257],\n",
            "        [  5.4822],\n",
            "        [-11.4204],\n",
            "        [  8.6322]], grad_fn=<AddBackward0>)\n",
            "Epoch 508/1000, Loss: 0.1044662594795227\n",
            "tensor([[ -1.0417],\n",
            "        [  1.4270],\n",
            "        [  5.4839],\n",
            "        [-11.4303],\n",
            "        [  8.6401]], grad_fn=<AddBackward0>)\n",
            "Epoch 509/1000, Loss: 0.10433949530124664\n",
            "tensor([[ -1.0431],\n",
            "        [  1.4283],\n",
            "        [  5.4855],\n",
            "        [-11.4401],\n",
            "        [  8.6479]], grad_fn=<AddBackward0>)\n",
            "Epoch 510/1000, Loss: 0.10421305894851685\n",
            "tensor([[ -1.0445],\n",
            "        [  1.4296],\n",
            "        [  5.4871],\n",
            "        [-11.4500],\n",
            "        [  8.6557]], grad_fn=<AddBackward0>)\n",
            "Epoch 511/1000, Loss: 0.10408680140972137\n",
            "tensor([[ -1.0460],\n",
            "        [  1.4309],\n",
            "        [  5.4887],\n",
            "        [-11.4599],\n",
            "        [  8.6636]], grad_fn=<AddBackward0>)\n",
            "Epoch 512/1000, Loss: 0.10396094620227814\n",
            "tensor([[ -1.0474],\n",
            "        [  1.4321],\n",
            "        [  5.4903],\n",
            "        [-11.4697],\n",
            "        [  8.6714]], grad_fn=<AddBackward0>)\n",
            "Epoch 513/1000, Loss: 0.10383538156747818\n",
            "tensor([[ -1.0488],\n",
            "        [  1.4334],\n",
            "        [  5.4919],\n",
            "        [-11.4796],\n",
            "        [  8.6792]], grad_fn=<AddBackward0>)\n",
            "Epoch 514/1000, Loss: 0.10370999574661255\n",
            "tensor([[ -1.0502],\n",
            "        [  1.4347],\n",
            "        [  5.4935],\n",
            "        [-11.4894],\n",
            "        [  8.6870]], grad_fn=<AddBackward0>)\n",
            "Epoch 515/1000, Loss: 0.10358484834432602\n",
            "tensor([[ -1.0517],\n",
            "        [  1.4360],\n",
            "        [  5.4951],\n",
            "        [-11.4992],\n",
            "        [  8.6948]], grad_fn=<AddBackward0>)\n",
            "Epoch 516/1000, Loss: 0.10346002876758575\n",
            "tensor([[ -1.0531],\n",
            "        [  1.4373],\n",
            "        [  5.4967],\n",
            "        [-11.5090],\n",
            "        [  8.7026]], grad_fn=<AddBackward0>)\n",
            "Epoch 517/1000, Loss: 0.10333548486232758\n",
            "tensor([[ -1.0545],\n",
            "        [  1.4386],\n",
            "        [  5.4983],\n",
            "        [-11.5188],\n",
            "        [  8.7104]], grad_fn=<AddBackward0>)\n",
            "Epoch 518/1000, Loss: 0.1032111868262291\n",
            "tensor([[ -1.0559],\n",
            "        [  1.4399],\n",
            "        [  5.4999],\n",
            "        [-11.5286],\n",
            "        [  8.7182]], grad_fn=<AddBackward0>)\n",
            "Epoch 519/1000, Loss: 0.10308720171451569\n",
            "tensor([[ -1.0573],\n",
            "        [  1.4412],\n",
            "        [  5.5015],\n",
            "        [-11.5383],\n",
            "        [  8.7260]], grad_fn=<AddBackward0>)\n",
            "Epoch 520/1000, Loss: 0.10296348482370377\n",
            "tensor([[ -1.0587],\n",
            "        [  1.4424],\n",
            "        [  5.5031],\n",
            "        [-11.5481],\n",
            "        [  8.7337]], grad_fn=<AddBackward0>)\n",
            "Epoch 521/1000, Loss: 0.10284000635147095\n",
            "tensor([[ -1.0602],\n",
            "        [  1.4437],\n",
            "        [  5.5047],\n",
            "        [-11.5578],\n",
            "        [  8.7415]], grad_fn=<AddBackward0>)\n",
            "Epoch 522/1000, Loss: 0.10271692276000977\n",
            "tensor([[ -1.0616],\n",
            "        [  1.4450],\n",
            "        [  5.5062],\n",
            "        [-11.5676],\n",
            "        [  8.7492]], grad_fn=<AddBackward0>)\n",
            "Epoch 523/1000, Loss: 0.10259391367435455\n",
            "tensor([[ -1.0630],\n",
            "        [  1.4463],\n",
            "        [  5.5078],\n",
            "        [-11.5773],\n",
            "        [  8.7569]], grad_fn=<AddBackward0>)\n",
            "Epoch 524/1000, Loss: 0.1024712324142456\n",
            "tensor([[ -1.0644],\n",
            "        [  1.4475],\n",
            "        [  5.5094],\n",
            "        [-11.5870],\n",
            "        [  8.7646]], grad_fn=<AddBackward0>)\n",
            "Epoch 525/1000, Loss: 0.10234875977039337\n",
            "tensor([[ -1.0658],\n",
            "        [  1.4488],\n",
            "        [  5.5110],\n",
            "        [-11.5967],\n",
            "        [  8.7724]], grad_fn=<AddBackward0>)\n",
            "Epoch 526/1000, Loss: 0.10222657769918442\n",
            "tensor([[ -1.0672],\n",
            "        [  1.4501],\n",
            "        [  5.5125],\n",
            "        [-11.6064],\n",
            "        [  8.7801]], grad_fn=<AddBackward0>)\n",
            "Epoch 527/1000, Loss: 0.10210468620061874\n",
            "tensor([[ -1.0686],\n",
            "        [  1.4513],\n",
            "        [  5.5141],\n",
            "        [-11.6161],\n",
            "        [  8.7878]], grad_fn=<AddBackward0>)\n",
            "Epoch 528/1000, Loss: 0.10198307037353516\n",
            "tensor([[ -1.0700],\n",
            "        [  1.4526],\n",
            "        [  5.5157],\n",
            "        [-11.6257],\n",
            "        [  8.7954]], grad_fn=<AddBackward0>)\n",
            "Epoch 529/1000, Loss: 0.10186165571212769\n",
            "tensor([[ -1.0714],\n",
            "        [  1.4539],\n",
            "        [  5.5172],\n",
            "        [-11.6354],\n",
            "        [  8.8031]], grad_fn=<AddBackward0>)\n",
            "Epoch 530/1000, Loss: 0.1017405167222023\n",
            "tensor([[ -1.0728],\n",
            "        [  1.4551],\n",
            "        [  5.5188],\n",
            "        [-11.6450],\n",
            "        [  8.8108]], grad_fn=<AddBackward0>)\n",
            "Epoch 531/1000, Loss: 0.1016196459531784\n",
            "tensor([[ -1.0742],\n",
            "        [  1.4564],\n",
            "        [  5.5204],\n",
            "        [-11.6547],\n",
            "        [  8.8185]], grad_fn=<AddBackward0>)\n",
            "Epoch 532/1000, Loss: 0.10149899870157242\n",
            "tensor([[ -1.0756],\n",
            "        [  1.4577],\n",
            "        [  5.5219],\n",
            "        [-11.6643],\n",
            "        [  8.8261]], grad_fn=<AddBackward0>)\n",
            "Epoch 533/1000, Loss: 0.1013786792755127\n",
            "tensor([[ -1.0770],\n",
            "        [  1.4589],\n",
            "        [  5.5235],\n",
            "        [-11.6739],\n",
            "        [  8.8338]], grad_fn=<AddBackward0>)\n",
            "Epoch 534/1000, Loss: 0.10125859826803207\n",
            "tensor([[ -1.0784],\n",
            "        [  1.4602],\n",
            "        [  5.5250],\n",
            "        [-11.6835],\n",
            "        [  8.8414]], grad_fn=<AddBackward0>)\n",
            "Epoch 535/1000, Loss: 0.10113873332738876\n",
            "tensor([[ -1.0798],\n",
            "        [  1.4615],\n",
            "        [  5.5266],\n",
            "        [-11.6931],\n",
            "        [  8.8490]], grad_fn=<AddBackward0>)\n",
            "Epoch 536/1000, Loss: 0.10101906210184097\n",
            "tensor([[ -1.0812],\n",
            "        [  1.4627],\n",
            "        [  5.5281],\n",
            "        [-11.7027],\n",
            "        [  8.8566]], grad_fn=<AddBackward0>)\n",
            "Epoch 537/1000, Loss: 0.10089977085590363\n",
            "tensor([[ -1.0826],\n",
            "        [  1.4640],\n",
            "        [  5.5297],\n",
            "        [-11.7123],\n",
            "        [  8.8643]], grad_fn=<AddBackward0>)\n",
            "Epoch 538/1000, Loss: 0.10078056156635284\n",
            "tensor([[ -1.0840],\n",
            "        [  1.4652],\n",
            "        [  5.5312],\n",
            "        [-11.7218],\n",
            "        [  8.8719]], grad_fn=<AddBackward0>)\n",
            "Epoch 539/1000, Loss: 0.1006617322564125\n",
            "tensor([[ -1.0854],\n",
            "        [  1.4665],\n",
            "        [  5.5328],\n",
            "        [-11.7314],\n",
            "        [  8.8795]], grad_fn=<AddBackward0>)\n",
            "Epoch 540/1000, Loss: 0.10054316371679306\n",
            "tensor([[ -1.0868],\n",
            "        [  1.4677],\n",
            "        [  5.5343],\n",
            "        [-11.7409],\n",
            "        [  8.8871]], grad_fn=<AddBackward0>)\n",
            "Epoch 541/1000, Loss: 0.10042472928762436\n",
            "tensor([[ -1.0881],\n",
            "        [  1.4690],\n",
            "        [  5.5358],\n",
            "        [-11.7504],\n",
            "        [  8.8946]], grad_fn=<AddBackward0>)\n",
            "Epoch 542/1000, Loss: 0.10030663013458252\n",
            "tensor([[ -1.0895],\n",
            "        [  1.4702],\n",
            "        [  5.5374],\n",
            "        [-11.7600],\n",
            "        [  8.9022]], grad_fn=<AddBackward0>)\n",
            "Epoch 543/1000, Loss: 0.10018886625766754\n",
            "tensor([[ -1.0909],\n",
            "        [  1.4715],\n",
            "        [  5.5389],\n",
            "        [-11.7695],\n",
            "        [  8.9098]], grad_fn=<AddBackward0>)\n",
            "Epoch 544/1000, Loss: 0.10007119178771973\n",
            "tensor([[ -1.0923],\n",
            "        [  1.4727],\n",
            "        [  5.5404],\n",
            "        [-11.7790],\n",
            "        [  8.9173]], grad_fn=<AddBackward0>)\n",
            "Epoch 545/1000, Loss: 0.0999537780880928\n",
            "tensor([[ -1.0937],\n",
            "        [  1.4740],\n",
            "        [  5.5420],\n",
            "        [-11.7885],\n",
            "        [  8.9249]], grad_fn=<AddBackward0>)\n",
            "Epoch 546/1000, Loss: 0.09983660280704498\n",
            "tensor([[ -1.0950],\n",
            "        [  1.4752],\n",
            "        [  5.5435],\n",
            "        [-11.7979],\n",
            "        [  8.9324]], grad_fn=<AddBackward0>)\n",
            "Epoch 547/1000, Loss: 0.09971980005502701\n",
            "tensor([[ -1.0964],\n",
            "        [  1.4765],\n",
            "        [  5.5450],\n",
            "        [-11.8074],\n",
            "        [  8.9400]], grad_fn=<AddBackward0>)\n",
            "Epoch 548/1000, Loss: 0.09960313141345978\n",
            "tensor([[ -1.0978],\n",
            "        [  1.4777],\n",
            "        [  5.5465],\n",
            "        [-11.8168],\n",
            "        [  8.9475]], grad_fn=<AddBackward0>)\n",
            "Epoch 549/1000, Loss: 0.09948677569627762\n",
            "tensor([[ -1.0992],\n",
            "        [  1.4790],\n",
            "        [  5.5481],\n",
            "        [-11.8263],\n",
            "        [  8.9550]], grad_fn=<AddBackward0>)\n",
            "Epoch 550/1000, Loss: 0.0993705540895462\n",
            "tensor([[ -1.1005],\n",
            "        [  1.4802],\n",
            "        [  5.5496],\n",
            "        [-11.8357],\n",
            "        [  8.9625]], grad_fn=<AddBackward0>)\n",
            "Epoch 551/1000, Loss: 0.0992545485496521\n",
            "tensor([[ -1.1019],\n",
            "        [  1.4815],\n",
            "        [  5.5511],\n",
            "        [-11.8452],\n",
            "        [  8.9700]], grad_fn=<AddBackward0>)\n",
            "Epoch 552/1000, Loss: 0.09913890063762665\n",
            "tensor([[ -1.1033],\n",
            "        [  1.4827],\n",
            "        [  5.5526],\n",
            "        [-11.8546],\n",
            "        [  8.9775]], grad_fn=<AddBackward0>)\n",
            "Epoch 553/1000, Loss: 0.0990234762430191\n",
            "tensor([[ -1.1046],\n",
            "        [  1.4839],\n",
            "        [  5.5541],\n",
            "        [-11.8640],\n",
            "        [  8.9850]], grad_fn=<AddBackward0>)\n",
            "Epoch 554/1000, Loss: 0.09890824556350708\n",
            "tensor([[ -1.1060],\n",
            "        [  1.4852],\n",
            "        [  5.5556],\n",
            "        [-11.8734],\n",
            "        [  8.9925]], grad_fn=<AddBackward0>)\n",
            "Epoch 555/1000, Loss: 0.09879334270954132\n",
            "tensor([[ -1.1074],\n",
            "        [  1.4864],\n",
            "        [  5.5571],\n",
            "        [-11.8827],\n",
            "        [  8.9999]], grad_fn=<AddBackward0>)\n",
            "Epoch 556/1000, Loss: 0.09867861866950989\n",
            "tensor([[ -1.1087],\n",
            "        [  1.4877],\n",
            "        [  5.5586],\n",
            "        [-11.8921],\n",
            "        [  9.0074]], grad_fn=<AddBackward0>)\n",
            "Epoch 557/1000, Loss: 0.09856408834457397\n",
            "tensor([[ -1.1101],\n",
            "        [  1.4889],\n",
            "        [  5.5601],\n",
            "        [-11.9015],\n",
            "        [  9.0148]], grad_fn=<AddBackward0>)\n",
            "Epoch 558/1000, Loss: 0.09844987094402313\n",
            "tensor([[ -1.1114],\n",
            "        [  1.4901],\n",
            "        [  5.5616],\n",
            "        [-11.9108],\n",
            "        [  9.0223]], grad_fn=<AddBackward0>)\n",
            "Epoch 559/1000, Loss: 0.09833580255508423\n",
            "tensor([[ -1.1128],\n",
            "        [  1.4913],\n",
            "        [  5.5631],\n",
            "        [-11.9202],\n",
            "        [  9.0297]], grad_fn=<AddBackward0>)\n",
            "Epoch 560/1000, Loss: 0.09822206199169159\n",
            "tensor([[ -1.1142],\n",
            "        [  1.4926],\n",
            "        [  5.5646],\n",
            "        [-11.9295],\n",
            "        [  9.0372]], grad_fn=<AddBackward0>)\n",
            "Epoch 561/1000, Loss: 0.09810857474803925\n",
            "tensor([[ -1.1155],\n",
            "        [  1.4938],\n",
            "        [  5.5661],\n",
            "        [-11.9388],\n",
            "        [  9.0446]], grad_fn=<AddBackward0>)\n",
            "Epoch 562/1000, Loss: 0.09799514710903168\n",
            "tensor([[ -1.1169],\n",
            "        [  1.4950],\n",
            "        [  5.5676],\n",
            "        [-11.9481],\n",
            "        [  9.0520]], grad_fn=<AddBackward0>)\n",
            "Epoch 563/1000, Loss: 0.09788204729557037\n",
            "tensor([[ -1.1182],\n",
            "        [  1.4963],\n",
            "        [  5.5691],\n",
            "        [-11.9574],\n",
            "        [  9.0594]], grad_fn=<AddBackward0>)\n",
            "Epoch 564/1000, Loss: 0.09776927530765533\n",
            "tensor([[ -1.1196],\n",
            "        [  1.4975],\n",
            "        [  5.5706],\n",
            "        [-11.9667],\n",
            "        [  9.0668]], grad_fn=<AddBackward0>)\n",
            "Epoch 565/1000, Loss: 0.09765654057264328\n",
            "tensor([[ -1.1209],\n",
            "        [  1.4987],\n",
            "        [  5.5721],\n",
            "        [-11.9760],\n",
            "        [  9.0742]], grad_fn=<AddBackward0>)\n",
            "Epoch 566/1000, Loss: 0.09754424542188644\n",
            "tensor([[ -1.1223],\n",
            "        [  1.4999],\n",
            "        [  5.5736],\n",
            "        [-11.9853],\n",
            "        [  9.0816]], grad_fn=<AddBackward0>)\n",
            "Epoch 567/1000, Loss: 0.09743201732635498\n",
            "tensor([[ -1.1236],\n",
            "        [  1.5012],\n",
            "        [  5.5751],\n",
            "        [-11.9945],\n",
            "        [  9.0890]], grad_fn=<AddBackward0>)\n",
            "Epoch 568/1000, Loss: 0.097320057451725\n",
            "tensor([[ -1.1249],\n",
            "        [  1.5024],\n",
            "        [  5.5765],\n",
            "        [-12.0038],\n",
            "        [  9.0963]], grad_fn=<AddBackward0>)\n",
            "Epoch 569/1000, Loss: 0.0972084254026413\n",
            "tensor([[ -1.1263],\n",
            "        [  1.5036],\n",
            "        [  5.5780],\n",
            "        [-12.0130],\n",
            "        [  9.1037]], grad_fn=<AddBackward0>)\n",
            "Epoch 570/1000, Loss: 0.09709693491458893\n",
            "tensor([[ -1.1276],\n",
            "        [  1.5048],\n",
            "        [  5.5795],\n",
            "        [-12.0223],\n",
            "        [  9.1111]], grad_fn=<AddBackward0>)\n",
            "Epoch 571/1000, Loss: 0.09698565304279327\n",
            "tensor([[ -1.1290],\n",
            "        [  1.5060],\n",
            "        [  5.5810],\n",
            "        [-12.0315],\n",
            "        [  9.1184]], grad_fn=<AddBackward0>)\n",
            "Epoch 572/1000, Loss: 0.09687468409538269\n",
            "tensor([[ -1.1303],\n",
            "        [  1.5073],\n",
            "        [  5.5824],\n",
            "        [-12.0407],\n",
            "        [  9.1257]], grad_fn=<AddBackward0>)\n",
            "Epoch 573/1000, Loss: 0.09676390141248703\n",
            "tensor([[ -1.1316],\n",
            "        [  1.5085],\n",
            "        [  5.5839],\n",
            "        [-12.0499],\n",
            "        [  9.1331]], grad_fn=<AddBackward0>)\n",
            "Epoch 574/1000, Loss: 0.09665323793888092\n",
            "tensor([[ -1.1330],\n",
            "        [  1.5097],\n",
            "        [  5.5854],\n",
            "        [-12.0591],\n",
            "        [  9.1404]], grad_fn=<AddBackward0>)\n",
            "Epoch 575/1000, Loss: 0.09654297679662704\n",
            "tensor([[ -1.1343],\n",
            "        [  1.5109],\n",
            "        [  5.5868],\n",
            "        [-12.0683],\n",
            "        [  9.1477]], grad_fn=<AddBackward0>)\n",
            "Epoch 576/1000, Loss: 0.09643281996250153\n",
            "tensor([[ -1.1356],\n",
            "        [  1.5121],\n",
            "        [  5.5883],\n",
            "        [-12.0774],\n",
            "        [  9.1550]], grad_fn=<AddBackward0>)\n",
            "Epoch 577/1000, Loss: 0.09632301330566406\n",
            "tensor([[ -1.1370],\n",
            "        [  1.5133],\n",
            "        [  5.5898],\n",
            "        [-12.0866],\n",
            "        [  9.1623]], grad_fn=<AddBackward0>)\n",
            "Epoch 578/1000, Loss: 0.09621322900056839\n",
            "tensor([[ -1.1383],\n",
            "        [  1.5145],\n",
            "        [  5.5912],\n",
            "        [-12.0957],\n",
            "        [  9.1696]], grad_fn=<AddBackward0>)\n",
            "Epoch 579/1000, Loss: 0.09610382467508316\n",
            "tensor([[ -1.1396],\n",
            "        [  1.5158],\n",
            "        [  5.5927],\n",
            "        [-12.1049],\n",
            "        [  9.1769]], grad_fn=<AddBackward0>)\n",
            "Epoch 580/1000, Loss: 0.09599454700946808\n",
            "tensor([[ -1.1410],\n",
            "        [  1.5170],\n",
            "        [  5.5941],\n",
            "        [-12.1140],\n",
            "        [  9.1842]], grad_fn=<AddBackward0>)\n",
            "Epoch 581/1000, Loss: 0.09588556736707687\n",
            "tensor([[ -1.1423],\n",
            "        [  1.5182],\n",
            "        [  5.5956],\n",
            "        [-12.1231],\n",
            "        [  9.1914]], grad_fn=<AddBackward0>)\n",
            "Epoch 582/1000, Loss: 0.09577679634094238\n",
            "tensor([[ -1.1436],\n",
            "        [  1.5194],\n",
            "        [  5.5971],\n",
            "        [-12.1322],\n",
            "        [  9.1987]], grad_fn=<AddBackward0>)\n",
            "Epoch 583/1000, Loss: 0.0956682562828064\n",
            "tensor([[ -1.1449],\n",
            "        [  1.5206],\n",
            "        [  5.5985],\n",
            "        [-12.1413],\n",
            "        [  9.2060]], grad_fn=<AddBackward0>)\n",
            "Epoch 584/1000, Loss: 0.09555990248918533\n",
            "tensor([[ -1.1463],\n",
            "        [  1.5218],\n",
            "        [  5.6000],\n",
            "        [-12.1504],\n",
            "        [  9.2132]], grad_fn=<AddBackward0>)\n",
            "Epoch 585/1000, Loss: 0.09545178711414337\n",
            "tensor([[ -1.1476],\n",
            "        [  1.5230],\n",
            "        [  5.6014],\n",
            "        [-12.1595],\n",
            "        [  9.2205]], grad_fn=<AddBackward0>)\n",
            "Epoch 586/1000, Loss: 0.09534381330013275\n",
            "tensor([[ -1.1489],\n",
            "        [  1.5242],\n",
            "        [  5.6028],\n",
            "        [-12.1686],\n",
            "        [  9.2277]], grad_fn=<AddBackward0>)\n",
            "Epoch 587/1000, Loss: 0.09523617476224899\n",
            "tensor([[ -1.1502],\n",
            "        [  1.5254],\n",
            "        [  5.6043],\n",
            "        [-12.1777],\n",
            "        [  9.2349]], grad_fn=<AddBackward0>)\n",
            "Epoch 588/1000, Loss: 0.09512864053249359\n",
            "tensor([[ -1.1515],\n",
            "        [  1.5266],\n",
            "        [  5.6057],\n",
            "        [-12.1867],\n",
            "        [  9.2421]], grad_fn=<AddBackward0>)\n",
            "Epoch 589/1000, Loss: 0.09502138197422028\n",
            "tensor([[ -1.1528],\n",
            "        [  1.5278],\n",
            "        [  5.6072],\n",
            "        [-12.1958],\n",
            "        [  9.2493]], grad_fn=<AddBackward0>)\n",
            "Epoch 590/1000, Loss: 0.09491434693336487\n",
            "tensor([[ -1.1541],\n",
            "        [  1.5290],\n",
            "        [  5.6086],\n",
            "        [-12.2048],\n",
            "        [  9.2566]], grad_fn=<AddBackward0>)\n",
            "Epoch 591/1000, Loss: 0.09480752795934677\n",
            "tensor([[ -1.1555],\n",
            "        [  1.5302],\n",
            "        [  5.6100],\n",
            "        [-12.2138],\n",
            "        [  9.2638]], grad_fn=<AddBackward0>)\n",
            "Epoch 592/1000, Loss: 0.0947008728981018\n",
            "tensor([[ -1.1568],\n",
            "        [  1.5314],\n",
            "        [  5.6115],\n",
            "        [-12.2228],\n",
            "        [  9.2709]], grad_fn=<AddBackward0>)\n",
            "Epoch 593/1000, Loss: 0.09459453076124191\n",
            "tensor([[ -1.1581],\n",
            "        [  1.5326],\n",
            "        [  5.6129],\n",
            "        [-12.2318],\n",
            "        [  9.2781]], grad_fn=<AddBackward0>)\n",
            "Epoch 594/1000, Loss: 0.09448836743831635\n",
            "tensor([[ -1.1594],\n",
            "        [  1.5338],\n",
            "        [  5.6143],\n",
            "        [-12.2408],\n",
            "        [  9.2853]], grad_fn=<AddBackward0>)\n",
            "Epoch 595/1000, Loss: 0.09438234567642212\n",
            "tensor([[ -1.1607],\n",
            "        [  1.5350],\n",
            "        [  5.6158],\n",
            "        [-12.2498],\n",
            "        [  9.2925]], grad_fn=<AddBackward0>)\n",
            "Epoch 596/1000, Loss: 0.09427663683891296\n",
            "tensor([[ -1.1620],\n",
            "        [  1.5361],\n",
            "        [  5.6172],\n",
            "        [-12.2588],\n",
            "        [  9.2996]], grad_fn=<AddBackward0>)\n",
            "Epoch 597/1000, Loss: 0.09417100995779037\n",
            "tensor([[ -1.1633],\n",
            "        [  1.5373],\n",
            "        [  5.6186],\n",
            "        [-12.2678],\n",
            "        [  9.3068]], grad_fn=<AddBackward0>)\n",
            "Epoch 598/1000, Loss: 0.09406568855047226\n",
            "tensor([[ -1.1646],\n",
            "        [  1.5385],\n",
            "        [  5.6200],\n",
            "        [-12.2767],\n",
            "        [  9.3139]], grad_fn=<AddBackward0>)\n",
            "Epoch 599/1000, Loss: 0.09396056085824966\n",
            "tensor([[ -1.1659],\n",
            "        [  1.5397],\n",
            "        [  5.6215],\n",
            "        [-12.2857],\n",
            "        [  9.3211]], grad_fn=<AddBackward0>)\n",
            "Epoch 600/1000, Loss: 0.0938556119799614\n",
            "tensor([[ -1.1672],\n",
            "        [  1.5409],\n",
            "        [  5.6229],\n",
            "        [-12.2946],\n",
            "        [  9.3282]], grad_fn=<AddBackward0>)\n",
            "Epoch 601/1000, Loss: 0.09375093132257462\n",
            "tensor([[ -1.1685],\n",
            "        [  1.5421],\n",
            "        [  5.6243],\n",
            "        [-12.3035],\n",
            "        [  9.3353]], grad_fn=<AddBackward0>)\n",
            "Epoch 602/1000, Loss: 0.09364642947912216\n",
            "tensor([[ -1.1698],\n",
            "        [  1.5433],\n",
            "        [  5.6257],\n",
            "        [-12.3125],\n",
            "        [  9.3424]], grad_fn=<AddBackward0>)\n",
            "Epoch 603/1000, Loss: 0.09354208409786224\n",
            "tensor([[ -1.1711],\n",
            "        [  1.5445],\n",
            "        [  5.6271],\n",
            "        [-12.3214],\n",
            "        [  9.3496]], grad_fn=<AddBackward0>)\n",
            "Epoch 604/1000, Loss: 0.09343800693750381\n",
            "tensor([[ -1.1724],\n",
            "        [  1.5456],\n",
            "        [  5.6285],\n",
            "        [-12.3303],\n",
            "        [  9.3567]], grad_fn=<AddBackward0>)\n",
            "Epoch 605/1000, Loss: 0.0933341383934021\n",
            "tensor([[ -1.1737],\n",
            "        [  1.5468],\n",
            "        [  5.6299],\n",
            "        [-12.3392],\n",
            "        [  9.3638]], grad_fn=<AddBackward0>)\n",
            "Epoch 606/1000, Loss: 0.09323045611381531\n",
            "tensor([[ -1.1750],\n",
            "        [  1.5480],\n",
            "        [  5.6314],\n",
            "        [-12.3481],\n",
            "        [  9.3708]], grad_fn=<AddBackward0>)\n",
            "Epoch 607/1000, Loss: 0.09312692284584045\n",
            "tensor([[ -1.1762],\n",
            "        [  1.5492],\n",
            "        [  5.6328],\n",
            "        [-12.3569],\n",
            "        [  9.3779]], grad_fn=<AddBackward0>)\n",
            "Epoch 608/1000, Loss: 0.09302371740341187\n",
            "tensor([[ -1.1775],\n",
            "        [  1.5504],\n",
            "        [  5.6342],\n",
            "        [-12.3658],\n",
            "        [  9.3850]], grad_fn=<AddBackward0>)\n",
            "Epoch 609/1000, Loss: 0.0929206907749176\n",
            "tensor([[ -1.1788],\n",
            "        [  1.5515],\n",
            "        [  5.6356],\n",
            "        [-12.3747],\n",
            "        [  9.3921]], grad_fn=<AddBackward0>)\n",
            "Epoch 610/1000, Loss: 0.09281782805919647\n",
            "tensor([[ -1.1801],\n",
            "        [  1.5527],\n",
            "        [  5.6370],\n",
            "        [-12.3835],\n",
            "        [  9.3991]], grad_fn=<AddBackward0>)\n",
            "Epoch 611/1000, Loss: 0.09271515160799026\n",
            "tensor([[ -1.1814],\n",
            "        [  1.5539],\n",
            "        [  5.6384],\n",
            "        [-12.3923],\n",
            "        [  9.4062]], grad_fn=<AddBackward0>)\n",
            "Epoch 612/1000, Loss: 0.09261266887187958\n",
            "tensor([[ -1.1827],\n",
            "        [  1.5551],\n",
            "        [  5.6398],\n",
            "        [-12.4012],\n",
            "        [  9.4132]], grad_fn=<AddBackward0>)\n",
            "Epoch 613/1000, Loss: 0.0925104022026062\n",
            "tensor([[ -1.1840],\n",
            "        [  1.5562],\n",
            "        [  5.6412],\n",
            "        [-12.4100],\n",
            "        [  9.4203]], grad_fn=<AddBackward0>)\n",
            "Epoch 614/1000, Loss: 0.0924084335565567\n",
            "tensor([[ -1.1852],\n",
            "        [  1.5574],\n",
            "        [  5.6425],\n",
            "        [-12.4188],\n",
            "        [  9.4273]], grad_fn=<AddBackward0>)\n",
            "Epoch 615/1000, Loss: 0.0923064798116684\n",
            "tensor([[ -1.1865],\n",
            "        [  1.5586],\n",
            "        [  5.6439],\n",
            "        [-12.4276],\n",
            "        [  9.4343]], grad_fn=<AddBackward0>)\n",
            "Epoch 616/1000, Loss: 0.09220487624406815\n",
            "tensor([[ -1.1878],\n",
            "        [  1.5597],\n",
            "        [  5.6453],\n",
            "        [-12.4364],\n",
            "        [  9.4413]], grad_fn=<AddBackward0>)\n",
            "Epoch 617/1000, Loss: 0.09210339188575745\n",
            "tensor([[ -1.1891],\n",
            "        [  1.5609],\n",
            "        [  5.6467],\n",
            "        [-12.4452],\n",
            "        [  9.4484]], grad_fn=<AddBackward0>)\n",
            "Epoch 618/1000, Loss: 0.09200215339660645\n",
            "tensor([[ -1.1903],\n",
            "        [  1.5621],\n",
            "        [  5.6481],\n",
            "        [-12.4539],\n",
            "        [  9.4554]], grad_fn=<AddBackward0>)\n",
            "Epoch 619/1000, Loss: 0.09190113842487335\n",
            "tensor([[ -1.1916],\n",
            "        [  1.5632],\n",
            "        [  5.6495],\n",
            "        [-12.4627],\n",
            "        [  9.4624]], grad_fn=<AddBackward0>)\n",
            "Epoch 620/1000, Loss: 0.0918002799153328\n",
            "tensor([[ -1.1929],\n",
            "        [  1.5644],\n",
            "        [  5.6509],\n",
            "        [-12.4715],\n",
            "        [  9.4693]], grad_fn=<AddBackward0>)\n",
            "Epoch 621/1000, Loss: 0.09169955551624298\n",
            "tensor([[ -1.1942],\n",
            "        [  1.5656],\n",
            "        [  5.6523],\n",
            "        [-12.4802],\n",
            "        [  9.4763]], grad_fn=<AddBackward0>)\n",
            "Epoch 622/1000, Loss: 0.09159915149211884\n",
            "tensor([[ -1.1954],\n",
            "        [  1.5667],\n",
            "        [  5.6536],\n",
            "        [-12.4889],\n",
            "        [  9.4833]], grad_fn=<AddBackward0>)\n",
            "Epoch 623/1000, Loss: 0.09149894118309021\n",
            "tensor([[ -1.1967],\n",
            "        [  1.5679],\n",
            "        [  5.6550],\n",
            "        [-12.4977],\n",
            "        [  9.4903]], grad_fn=<AddBackward0>)\n",
            "Epoch 624/1000, Loss: 0.09139890968799591\n",
            "tensor([[ -1.1980],\n",
            "        [  1.5691],\n",
            "        [  5.6564],\n",
            "        [-12.5064],\n",
            "        [  9.4972]], grad_fn=<AddBackward0>)\n",
            "Epoch 625/1000, Loss: 0.09129906445741653\n",
            "tensor([[ -1.1992],\n",
            "        [  1.5702],\n",
            "        [  5.6578],\n",
            "        [-12.5151],\n",
            "        [  9.5042]], grad_fn=<AddBackward0>)\n",
            "Epoch 626/1000, Loss: 0.09119930863380432\n",
            "tensor([[ -1.2005],\n",
            "        [  1.5714],\n",
            "        [  5.6591],\n",
            "        [-12.5238],\n",
            "        [  9.5111]], grad_fn=<AddBackward0>)\n",
            "Epoch 627/1000, Loss: 0.09109990298748016\n",
            "tensor([[ -1.2018],\n",
            "        [  1.5725],\n",
            "        [  5.6605],\n",
            "        [-12.5325],\n",
            "        [  9.5181]], grad_fn=<AddBackward0>)\n",
            "Epoch 628/1000, Loss: 0.09100053459405899\n",
            "tensor([[ -1.2030],\n",
            "        [  1.5737],\n",
            "        [  5.6619],\n",
            "        [-12.5412],\n",
            "        [  9.5250]], grad_fn=<AddBackward0>)\n",
            "Epoch 629/1000, Loss: 0.09090152382850647\n",
            "tensor([[ -1.2043],\n",
            "        [  1.5748],\n",
            "        [  5.6632],\n",
            "        [-12.5499],\n",
            "        [  9.5319]], grad_fn=<AddBackward0>)\n",
            "Epoch 630/1000, Loss: 0.09080260246992111\n",
            "tensor([[ -1.2055],\n",
            "        [  1.5760],\n",
            "        [  5.6646],\n",
            "        [-12.5585],\n",
            "        [  9.5389]], grad_fn=<AddBackward0>)\n",
            "Epoch 631/1000, Loss: 0.09070383012294769\n",
            "tensor([[ -1.2068],\n",
            "        [  1.5772],\n",
            "        [  5.6660],\n",
            "        [-12.5672],\n",
            "        [  9.5458]], grad_fn=<AddBackward0>)\n",
            "Epoch 632/1000, Loss: 0.09060539305210114\n",
            "tensor([[ -1.2080],\n",
            "        [  1.5783],\n",
            "        [  5.6673],\n",
            "        [-12.5758],\n",
            "        [  9.5527]], grad_fn=<AddBackward0>)\n",
            "Epoch 633/1000, Loss: 0.09050700813531876\n",
            "tensor([[ -1.2093],\n",
            "        [  1.5795],\n",
            "        [  5.6687],\n",
            "        [-12.5845],\n",
            "        [  9.5596]], grad_fn=<AddBackward0>)\n",
            "Epoch 634/1000, Loss: 0.09040901064872742\n",
            "tensor([[ -1.2106],\n",
            "        [  1.5806],\n",
            "        [  5.6700],\n",
            "        [-12.5931],\n",
            "        [  9.5665]], grad_fn=<AddBackward0>)\n",
            "Epoch 635/1000, Loss: 0.09031103551387787\n",
            "tensor([[ -1.2118],\n",
            "        [  1.5818],\n",
            "        [  5.6714],\n",
            "        [-12.6017],\n",
            "        [  9.5734]], grad_fn=<AddBackward0>)\n",
            "Epoch 636/1000, Loss: 0.09021320194005966\n",
            "tensor([[ -1.2131],\n",
            "        [  1.5829],\n",
            "        [  5.6728],\n",
            "        [-12.6103],\n",
            "        [  9.5803]], grad_fn=<AddBackward0>)\n",
            "Epoch 637/1000, Loss: 0.09011567384004593\n",
            "tensor([[ -1.2143],\n",
            "        [  1.5841],\n",
            "        [  5.6741],\n",
            "        [-12.6189],\n",
            "        [  9.5871]], grad_fn=<AddBackward0>)\n",
            "Epoch 638/1000, Loss: 0.0900183767080307\n",
            "tensor([[ -1.2156],\n",
            "        [  1.5852],\n",
            "        [  5.6755],\n",
            "        [-12.6275],\n",
            "        [  9.5940]], grad_fn=<AddBackward0>)\n",
            "Epoch 639/1000, Loss: 0.08992110192775726\n",
            "tensor([[ -1.2168],\n",
            "        [  1.5863],\n",
            "        [  5.6768],\n",
            "        [-12.6361],\n",
            "        [  9.6009]], grad_fn=<AddBackward0>)\n",
            "Epoch 640/1000, Loss: 0.08982417732477188\n",
            "tensor([[ -1.2181],\n",
            "        [  1.5875],\n",
            "        [  5.6782],\n",
            "        [-12.6447],\n",
            "        [  9.6077]], grad_fn=<AddBackward0>)\n",
            "Epoch 641/1000, Loss: 0.08972740173339844\n",
            "tensor([[ -1.2193],\n",
            "        [  1.5886],\n",
            "        [  5.6795],\n",
            "        [-12.6533],\n",
            "        [  9.6146]], grad_fn=<AddBackward0>)\n",
            "Epoch 642/1000, Loss: 0.08963065594434738\n",
            "tensor([[ -1.2205],\n",
            "        [  1.5898],\n",
            "        [  5.6808],\n",
            "        [-12.6618],\n",
            "        [  9.6214]], grad_fn=<AddBackward0>)\n",
            "Epoch 643/1000, Loss: 0.08953426778316498\n",
            "tensor([[ -1.2218],\n",
            "        [  1.5909],\n",
            "        [  5.6822],\n",
            "        [-12.6704],\n",
            "        [  9.6282]], grad_fn=<AddBackward0>)\n",
            "Epoch 644/1000, Loss: 0.0894380658864975\n",
            "tensor([[ -1.2230],\n",
            "        [  1.5921],\n",
            "        [  5.6835],\n",
            "        [-12.6789],\n",
            "        [  9.6351]], grad_fn=<AddBackward0>)\n",
            "Epoch 645/1000, Loss: 0.08934194594621658\n",
            "tensor([[ -1.2243],\n",
            "        [  1.5932],\n",
            "        [  5.6849],\n",
            "        [-12.6875],\n",
            "        [  9.6419]], grad_fn=<AddBackward0>)\n",
            "Epoch 646/1000, Loss: 0.08924610912799835\n",
            "tensor([[ -1.2255],\n",
            "        [  1.5943],\n",
            "        [  5.6862],\n",
            "        [-12.6960],\n",
            "        [  9.6487]], grad_fn=<AddBackward0>)\n",
            "Epoch 647/1000, Loss: 0.08915035426616669\n",
            "tensor([[ -1.2267],\n",
            "        [  1.5955],\n",
            "        [  5.6875],\n",
            "        [-12.7045],\n",
            "        [  9.6555]], grad_fn=<AddBackward0>)\n",
            "Epoch 648/1000, Loss: 0.08905485272407532\n",
            "tensor([[ -1.2280],\n",
            "        [  1.5966],\n",
            "        [  5.6889],\n",
            "        [-12.7130],\n",
            "        [  9.6623]], grad_fn=<AddBackward0>)\n",
            "Epoch 649/1000, Loss: 0.08895960450172424\n",
            "tensor([[ -1.2292],\n",
            "        [  1.5977],\n",
            "        [  5.6902],\n",
            "        [-12.7215],\n",
            "        [  9.6691]], grad_fn=<AddBackward0>)\n",
            "Epoch 650/1000, Loss: 0.08886443078517914\n",
            "tensor([[ -1.2304],\n",
            "        [  1.5989],\n",
            "        [  5.6916],\n",
            "        [-12.7300],\n",
            "        [  9.6759]], grad_fn=<AddBackward0>)\n",
            "Epoch 651/1000, Loss: 0.08876945078372955\n",
            "tensor([[ -1.2317],\n",
            "        [  1.6000],\n",
            "        [  5.6929],\n",
            "        [-12.7385],\n",
            "        [  9.6827]], grad_fn=<AddBackward0>)\n",
            "Epoch 652/1000, Loss: 0.08867473900318146\n",
            "tensor([[ -1.2329],\n",
            "        [  1.6011],\n",
            "        [  5.6942],\n",
            "        [-12.7470],\n",
            "        [  9.6894]], grad_fn=<AddBackward0>)\n",
            "Epoch 653/1000, Loss: 0.08858013898134232\n",
            "tensor([[ -1.2341],\n",
            "        [  1.6023],\n",
            "        [  5.6955],\n",
            "        [-12.7554],\n",
            "        [  9.6962]], grad_fn=<AddBackward0>)\n",
            "Epoch 654/1000, Loss: 0.08848565071821213\n",
            "tensor([[ -1.2354],\n",
            "        [  1.6034],\n",
            "        [  5.6969],\n",
            "        [-12.7639],\n",
            "        [  9.7030]], grad_fn=<AddBackward0>)\n",
            "Epoch 655/1000, Loss: 0.08839148283004761\n",
            "tensor([[ -1.2366],\n",
            "        [  1.6045],\n",
            "        [  5.6982],\n",
            "        [-12.7723],\n",
            "        [  9.7097]], grad_fn=<AddBackward0>)\n",
            "Epoch 656/1000, Loss: 0.08829732239246368\n",
            "tensor([[ -1.2378],\n",
            "        [  1.6057],\n",
            "        [  5.6995],\n",
            "        [-12.7808],\n",
            "        [  9.7165]], grad_fn=<AddBackward0>)\n",
            "Epoch 657/1000, Loss: 0.08820350468158722\n",
            "tensor([[ -1.2390],\n",
            "        [  1.6068],\n",
            "        [  5.7008],\n",
            "        [-12.7892],\n",
            "        [  9.7232]], grad_fn=<AddBackward0>)\n",
            "Epoch 658/1000, Loss: 0.08810984343290329\n",
            "tensor([[ -1.2403],\n",
            "        [  1.6079],\n",
            "        [  5.7021],\n",
            "        [-12.7976],\n",
            "        [  9.7299]], grad_fn=<AddBackward0>)\n",
            "Epoch 659/1000, Loss: 0.08801634609699249\n",
            "tensor([[ -1.2415],\n",
            "        [  1.6090],\n",
            "        [  5.7035],\n",
            "        [-12.8060],\n",
            "        [  9.7367]], grad_fn=<AddBackward0>)\n",
            "Epoch 660/1000, Loss: 0.08792294561862946\n",
            "tensor([[ -1.2427],\n",
            "        [  1.6102],\n",
            "        [  5.7048],\n",
            "        [-12.8145],\n",
            "        [  9.7434]], grad_fn=<AddBackward0>)\n",
            "Epoch 661/1000, Loss: 0.08782979846000671\n",
            "tensor([[ -1.2439],\n",
            "        [  1.6113],\n",
            "        [  5.7061],\n",
            "        [-12.8228],\n",
            "        [  9.7501]], grad_fn=<AddBackward0>)\n",
            "Epoch 662/1000, Loss: 0.08773675560951233\n",
            "tensor([[ -1.2452],\n",
            "        [  1.6124],\n",
            "        [  5.7074],\n",
            "        [-12.8312],\n",
            "        [  9.7568]], grad_fn=<AddBackward0>)\n",
            "Epoch 663/1000, Loss: 0.08764399588108063\n",
            "tensor([[ -1.2464],\n",
            "        [  1.6135],\n",
            "        [  5.7087],\n",
            "        [-12.8396],\n",
            "        [  9.7635]], grad_fn=<AddBackward0>)\n",
            "Epoch 664/1000, Loss: 0.08755128085613251\n",
            "tensor([[ -1.2476],\n",
            "        [  1.6147],\n",
            "        [  5.7100],\n",
            "        [-12.8480],\n",
            "        [  9.7702]], grad_fn=<AddBackward0>)\n",
            "Epoch 665/1000, Loss: 0.08745883405208588\n",
            "tensor([[ -1.2488],\n",
            "        [  1.6158],\n",
            "        [  5.7113],\n",
            "        [-12.8564],\n",
            "        [  9.7769]], grad_fn=<AddBackward0>)\n",
            "Epoch 666/1000, Loss: 0.08736664801836014\n",
            "tensor([[ -1.2500],\n",
            "        [  1.6169],\n",
            "        [  5.7126],\n",
            "        [-12.8647],\n",
            "        [  9.7836]], grad_fn=<AddBackward0>)\n",
            "Epoch 667/1000, Loss: 0.08727452903985977\n",
            "tensor([[ -1.2512],\n",
            "        [  1.6180],\n",
            "        [  5.7139],\n",
            "        [-12.8731],\n",
            "        [  9.7903]], grad_fn=<AddBackward0>)\n",
            "Epoch 668/1000, Loss: 0.08718262612819672\n",
            "tensor([[ -1.2524],\n",
            "        [  1.6191],\n",
            "        [  5.7152],\n",
            "        [-12.8814],\n",
            "        [  9.7969]], grad_fn=<AddBackward0>)\n",
            "Epoch 669/1000, Loss: 0.08709073811769485\n",
            "tensor([[ -1.2537],\n",
            "        [  1.6202],\n",
            "        [  5.7165],\n",
            "        [-12.8897],\n",
            "        [  9.8036]], grad_fn=<AddBackward0>)\n",
            "Epoch 670/1000, Loss: 0.08699920773506165\n",
            "tensor([[ -1.2549],\n",
            "        [  1.6214],\n",
            "        [  5.7178],\n",
            "        [-12.8981],\n",
            "        [  9.8103]], grad_fn=<AddBackward0>)\n",
            "Epoch 671/1000, Loss: 0.08690769970417023\n",
            "tensor([[ -1.2561],\n",
            "        [  1.6225],\n",
            "        [  5.7191],\n",
            "        [-12.9064],\n",
            "        [  9.8169]], grad_fn=<AddBackward0>)\n",
            "Epoch 672/1000, Loss: 0.08681664615869522\n",
            "tensor([[ -1.2573],\n",
            "        [  1.6236],\n",
            "        [  5.7204],\n",
            "        [-12.9147],\n",
            "        [  9.8236]], grad_fn=<AddBackward0>)\n",
            "Epoch 673/1000, Loss: 0.08672544360160828\n",
            "tensor([[ -1.2585],\n",
            "        [  1.6247],\n",
            "        [  5.7217],\n",
            "        [-12.9230],\n",
            "        [  9.8302]], grad_fn=<AddBackward0>)\n",
            "Epoch 674/1000, Loss: 0.08663459122180939\n",
            "tensor([[ -1.2597],\n",
            "        [  1.6258],\n",
            "        [  5.7230],\n",
            "        [-12.9313],\n",
            "        [  9.8368]], grad_fn=<AddBackward0>)\n",
            "Epoch 675/1000, Loss: 0.08654394745826721\n",
            "tensor([[ -1.2609],\n",
            "        [  1.6269],\n",
            "        [  5.7243],\n",
            "        [-12.9396],\n",
            "        [  9.8434]], grad_fn=<AddBackward0>)\n",
            "Epoch 676/1000, Loss: 0.08645334839820862\n",
            "tensor([[ -1.2621],\n",
            "        [  1.6280],\n",
            "        [  5.7256],\n",
            "        [-12.9478],\n",
            "        [  9.8501]], grad_fn=<AddBackward0>)\n",
            "Epoch 677/1000, Loss: 0.08636291325092316\n",
            "tensor([[ -1.2633],\n",
            "        [  1.6291],\n",
            "        [  5.7269],\n",
            "        [-12.9561],\n",
            "        [  9.8567]], grad_fn=<AddBackward0>)\n",
            "Epoch 678/1000, Loss: 0.08627276867628098\n",
            "tensor([[ -1.2645],\n",
            "        [  1.6302],\n",
            "        [  5.7282],\n",
            "        [-12.9644],\n",
            "        [  9.8633]], grad_fn=<AddBackward0>)\n",
            "Epoch 679/1000, Loss: 0.08618266880512238\n",
            "tensor([[ -1.2657],\n",
            "        [  1.6314],\n",
            "        [  5.7295],\n",
            "        [-12.9726],\n",
            "        [  9.8699]], grad_fn=<AddBackward0>)\n",
            "Epoch 680/1000, Loss: 0.08609279245138168\n",
            "tensor([[ -1.2669],\n",
            "        [  1.6325],\n",
            "        [  5.7308],\n",
            "        [-12.9809],\n",
            "        [  9.8765]], grad_fn=<AddBackward0>)\n",
            "Epoch 681/1000, Loss: 0.08600310236215591\n",
            "tensor([[ -1.2681],\n",
            "        [  1.6336],\n",
            "        [  5.7320],\n",
            "        [-12.9891],\n",
            "        [  9.8831]], grad_fn=<AddBackward0>)\n",
            "Epoch 682/1000, Loss: 0.08591355383396149\n",
            "tensor([[ -1.2693],\n",
            "        [  1.6347],\n",
            "        [  5.7333],\n",
            "        [-12.9973],\n",
            "        [  9.8897]], grad_fn=<AddBackward0>)\n",
            "Epoch 683/1000, Loss: 0.08582420647144318\n",
            "tensor([[ -1.2705],\n",
            "        [  1.6358],\n",
            "        [  5.7346],\n",
            "        [-13.0055],\n",
            "        [  9.8962]], grad_fn=<AddBackward0>)\n",
            "Epoch 684/1000, Loss: 0.08573497831821442\n",
            "tensor([[ -1.2717],\n",
            "        [  1.6369],\n",
            "        [  5.7359],\n",
            "        [-13.0138],\n",
            "        [  9.9028]], grad_fn=<AddBackward0>)\n",
            "Epoch 685/1000, Loss: 0.08564592897891998\n",
            "tensor([[ -1.2729],\n",
            "        [  1.6380],\n",
            "        [  5.7372],\n",
            "        [-13.0220],\n",
            "        [  9.9094]], grad_fn=<AddBackward0>)\n",
            "Epoch 686/1000, Loss: 0.0855570062994957\n",
            "tensor([[ -1.2740],\n",
            "        [  1.6391],\n",
            "        [  5.7384],\n",
            "        [-13.0301],\n",
            "        [  9.9159]], grad_fn=<AddBackward0>)\n",
            "Epoch 687/1000, Loss: 0.0854683443903923\n",
            "tensor([[ -1.2752],\n",
            "        [  1.6402],\n",
            "        [  5.7397],\n",
            "        [-13.0383],\n",
            "        [  9.9225]], grad_fn=<AddBackward0>)\n",
            "Epoch 688/1000, Loss: 0.08537980914115906\n",
            "tensor([[ -1.2764],\n",
            "        [  1.6413],\n",
            "        [  5.7410],\n",
            "        [-13.0465],\n",
            "        [  9.9290]], grad_fn=<AddBackward0>)\n",
            "Epoch 689/1000, Loss: 0.08529140055179596\n",
            "tensor([[ -1.2776],\n",
            "        [  1.6424],\n",
            "        [  5.7423],\n",
            "        [-13.0547],\n",
            "        [  9.9356]], grad_fn=<AddBackward0>)\n",
            "Epoch 690/1000, Loss: 0.08520326763391495\n",
            "tensor([[ -1.2788],\n",
            "        [  1.6435],\n",
            "        [  5.7435],\n",
            "        [-13.0628],\n",
            "        [  9.9421]], grad_fn=<AddBackward0>)\n",
            "Epoch 691/1000, Loss: 0.08511520177125931\n",
            "tensor([[ -1.2800],\n",
            "        [  1.6446],\n",
            "        [  5.7448],\n",
            "        [-13.0710],\n",
            "        [  9.9486]], grad_fn=<AddBackward0>)\n",
            "Epoch 692/1000, Loss: 0.08502738922834396\n",
            "tensor([[ -1.2812],\n",
            "        [  1.6456],\n",
            "        [  5.7461],\n",
            "        [-13.0791],\n",
            "        [  9.9551]], grad_fn=<AddBackward0>)\n",
            "Epoch 693/1000, Loss: 0.08493958413600922\n",
            "tensor([[ -1.2823],\n",
            "        [  1.6467],\n",
            "        [  5.7473],\n",
            "        [-13.0873],\n",
            "        [  9.9617]], grad_fn=<AddBackward0>)\n",
            "Epoch 694/1000, Loss: 0.08485208451747894\n",
            "tensor([[ -1.2835],\n",
            "        [  1.6478],\n",
            "        [  5.7486],\n",
            "        [-13.0954],\n",
            "        [  9.9682]], grad_fn=<AddBackward0>)\n",
            "Epoch 695/1000, Loss: 0.08476467430591583\n",
            "tensor([[ -1.2847],\n",
            "        [  1.6489],\n",
            "        [  5.7499],\n",
            "        [-13.1035],\n",
            "        [  9.9747]], grad_fn=<AddBackward0>)\n",
            "Epoch 696/1000, Loss: 0.08467742800712585\n",
            "tensor([[ -1.2859],\n",
            "        [  1.6500],\n",
            "        [  5.7511],\n",
            "        [-13.1117],\n",
            "        [  9.9812]], grad_fn=<AddBackward0>)\n",
            "Epoch 697/1000, Loss: 0.08459034562110901\n",
            "tensor([[ -1.2871],\n",
            "        [  1.6511],\n",
            "        [  5.7524],\n",
            "        [-13.1198],\n",
            "        [  9.9876]], grad_fn=<AddBackward0>)\n",
            "Epoch 698/1000, Loss: 0.08450343459844589\n",
            "tensor([[ -1.2882],\n",
            "        [  1.6522],\n",
            "        [  5.7536],\n",
            "        [-13.1279],\n",
            "        [  9.9941]], grad_fn=<AddBackward0>)\n",
            "Epoch 699/1000, Loss: 0.0844167098402977\n",
            "tensor([[ -1.2894],\n",
            "        [  1.6533],\n",
            "        [  5.7549],\n",
            "        [-13.1360],\n",
            "        [ 10.0006]], grad_fn=<AddBackward0>)\n",
            "Epoch 700/1000, Loss: 0.08433016389608383\n",
            "tensor([[ -1.2906],\n",
            "        [  1.6544],\n",
            "        [  5.7561],\n",
            "        [-13.1440],\n",
            "        [ 10.0071]], grad_fn=<AddBackward0>)\n",
            "Epoch 701/1000, Loss: 0.08424370735883713\n",
            "tensor([[ -1.2918],\n",
            "        [  1.6554],\n",
            "        [  5.7574],\n",
            "        [-13.1521],\n",
            "        [ 10.0135]], grad_fn=<AddBackward0>)\n",
            "Epoch 702/1000, Loss: 0.08415733277797699\n",
            "tensor([[ -1.2929],\n",
            "        [  1.6565],\n",
            "        [  5.7586],\n",
            "        [-13.1602],\n",
            "        [ 10.0200]], grad_fn=<AddBackward0>)\n",
            "Epoch 703/1000, Loss: 0.08407127857208252\n",
            "tensor([[ -1.2941],\n",
            "        [  1.6576],\n",
            "        [  5.7599],\n",
            "        [-13.1682],\n",
            "        [ 10.0265]], grad_fn=<AddBackward0>)\n",
            "Epoch 704/1000, Loss: 0.08398531377315521\n",
            "tensor([[ -1.2953],\n",
            "        [  1.6587],\n",
            "        [  5.7611],\n",
            "        [-13.1763],\n",
            "        [ 10.0329]], grad_fn=<AddBackward0>)\n",
            "Epoch 705/1000, Loss: 0.08389954268932343\n",
            "tensor([[ -1.2964],\n",
            "        [  1.6598],\n",
            "        [  5.7624],\n",
            "        [-13.1843],\n",
            "        [ 10.0393]], grad_fn=<AddBackward0>)\n",
            "Epoch 706/1000, Loss: 0.08381389826536179\n",
            "tensor([[ -1.2976],\n",
            "        [  1.6609],\n",
            "        [  5.7636],\n",
            "        [-13.1924],\n",
            "        [ 10.0458]], grad_fn=<AddBackward0>)\n",
            "Epoch 707/1000, Loss: 0.08372849971055984\n",
            "tensor([[ -1.2988],\n",
            "        [  1.6619],\n",
            "        [  5.7649],\n",
            "        [-13.2004],\n",
            "        [ 10.0522]], grad_fn=<AddBackward0>)\n",
            "Epoch 708/1000, Loss: 0.08364317566156387\n",
            "tensor([[ -1.2999],\n",
            "        [  1.6630],\n",
            "        [  5.7661],\n",
            "        [-13.2084],\n",
            "        [ 10.0586]], grad_fn=<AddBackward0>)\n",
            "Epoch 709/1000, Loss: 0.08355796337127686\n",
            "tensor([[ -1.3011],\n",
            "        [  1.6641],\n",
            "        [  5.7674],\n",
            "        [-13.2164],\n",
            "        [ 10.0651]], grad_fn=<AddBackward0>)\n",
            "Epoch 710/1000, Loss: 0.08347292244434357\n",
            "tensor([[ -1.3023],\n",
            "        [  1.6652],\n",
            "        [  5.7686],\n",
            "        [-13.2244],\n",
            "        [ 10.0715]], grad_fn=<AddBackward0>)\n",
            "Epoch 711/1000, Loss: 0.08338810503482819\n",
            "tensor([[ -1.3034],\n",
            "        [  1.6662],\n",
            "        [  5.7698],\n",
            "        [-13.2324],\n",
            "        [ 10.0779]], grad_fn=<AddBackward0>)\n",
            "Epoch 712/1000, Loss: 0.08330331742763519\n",
            "tensor([[ -1.3046],\n",
            "        [  1.6673],\n",
            "        [  5.7711],\n",
            "        [-13.2404],\n",
            "        [ 10.0843]], grad_fn=<AddBackward0>)\n",
            "Epoch 713/1000, Loss: 0.08321879804134369\n",
            "tensor([[ -1.3058],\n",
            "        [  1.6684],\n",
            "        [  5.7723],\n",
            "        [-13.2484],\n",
            "        [ 10.0907]], grad_fn=<AddBackward0>)\n",
            "Epoch 714/1000, Loss: 0.08313446491956711\n",
            "tensor([[ -1.3069],\n",
            "        [  1.6695],\n",
            "        [  5.7736],\n",
            "        [-13.2564],\n",
            "        [ 10.0971]], grad_fn=<AddBackward0>)\n",
            "Epoch 715/1000, Loss: 0.0830501839518547\n",
            "tensor([[ -1.3081],\n",
            "        [  1.6705],\n",
            "        [  5.7748],\n",
            "        [-13.2644],\n",
            "        [ 10.1034]], grad_fn=<AddBackward0>)\n",
            "Epoch 716/1000, Loss: 0.08296613395214081\n",
            "tensor([[ -1.3092],\n",
            "        [  1.6716],\n",
            "        [  5.7760],\n",
            "        [-13.2723],\n",
            "        [ 10.1098]], grad_fn=<AddBackward0>)\n",
            "Epoch 717/1000, Loss: 0.08288227021694183\n",
            "tensor([[ -1.3104],\n",
            "        [  1.6727],\n",
            "        [  5.7773],\n",
            "        [-13.2803],\n",
            "        [ 10.1162]], grad_fn=<AddBackward0>)\n",
            "Epoch 718/1000, Loss: 0.08279840648174286\n",
            "tensor([[ -1.3115],\n",
            "        [  1.6738],\n",
            "        [  5.7785],\n",
            "        [-13.2882],\n",
            "        [ 10.1226]], grad_fn=<AddBackward0>)\n",
            "Epoch 719/1000, Loss: 0.08271480351686478\n",
            "tensor([[ -1.3127],\n",
            "        [  1.6748],\n",
            "        [  5.7797],\n",
            "        [-13.2962],\n",
            "        [ 10.1289]], grad_fn=<AddBackward0>)\n",
            "Epoch 720/1000, Loss: 0.08263124525547028\n",
            "tensor([[ -1.3138],\n",
            "        [  1.6759],\n",
            "        [  5.7809],\n",
            "        [-13.3041],\n",
            "        [ 10.1353]], grad_fn=<AddBackward0>)\n",
            "Epoch 721/1000, Loss: 0.082548126578331\n",
            "tensor([[ -1.3150],\n",
            "        [  1.6769],\n",
            "        [  5.7822],\n",
            "        [-13.3120],\n",
            "        [ 10.1416]], grad_fn=<AddBackward0>)\n",
            "Epoch 722/1000, Loss: 0.08246488124132156\n",
            "tensor([[ -1.3161],\n",
            "        [  1.6780],\n",
            "        [  5.7834],\n",
            "        [-13.3199],\n",
            "        [ 10.1480]], grad_fn=<AddBackward0>)\n",
            "Epoch 723/1000, Loss: 0.08238188922405243\n",
            "tensor([[ -1.3173],\n",
            "        [  1.6791],\n",
            "        [  5.7846],\n",
            "        [-13.3278],\n",
            "        [ 10.1543]], grad_fn=<AddBackward0>)\n",
            "Epoch 724/1000, Loss: 0.08229900151491165\n",
            "tensor([[ -1.3184],\n",
            "        [  1.6801],\n",
            "        [  5.7858],\n",
            "        [-13.3357],\n",
            "        [ 10.1606]], grad_fn=<AddBackward0>)\n",
            "Epoch 725/1000, Loss: 0.08221624791622162\n",
            "tensor([[ -1.3196],\n",
            "        [  1.6812],\n",
            "        [  5.7871],\n",
            "        [-13.3436],\n",
            "        [ 10.1669]], grad_fn=<AddBackward0>)\n",
            "Epoch 726/1000, Loss: 0.0821336954832077\n",
            "tensor([[ -1.3207],\n",
            "        [  1.6823],\n",
            "        [  5.7883],\n",
            "        [-13.3515],\n",
            "        [ 10.1733]], grad_fn=<AddBackward0>)\n",
            "Epoch 727/1000, Loss: 0.08205122500658035\n",
            "tensor([[ -1.3219],\n",
            "        [  1.6833],\n",
            "        [  5.7895],\n",
            "        [-13.3594],\n",
            "        [ 10.1796]], grad_fn=<AddBackward0>)\n",
            "Epoch 728/1000, Loss: 0.08196906000375748\n",
            "tensor([[ -1.3230],\n",
            "        [  1.6844],\n",
            "        [  5.7907],\n",
            "        [-13.3673],\n",
            "        [ 10.1859]], grad_fn=<AddBackward0>)\n",
            "Epoch 729/1000, Loss: 0.08188699185848236\n",
            "tensor([[ -1.3242],\n",
            "        [  1.6855],\n",
            "        [  5.7919],\n",
            "        [-13.3751],\n",
            "        [ 10.1922]], grad_fn=<AddBackward0>)\n",
            "Epoch 730/1000, Loss: 0.08180494606494904\n",
            "tensor([[ -1.3253],\n",
            "        [  1.6865],\n",
            "        [  5.7931],\n",
            "        [-13.3830],\n",
            "        [ 10.1985]], grad_fn=<AddBackward0>)\n",
            "Epoch 731/1000, Loss: 0.08172319829463959\n",
            "tensor([[ -1.3264],\n",
            "        [  1.6876],\n",
            "        [  5.7943],\n",
            "        [-13.3908],\n",
            "        [ 10.2048]], grad_fn=<AddBackward0>)\n",
            "Epoch 732/1000, Loss: 0.08164144307374954\n",
            "tensor([[ -1.3276],\n",
            "        [  1.6886],\n",
            "        [  5.7956],\n",
            "        [-13.3987],\n",
            "        [ 10.2110]], grad_fn=<AddBackward0>)\n",
            "Epoch 733/1000, Loss: 0.08155988156795502\n",
            "tensor([[ -1.3287],\n",
            "        [  1.6897],\n",
            "        [  5.7968],\n",
            "        [-13.4065],\n",
            "        [ 10.2173]], grad_fn=<AddBackward0>)\n",
            "Epoch 734/1000, Loss: 0.08147858083248138\n",
            "tensor([[ -1.3299],\n",
            "        [  1.6907],\n",
            "        [  5.7980],\n",
            "        [-13.4143],\n",
            "        [ 10.2236]], grad_fn=<AddBackward0>)\n",
            "Epoch 735/1000, Loss: 0.0813973993062973\n",
            "tensor([[ -1.3310],\n",
            "        [  1.6918],\n",
            "        [  5.7992],\n",
            "        [-13.4222],\n",
            "        [ 10.2299]], grad_fn=<AddBackward0>)\n",
            "Epoch 736/1000, Loss: 0.08131629973649979\n",
            "tensor([[ -1.3321],\n",
            "        [  1.6928],\n",
            "        [  5.8004],\n",
            "        [-13.4300],\n",
            "        [ 10.2361]], grad_fn=<AddBackward0>)\n",
            "Epoch 737/1000, Loss: 0.0812353789806366\n",
            "tensor([[ -1.3333],\n",
            "        [  1.6939],\n",
            "        [  5.8016],\n",
            "        [-13.4378],\n",
            "        [ 10.2424]], grad_fn=<AddBackward0>)\n",
            "Epoch 738/1000, Loss: 0.08115452527999878\n",
            "tensor([[ -1.3344],\n",
            "        [  1.6950],\n",
            "        [  5.8028],\n",
            "        [-13.4456],\n",
            "        [ 10.2486]], grad_fn=<AddBackward0>)\n",
            "Epoch 739/1000, Loss: 0.08107389509677887\n",
            "tensor([[ -1.3355],\n",
            "        [  1.6960],\n",
            "        [  5.8040],\n",
            "        [-13.4534],\n",
            "        [ 10.2549]], grad_fn=<AddBackward0>)\n",
            "Epoch 740/1000, Loss: 0.08099345117807388\n",
            "tensor([[ -1.3367],\n",
            "        [  1.6971],\n",
            "        [  5.8052],\n",
            "        [-13.4611],\n",
            "        [ 10.2611]], grad_fn=<AddBackward0>)\n",
            "Epoch 741/1000, Loss: 0.08091304451227188\n",
            "tensor([[ -1.3378],\n",
            "        [  1.6981],\n",
            "        [  5.8064],\n",
            "        [-13.4689],\n",
            "        [ 10.2673]], grad_fn=<AddBackward0>)\n",
            "Epoch 742/1000, Loss: 0.08083290606737137\n",
            "tensor([[ -1.3389],\n",
            "        [  1.6991],\n",
            "        [  5.8076],\n",
            "        [-13.4767],\n",
            "        [ 10.2736]], grad_fn=<AddBackward0>)\n",
            "Epoch 743/1000, Loss: 0.08075279742479324\n",
            "tensor([[ -1.3400],\n",
            "        [  1.7002],\n",
            "        [  5.8088],\n",
            "        [-13.4845],\n",
            "        [ 10.2798]], grad_fn=<AddBackward0>)\n",
            "Epoch 744/1000, Loss: 0.08067281544208527\n",
            "tensor([[ -1.3412],\n",
            "        [  1.7012],\n",
            "        [  5.8100],\n",
            "        [-13.4922],\n",
            "        [ 10.2860]], grad_fn=<AddBackward0>)\n",
            "Epoch 745/1000, Loss: 0.0805930644273758\n",
            "tensor([[ -1.3423],\n",
            "        [  1.7023],\n",
            "        [  5.8112],\n",
            "        [-13.5000],\n",
            "        [ 10.2922]], grad_fn=<AddBackward0>)\n",
            "Epoch 746/1000, Loss: 0.0805133581161499\n",
            "tensor([[ -1.3434],\n",
            "        [  1.7033],\n",
            "        [  5.8124],\n",
            "        [-13.5077],\n",
            "        [ 10.2984]], grad_fn=<AddBackward0>)\n",
            "Epoch 747/1000, Loss: 0.0804339200258255\n",
            "tensor([[ -1.3445],\n",
            "        [  1.7044],\n",
            "        [  5.8135],\n",
            "        [-13.5154],\n",
            "        [ 10.3046]], grad_fn=<AddBackward0>)\n",
            "Epoch 748/1000, Loss: 0.08035457879304886\n",
            "tensor([[ -1.3457],\n",
            "        [  1.7054],\n",
            "        [  5.8147],\n",
            "        [-13.5232],\n",
            "        [ 10.3108]], grad_fn=<AddBackward0>)\n",
            "Epoch 749/1000, Loss: 0.08027538657188416\n",
            "tensor([[ -1.3468],\n",
            "        [  1.7065],\n",
            "        [  5.8159],\n",
            "        [-13.5309],\n",
            "        [ 10.3170]], grad_fn=<AddBackward0>)\n",
            "Epoch 750/1000, Loss: 0.08019623905420303\n",
            "tensor([[ -1.3479],\n",
            "        [  1.7075],\n",
            "        [  5.8171],\n",
            "        [-13.5386],\n",
            "        [ 10.3232]], grad_fn=<AddBackward0>)\n",
            "Epoch 751/1000, Loss: 0.08011741936206818\n",
            "tensor([[ -1.3490],\n",
            "        [  1.7085],\n",
            "        [  5.8183],\n",
            "        [-13.5463],\n",
            "        [ 10.3294]], grad_fn=<AddBackward0>)\n",
            "Epoch 752/1000, Loss: 0.08003860712051392\n",
            "tensor([[ -1.3501],\n",
            "        [  1.7096],\n",
            "        [  5.8195],\n",
            "        [-13.5540],\n",
            "        [ 10.3355]], grad_fn=<AddBackward0>)\n",
            "Epoch 753/1000, Loss: 0.07995981723070145\n",
            "tensor([[ -1.3513],\n",
            "        [  1.7106],\n",
            "        [  5.8207],\n",
            "        [-13.5617],\n",
            "        [ 10.3417]], grad_fn=<AddBackward0>)\n",
            "Epoch 754/1000, Loss: 0.07988137751817703\n",
            "tensor([[ -1.3524],\n",
            "        [  1.7117],\n",
            "        [  5.8218],\n",
            "        [-13.5694],\n",
            "        [ 10.3479]], grad_fn=<AddBackward0>)\n",
            "Epoch 755/1000, Loss: 0.07980296015739441\n",
            "tensor([[ -1.3535],\n",
            "        [  1.7127],\n",
            "        [  5.8230],\n",
            "        [-13.5770],\n",
            "        [ 10.3540]], grad_fn=<AddBackward0>)\n",
            "Epoch 756/1000, Loss: 0.07972480356693268\n",
            "tensor([[ -1.3546],\n",
            "        [  1.7137],\n",
            "        [  5.8242],\n",
            "        [-13.5847],\n",
            "        [ 10.3602]], grad_fn=<AddBackward0>)\n",
            "Epoch 757/1000, Loss: 0.0796467736363411\n",
            "tensor([[ -1.3557],\n",
            "        [  1.7148],\n",
            "        [  5.8254],\n",
            "        [-13.5924],\n",
            "        [ 10.3663]], grad_fn=<AddBackward0>)\n",
            "Epoch 758/1000, Loss: 0.07956872880458832\n",
            "tensor([[ -1.3568],\n",
            "        [  1.7158],\n",
            "        [  5.8266],\n",
            "        [-13.6000],\n",
            "        [ 10.3724]], grad_fn=<AddBackward0>)\n",
            "Epoch 759/1000, Loss: 0.07949098944664001\n",
            "tensor([[ -1.3579],\n",
            "        [  1.7168],\n",
            "        [  5.8277],\n",
            "        [-13.6077],\n",
            "        [ 10.3786]], grad_fn=<AddBackward0>)\n",
            "Epoch 760/1000, Loss: 0.07941325753927231\n",
            "tensor([[ -1.3590],\n",
            "        [  1.7179],\n",
            "        [  5.8289],\n",
            "        [-13.6153],\n",
            "        [ 10.3847]], grad_fn=<AddBackward0>)\n",
            "Epoch 761/1000, Loss: 0.07933571934700012\n",
            "tensor([[ -1.3602],\n",
            "        [  1.7189],\n",
            "        [  5.8301],\n",
            "        [-13.6229],\n",
            "        [ 10.3908]], grad_fn=<AddBackward0>)\n",
            "Epoch 762/1000, Loss: 0.07925838232040405\n",
            "tensor([[ -1.3613],\n",
            "        [  1.7199],\n",
            "        [  5.8313],\n",
            "        [-13.6306],\n",
            "        [ 10.3969]], grad_fn=<AddBackward0>)\n",
            "Epoch 763/1000, Loss: 0.07918098568916321\n",
            "tensor([[ -1.3624],\n",
            "        [  1.7210],\n",
            "        [  5.8324],\n",
            "        [-13.6382],\n",
            "        [ 10.4030]], grad_fn=<AddBackward0>)\n",
            "Epoch 764/1000, Loss: 0.07910396158695221\n",
            "tensor([[ -1.3635],\n",
            "        [  1.7220],\n",
            "        [  5.8336],\n",
            "        [-13.6458],\n",
            "        [ 10.4091]], grad_fn=<AddBackward0>)\n",
            "Epoch 765/1000, Loss: 0.0790269672870636\n",
            "tensor([[ -1.3646],\n",
            "        [  1.7230],\n",
            "        [  5.8348],\n",
            "        [-13.6534],\n",
            "        [ 10.4152]], grad_fn=<AddBackward0>)\n",
            "Epoch 766/1000, Loss: 0.07895003259181976\n",
            "tensor([[ -1.3657],\n",
            "        [  1.7240],\n",
            "        [  5.8359],\n",
            "        [-13.6610],\n",
            "        [ 10.4213]], grad_fn=<AddBackward0>)\n",
            "Epoch 767/1000, Loss: 0.07887334376573563\n",
            "tensor([[ -1.3668],\n",
            "        [  1.7251],\n",
            "        [  5.8371],\n",
            "        [-13.6686],\n",
            "        [ 10.4274]], grad_fn=<AddBackward0>)\n",
            "Epoch 768/1000, Loss: 0.07879675924777985\n",
            "tensor([[ -1.3679],\n",
            "        [  1.7261],\n",
            "        [  5.8383],\n",
            "        [-13.6762],\n",
            "        [ 10.4335]], grad_fn=<AddBackward0>)\n",
            "Epoch 769/1000, Loss: 0.0787203386425972\n",
            "tensor([[ -1.3690],\n",
            "        [  1.7271],\n",
            "        [  5.8394],\n",
            "        [-13.6837],\n",
            "        [ 10.4396]], grad_fn=<AddBackward0>)\n",
            "Epoch 770/1000, Loss: 0.07864399999380112\n",
            "tensor([[ -1.3701],\n",
            "        [  1.7281],\n",
            "        [  5.8406],\n",
            "        [-13.6913],\n",
            "        [ 10.4457]], grad_fn=<AddBackward0>)\n",
            "Epoch 771/1000, Loss: 0.07856790721416473\n",
            "tensor([[ -1.3712],\n",
            "        [  1.7292],\n",
            "        [  5.8417],\n",
            "        [-13.6989],\n",
            "        [ 10.4517]], grad_fn=<AddBackward0>)\n",
            "Epoch 772/1000, Loss: 0.0784917026758194\n",
            "tensor([[ -1.3723],\n",
            "        [  1.7302],\n",
            "        [  5.8429],\n",
            "        [-13.7064],\n",
            "        [ 10.4578]], grad_fn=<AddBackward0>)\n",
            "Epoch 773/1000, Loss: 0.0784158930182457\n",
            "tensor([[ -1.3734],\n",
            "        [  1.7312],\n",
            "        [  5.8441],\n",
            "        [-13.7140],\n",
            "        [ 10.4638]], grad_fn=<AddBackward0>)\n",
            "Epoch 774/1000, Loss: 0.078340083360672\n",
            "tensor([[ -1.3745],\n",
            "        [  1.7322],\n",
            "        [  5.8452],\n",
            "        [-13.7215],\n",
            "        [ 10.4699]], grad_fn=<AddBackward0>)\n",
            "Epoch 775/1000, Loss: 0.07826442271471024\n",
            "tensor([[ -1.3756],\n",
            "        [  1.7332],\n",
            "        [  5.8464],\n",
            "        [-13.7291],\n",
            "        [ 10.4759]], grad_fn=<AddBackward0>)\n",
            "Epoch 776/1000, Loss: 0.07818891108036041\n",
            "tensor([[ -1.3767],\n",
            "        [  1.7343],\n",
            "        [  5.8475],\n",
            "        [-13.7366],\n",
            "        [ 10.4820]], grad_fn=<AddBackward0>)\n",
            "Epoch 777/1000, Loss: 0.07811342924833298\n",
            "tensor([[ -1.3778],\n",
            "        [  1.7353],\n",
            "        [  5.8487],\n",
            "        [-13.7441],\n",
            "        [ 10.4880]], grad_fn=<AddBackward0>)\n",
            "Epoch 778/1000, Loss: 0.078038290143013\n",
            "tensor([[ -1.3788],\n",
            "        [  1.7363],\n",
            "        [  5.8498],\n",
            "        [-13.7516],\n",
            "        [ 10.4940]], grad_fn=<AddBackward0>)\n",
            "Epoch 779/1000, Loss: 0.07796315848827362\n",
            "tensor([[ -1.3799],\n",
            "        [  1.7373],\n",
            "        [  5.8510],\n",
            "        [-13.7591],\n",
            "        [ 10.5001]], grad_fn=<AddBackward0>)\n",
            "Epoch 780/1000, Loss: 0.07788815349340439\n",
            "tensor([[ -1.3810],\n",
            "        [  1.7383],\n",
            "        [  5.8521],\n",
            "        [-13.7666],\n",
            "        [ 10.5061]], grad_fn=<AddBackward0>)\n",
            "Epoch 781/1000, Loss: 0.0778132826089859\n",
            "tensor([[ -1.3821],\n",
            "        [  1.7393],\n",
            "        [  5.8533],\n",
            "        [-13.7741],\n",
            "        [ 10.5121]], grad_fn=<AddBackward0>)\n",
            "Epoch 782/1000, Loss: 0.07773853838443756\n",
            "tensor([[ -1.3832],\n",
            "        [  1.7404],\n",
            "        [  5.8544],\n",
            "        [-13.7816],\n",
            "        [ 10.5181]], grad_fn=<AddBackward0>)\n",
            "Epoch 783/1000, Loss: 0.07766390591859818\n",
            "tensor([[ -1.3843],\n",
            "        [  1.7414],\n",
            "        [  5.8556],\n",
            "        [-13.7891],\n",
            "        [ 10.5241]], grad_fn=<AddBackward0>)\n",
            "Epoch 784/1000, Loss: 0.07758945226669312\n",
            "tensor([[ -1.3854],\n",
            "        [  1.7424],\n",
            "        [  5.8567],\n",
            "        [-13.7966],\n",
            "        [ 10.5301]], grad_fn=<AddBackward0>)\n",
            "Epoch 785/1000, Loss: 0.0775151401758194\n",
            "tensor([[ -1.3865],\n",
            "        [  1.7434],\n",
            "        [  5.8578],\n",
            "        [-13.8041],\n",
            "        [ 10.5361]], grad_fn=<AddBackward0>)\n",
            "Epoch 786/1000, Loss: 0.07744090259075165\n",
            "tensor([[ -1.3875],\n",
            "        [  1.7444],\n",
            "        [  5.8590],\n",
            "        [-13.8115],\n",
            "        [ 10.5421]], grad_fn=<AddBackward0>)\n",
            "Epoch 787/1000, Loss: 0.07736673206090927\n",
            "tensor([[ -1.3886],\n",
            "        [  1.7454],\n",
            "        [  5.8601],\n",
            "        [-13.8190],\n",
            "        [ 10.5481]], grad_fn=<AddBackward0>)\n",
            "Epoch 788/1000, Loss: 0.07729290425777435\n",
            "tensor([[ -1.3897],\n",
            "        [  1.7464],\n",
            "        [  5.8613],\n",
            "        [-13.8264],\n",
            "        [ 10.5541]], grad_fn=<AddBackward0>)\n",
            "Epoch 789/1000, Loss: 0.07721899449825287\n",
            "tensor([[ -1.3908],\n",
            "        [  1.7474],\n",
            "        [  5.8624],\n",
            "        [-13.8339],\n",
            "        [ 10.5600]], grad_fn=<AddBackward0>)\n",
            "Epoch 790/1000, Loss: 0.07714535295963287\n",
            "tensor([[ -1.3919],\n",
            "        [  1.7484],\n",
            "        [  5.8635],\n",
            "        [-13.8413],\n",
            "        [ 10.5660]], grad_fn=<AddBackward0>)\n",
            "Epoch 791/1000, Loss: 0.07707177847623825\n",
            "tensor([[ -1.3929],\n",
            "        [  1.7494],\n",
            "        [  5.8647],\n",
            "        [-13.8487],\n",
            "        [ 10.5719]], grad_fn=<AddBackward0>)\n",
            "Epoch 792/1000, Loss: 0.07699833810329437\n",
            "tensor([[ -1.3940],\n",
            "        [  1.7505],\n",
            "        [  5.8658],\n",
            "        [-13.8561],\n",
            "        [ 10.5779]], grad_fn=<AddBackward0>)\n",
            "Epoch 793/1000, Loss: 0.07692499458789825\n",
            "tensor([[ -1.3951],\n",
            "        [  1.7515],\n",
            "        [  5.8669],\n",
            "        [-13.8636],\n",
            "        [ 10.5839]], grad_fn=<AddBackward0>)\n",
            "Epoch 794/1000, Loss: 0.07685186713933945\n",
            "tensor([[ -1.3962],\n",
            "        [  1.7525],\n",
            "        [  5.8681],\n",
            "        [-13.8710],\n",
            "        [ 10.5898]], grad_fn=<AddBackward0>)\n",
            "Epoch 795/1000, Loss: 0.07677875459194183\n",
            "tensor([[ -1.3973],\n",
            "        [  1.7535],\n",
            "        [  5.8692],\n",
            "        [-13.8784],\n",
            "        [ 10.5957]], grad_fn=<AddBackward0>)\n",
            "Epoch 796/1000, Loss: 0.07670587301254272\n",
            "tensor([[ -1.3983],\n",
            "        [  1.7545],\n",
            "        [  5.8703],\n",
            "        [-13.8858],\n",
            "        [ 10.6017]], grad_fn=<AddBackward0>)\n",
            "Epoch 797/1000, Loss: 0.07663299143314362\n",
            "tensor([[ -1.3994],\n",
            "        [  1.7555],\n",
            "        [  5.8715],\n",
            "        [-13.8931],\n",
            "        [ 10.6076]], grad_fn=<AddBackward0>)\n",
            "Epoch 798/1000, Loss: 0.07656032592058182\n",
            "tensor([[ -1.4005],\n",
            "        [  1.7565],\n",
            "        [  5.8726],\n",
            "        [-13.9005],\n",
            "        [ 10.6135]], grad_fn=<AddBackward0>)\n",
            "Epoch 799/1000, Loss: 0.07648781687021255\n",
            "tensor([[ -1.4015],\n",
            "        [  1.7575],\n",
            "        [  5.8737],\n",
            "        [-13.9079],\n",
            "        [ 10.6195]], grad_fn=<AddBackward0>)\n",
            "Epoch 800/1000, Loss: 0.07641533762216568\n",
            "tensor([[ -1.4026],\n",
            "        [  1.7585],\n",
            "        [  5.8748],\n",
            "        [-13.9153],\n",
            "        [ 10.6254]], grad_fn=<AddBackward0>)\n",
            "Epoch 801/1000, Loss: 0.07634292542934418\n",
            "tensor([[ -1.4037],\n",
            "        [  1.7595],\n",
            "        [  5.8760],\n",
            "        [-13.9226],\n",
            "        [ 10.6313]], grad_fn=<AddBackward0>)\n",
            "Epoch 802/1000, Loss: 0.07627078890800476\n",
            "tensor([[ -1.4048],\n",
            "        [  1.7605],\n",
            "        [  5.8771],\n",
            "        [-13.9300],\n",
            "        [ 10.6372]], grad_fn=<AddBackward0>)\n",
            "Epoch 803/1000, Loss: 0.07619874179363251\n",
            "tensor([[ -1.4058],\n",
            "        [  1.7615],\n",
            "        [  5.8782],\n",
            "        [-13.9373],\n",
            "        [ 10.6431]], grad_fn=<AddBackward0>)\n",
            "Epoch 804/1000, Loss: 0.076126828789711\n",
            "tensor([[ -1.4069],\n",
            "        [  1.7625],\n",
            "        [  5.8793],\n",
            "        [-13.9447],\n",
            "        [ 10.6490]], grad_fn=<AddBackward0>)\n",
            "Epoch 805/1000, Loss: 0.07605500519275665\n",
            "tensor([[ -1.4080],\n",
            "        [  1.7635],\n",
            "        [  5.8805],\n",
            "        [-13.9520],\n",
            "        [ 10.6549]], grad_fn=<AddBackward0>)\n",
            "Epoch 806/1000, Loss: 0.07598333060741425\n",
            "tensor([[ -1.4090],\n",
            "        [  1.7644],\n",
            "        [  5.8816],\n",
            "        [-13.9593],\n",
            "        [ 10.6608]], grad_fn=<AddBackward0>)\n",
            "Epoch 807/1000, Loss: 0.075911745429039\n",
            "tensor([[ -1.4101],\n",
            "        [  1.7654],\n",
            "        [  5.8827],\n",
            "        [-13.9667],\n",
            "        [ 10.6666]], grad_fn=<AddBackward0>)\n",
            "Epoch 808/1000, Loss: 0.07584025710821152\n",
            "tensor([[ -1.4111],\n",
            "        [  1.7664],\n",
            "        [  5.8838],\n",
            "        [-13.9740],\n",
            "        [ 10.6725]], grad_fn=<AddBackward0>)\n",
            "Epoch 809/1000, Loss: 0.07576888054609299\n",
            "tensor([[ -1.4122],\n",
            "        [  1.7674],\n",
            "        [  5.8849],\n",
            "        [-13.9813],\n",
            "        [ 10.6784]], grad_fn=<AddBackward0>)\n",
            "Epoch 810/1000, Loss: 0.07569770514965057\n",
            "tensor([[ -1.4133],\n",
            "        [  1.7684],\n",
            "        [  5.8860],\n",
            "        [-13.9886],\n",
            "        [ 10.6842]], grad_fn=<AddBackward0>)\n",
            "Epoch 811/1000, Loss: 0.07562664896249771\n",
            "tensor([[ -1.4143],\n",
            "        [  1.7694],\n",
            "        [  5.8871],\n",
            "        [-13.9959],\n",
            "        [ 10.6901]], grad_fn=<AddBackward0>)\n",
            "Epoch 812/1000, Loss: 0.0755557268857956\n",
            "tensor([[ -1.4154],\n",
            "        [  1.7704],\n",
            "        [  5.8883],\n",
            "        [-14.0032],\n",
            "        [ 10.6959]], grad_fn=<AddBackward0>)\n",
            "Epoch 813/1000, Loss: 0.07548486441373825\n",
            "tensor([[ -1.4164],\n",
            "        [  1.7714],\n",
            "        [  5.8894],\n",
            "        [-14.0105],\n",
            "        [ 10.7018]], grad_fn=<AddBackward0>)\n",
            "Epoch 814/1000, Loss: 0.07541419565677643\n",
            "tensor([[ -1.4175],\n",
            "        [  1.7724],\n",
            "        [  5.8905],\n",
            "        [-14.0177],\n",
            "        [ 10.7076]], grad_fn=<AddBackward0>)\n",
            "Epoch 815/1000, Loss: 0.0753435343503952\n",
            "tensor([[ -1.4186],\n",
            "        [  1.7734],\n",
            "        [  5.8916],\n",
            "        [-14.0250],\n",
            "        [ 10.7135]], grad_fn=<AddBackward0>)\n",
            "Epoch 816/1000, Loss: 0.07527311891317368\n",
            "tensor([[ -1.4196],\n",
            "        [  1.7743],\n",
            "        [  5.8927],\n",
            "        [-14.0323],\n",
            "        [ 10.7193]], grad_fn=<AddBackward0>)\n",
            "Epoch 817/1000, Loss: 0.0752028077840805\n",
            "tensor([[ -1.4207],\n",
            "        [  1.7753],\n",
            "        [  5.8938],\n",
            "        [-14.0395],\n",
            "        [ 10.7251]], grad_fn=<AddBackward0>)\n",
            "Epoch 818/1000, Loss: 0.07513247430324554\n",
            "tensor([[ -1.4217],\n",
            "        [  1.7763],\n",
            "        [  5.8949],\n",
            "        [-14.0468],\n",
            "        [ 10.7310]], grad_fn=<AddBackward0>)\n",
            "Epoch 819/1000, Loss: 0.07506231963634491\n",
            "tensor([[ -1.4228],\n",
            "        [  1.7773],\n",
            "        [  5.8960],\n",
            "        [-14.0540],\n",
            "        [ 10.7368]], grad_fn=<AddBackward0>)\n",
            "Epoch 820/1000, Loss: 0.0749923437833786\n",
            "tensor([[ -1.4238],\n",
            "        [  1.7783],\n",
            "        [  5.8971],\n",
            "        [-14.0613],\n",
            "        [ 10.7426]], grad_fn=<AddBackward0>)\n",
            "Epoch 821/1000, Loss: 0.07492250204086304\n",
            "tensor([[ -1.4249],\n",
            "        [  1.7793],\n",
            "        [  5.8982],\n",
            "        [-14.0685],\n",
            "        [ 10.7484]], grad_fn=<AddBackward0>)\n",
            "Epoch 822/1000, Loss: 0.07485268265008926\n",
            "tensor([[ -1.4259],\n",
            "        [  1.7802],\n",
            "        [  5.8993],\n",
            "        [-14.0757],\n",
            "        [ 10.7542]], grad_fn=<AddBackward0>)\n",
            "Epoch 823/1000, Loss: 0.07478306442499161\n",
            "tensor([[ -1.4270],\n",
            "        [  1.7812],\n",
            "        [  5.9004],\n",
            "        [-14.0830],\n",
            "        [ 10.7600]], grad_fn=<AddBackward0>)\n",
            "Epoch 824/1000, Loss: 0.07471351325511932\n",
            "tensor([[ -1.4280],\n",
            "        [  1.7822],\n",
            "        [  5.9015],\n",
            "        [-14.0902],\n",
            "        [ 10.7658]], grad_fn=<AddBackward0>)\n",
            "Epoch 825/1000, Loss: 0.07464413344860077\n",
            "tensor([[ -1.4291],\n",
            "        [  1.7832],\n",
            "        [  5.9026],\n",
            "        [-14.0974],\n",
            "        [ 10.7716]], grad_fn=<AddBackward0>)\n",
            "Epoch 826/1000, Loss: 0.0745747834444046\n",
            "tensor([[ -1.4301],\n",
            "        [  1.7842],\n",
            "        [  5.9037],\n",
            "        [-14.1046],\n",
            "        [ 10.7774]], grad_fn=<AddBackward0>)\n",
            "Epoch 827/1000, Loss: 0.07450566440820694\n",
            "tensor([[ -1.4312],\n",
            "        [  1.7851],\n",
            "        [  5.9048],\n",
            "        [-14.1118],\n",
            "        [ 10.7831]], grad_fn=<AddBackward0>)\n",
            "Epoch 828/1000, Loss: 0.07443662732839584\n",
            "tensor([[ -1.4322],\n",
            "        [  1.7861],\n",
            "        [  5.9059],\n",
            "        [-14.1190],\n",
            "        [ 10.7889]], grad_fn=<AddBackward0>)\n",
            "Epoch 829/1000, Loss: 0.07436772435903549\n",
            "tensor([[ -1.4332],\n",
            "        [  1.7871],\n",
            "        [  5.9070],\n",
            "        [-14.1261],\n",
            "        [ 10.7947]], grad_fn=<AddBackward0>)\n",
            "Epoch 830/1000, Loss: 0.07429885864257812\n",
            "tensor([[ -1.4343],\n",
            "        [  1.7881],\n",
            "        [  5.9081],\n",
            "        [-14.1333],\n",
            "        [ 10.8005]], grad_fn=<AddBackward0>)\n",
            "Epoch 831/1000, Loss: 0.0742301195859909\n",
            "tensor([[ -1.4353],\n",
            "        [  1.7890],\n",
            "        [  5.9092],\n",
            "        [-14.1405],\n",
            "        [ 10.8062]], grad_fn=<AddBackward0>)\n",
            "Epoch 832/1000, Loss: 0.07416150718927383\n",
            "tensor([[ -1.4364],\n",
            "        [  1.7900],\n",
            "        [  5.9103],\n",
            "        [-14.1477],\n",
            "        [ 10.8120]], grad_fn=<AddBackward0>)\n",
            "Epoch 833/1000, Loss: 0.0740930363535881\n",
            "tensor([[ -1.4374],\n",
            "        [  1.7910],\n",
            "        [  5.9113],\n",
            "        [-14.1548],\n",
            "        [ 10.8177]], grad_fn=<AddBackward0>)\n",
            "Epoch 834/1000, Loss: 0.07402466982603073\n",
            "tensor([[ -1.4384],\n",
            "        [  1.7920],\n",
            "        [  5.9124],\n",
            "        [-14.1620],\n",
            "        [ 10.8235]], grad_fn=<AddBackward0>)\n",
            "Epoch 835/1000, Loss: 0.07395638525485992\n",
            "tensor([[ -1.4395],\n",
            "        [  1.7929],\n",
            "        [  5.9135],\n",
            "        [-14.1691],\n",
            "        [ 10.8292]], grad_fn=<AddBackward0>)\n",
            "Epoch 836/1000, Loss: 0.07388828694820404\n",
            "tensor([[ -1.4405],\n",
            "        [  1.7939],\n",
            "        [  5.9146],\n",
            "        [-14.1763],\n",
            "        [ 10.8349]], grad_fn=<AddBackward0>)\n",
            "Epoch 837/1000, Loss: 0.07382030040025711\n",
            "tensor([[ -1.4416],\n",
            "        [  1.7949],\n",
            "        [  5.9157],\n",
            "        [-14.1834],\n",
            "        [ 10.8407]], grad_fn=<AddBackward0>)\n",
            "Epoch 838/1000, Loss: 0.07375236600637436\n",
            "tensor([[ -1.4426],\n",
            "        [  1.7958],\n",
            "        [  5.9168],\n",
            "        [-14.1905],\n",
            "        [ 10.8464]], grad_fn=<AddBackward0>)\n",
            "Epoch 839/1000, Loss: 0.07368455827236176\n",
            "tensor([[ -1.4436],\n",
            "        [  1.7968],\n",
            "        [  5.9178],\n",
            "        [-14.1976],\n",
            "        [ 10.8521]], grad_fn=<AddBackward0>)\n",
            "Epoch 840/1000, Loss: 0.07361694425344467\n",
            "tensor([[ -1.4447],\n",
            "        [  1.7978],\n",
            "        [  5.9189],\n",
            "        [-14.2047],\n",
            "        [ 10.8578]], grad_fn=<AddBackward0>)\n",
            "Epoch 841/1000, Loss: 0.07354936748743057\n",
            "tensor([[ -1.4457],\n",
            "        [  1.7987],\n",
            "        [  5.9200],\n",
            "        [-14.2119],\n",
            "        [ 10.8635]], grad_fn=<AddBackward0>)\n",
            "Epoch 842/1000, Loss: 0.0734819546341896\n",
            "tensor([[ -1.4467],\n",
            "        [  1.7997],\n",
            "        [  5.9211],\n",
            "        [-14.2190],\n",
            "        [ 10.8693]], grad_fn=<AddBackward0>)\n",
            "Epoch 843/1000, Loss: 0.0734146237373352\n",
            "tensor([[ -1.4478],\n",
            "        [  1.8007],\n",
            "        [  5.9222],\n",
            "        [-14.2260],\n",
            "        [ 10.8750]], grad_fn=<AddBackward0>)\n",
            "Epoch 844/1000, Loss: 0.07334735989570618\n",
            "tensor([[ -1.4488],\n",
            "        [  1.8016],\n",
            "        [  5.9232],\n",
            "        [-14.2331],\n",
            "        [ 10.8806]], grad_fn=<AddBackward0>)\n",
            "Epoch 845/1000, Loss: 0.07328023761510849\n",
            "tensor([[ -1.4498],\n",
            "        [  1.8026],\n",
            "        [  5.9243],\n",
            "        [-14.2402],\n",
            "        [ 10.8863]], grad_fn=<AddBackward0>)\n",
            "Epoch 846/1000, Loss: 0.07321323454380035\n",
            "tensor([[ -1.4508],\n",
            "        [  1.8036],\n",
            "        [  5.9254],\n",
            "        [-14.2473],\n",
            "        [ 10.8920]], grad_fn=<AddBackward0>)\n",
            "Epoch 847/1000, Loss: 0.0731462836265564\n",
            "tensor([[ -1.4519],\n",
            "        [  1.8045],\n",
            "        [  5.9265],\n",
            "        [-14.2544],\n",
            "        [ 10.8977]], grad_fn=<AddBackward0>)\n",
            "Epoch 848/1000, Loss: 0.07307952642440796\n",
            "tensor([[ -1.4529],\n",
            "        [  1.8055],\n",
            "        [  5.9275],\n",
            "        [-14.2614],\n",
            "        [ 10.9034]], grad_fn=<AddBackward0>)\n",
            "Epoch 849/1000, Loss: 0.07301285862922668\n",
            "tensor([[ -1.4539],\n",
            "        [  1.8065],\n",
            "        [  5.9286],\n",
            "        [-14.2685],\n",
            "        [ 10.9091]], grad_fn=<AddBackward0>)\n",
            "Epoch 850/1000, Loss: 0.07294630259275436\n",
            "tensor([[ -1.4549],\n",
            "        [  1.8074],\n",
            "        [  5.9297],\n",
            "        [-14.2756],\n",
            "        [ 10.9147]], grad_fn=<AddBackward0>)\n",
            "Epoch 851/1000, Loss: 0.07287988811731339\n",
            "tensor([[ -1.4560],\n",
            "        [  1.8084],\n",
            "        [  5.9307],\n",
            "        [-14.2826],\n",
            "        [ 10.9204]], grad_fn=<AddBackward0>)\n",
            "Epoch 852/1000, Loss: 0.07281354814767838\n",
            "tensor([[ -1.4570],\n",
            "        [  1.8093],\n",
            "        [  5.9318],\n",
            "        [-14.2896],\n",
            "        [ 10.9260]], grad_fn=<AddBackward0>)\n",
            "Epoch 853/1000, Loss: 0.07274727523326874\n",
            "tensor([[ -1.4580],\n",
            "        [  1.8103],\n",
            "        [  5.9329],\n",
            "        [-14.2967],\n",
            "        [ 10.9317]], grad_fn=<AddBackward0>)\n",
            "Epoch 854/1000, Loss: 0.07268121838569641\n",
            "tensor([[ -1.4590],\n",
            "        [  1.8112],\n",
            "        [  5.9339],\n",
            "        [-14.3037],\n",
            "        [ 10.9374]], grad_fn=<AddBackward0>)\n",
            "Epoch 855/1000, Loss: 0.0726151168346405\n",
            "tensor([[ -1.4600],\n",
            "        [  1.8122],\n",
            "        [  5.9350],\n",
            "        [-14.3107],\n",
            "        [ 10.9430]], grad_fn=<AddBackward0>)\n",
            "Epoch 856/1000, Loss: 0.07254929095506668\n",
            "tensor([[ -1.4611],\n",
            "        [  1.8132],\n",
            "        [  5.9361],\n",
            "        [-14.3177],\n",
            "        [ 10.9486]], grad_fn=<AddBackward0>)\n",
            "Epoch 857/1000, Loss: 0.07248350977897644\n",
            "tensor([[ -1.4621],\n",
            "        [  1.8141],\n",
            "        [  5.9371],\n",
            "        [-14.3248],\n",
            "        [ 10.9543]], grad_fn=<AddBackward0>)\n",
            "Epoch 858/1000, Loss: 0.07241782546043396\n",
            "tensor([[ -1.4631],\n",
            "        [  1.8151],\n",
            "        [  5.9382],\n",
            "        [-14.3318],\n",
            "        [ 10.9599]], grad_fn=<AddBackward0>)\n",
            "Epoch 859/1000, Loss: 0.07235220819711685\n",
            "tensor([[ -1.4641],\n",
            "        [  1.8160],\n",
            "        [  5.9392],\n",
            "        [-14.3388],\n",
            "        [ 10.9655]], grad_fn=<AddBackward0>)\n",
            "Epoch 860/1000, Loss: 0.07228682190179825\n",
            "tensor([[ -1.4651],\n",
            "        [  1.8170],\n",
            "        [  5.9403],\n",
            "        [-14.3458],\n",
            "        [ 10.9711]], grad_fn=<AddBackward0>)\n",
            "Epoch 861/1000, Loss: 0.07222142070531845\n",
            "tensor([[ -1.4662],\n",
            "        [  1.8179],\n",
            "        [  5.9414],\n",
            "        [-14.3527],\n",
            "        [ 10.9768]], grad_fn=<AddBackward0>)\n",
            "Epoch 862/1000, Loss: 0.07215619087219238\n",
            "tensor([[ -1.4672],\n",
            "        [  1.8189],\n",
            "        [  5.9424],\n",
            "        [-14.3597],\n",
            "        [ 10.9824]], grad_fn=<AddBackward0>)\n",
            "Epoch 863/1000, Loss: 0.07209109514951706\n",
            "tensor([[ -1.4682],\n",
            "        [  1.8198],\n",
            "        [  5.9435],\n",
            "        [-14.3667],\n",
            "        [ 10.9880]], grad_fn=<AddBackward0>)\n",
            "Epoch 864/1000, Loss: 0.07202605903148651\n",
            "tensor([[ -1.4692],\n",
            "        [  1.8208],\n",
            "        [  5.9445],\n",
            "        [-14.3737],\n",
            "        [ 10.9936]], grad_fn=<AddBackward0>)\n",
            "Epoch 865/1000, Loss: 0.07196109741926193\n",
            "tensor([[ -1.4702],\n",
            "        [  1.8217],\n",
            "        [  5.9456],\n",
            "        [-14.3806],\n",
            "        [ 10.9992]], grad_fn=<AddBackward0>)\n",
            "Epoch 866/1000, Loss: 0.0718962624669075\n",
            "tensor([[ -1.4712],\n",
            "        [  1.8227],\n",
            "        [  5.9466],\n",
            "        [-14.3876],\n",
            "        [ 11.0048]], grad_fn=<AddBackward0>)\n",
            "Epoch 867/1000, Loss: 0.0718315839767456\n",
            "tensor([[ -1.4722],\n",
            "        [  1.8236],\n",
            "        [  5.9477],\n",
            "        [-14.3945],\n",
            "        [ 11.0104]], grad_fn=<AddBackward0>)\n",
            "Epoch 868/1000, Loss: 0.0717669352889061\n",
            "tensor([[ -1.4732],\n",
            "        [  1.8246],\n",
            "        [  5.9487],\n",
            "        [-14.4015],\n",
            "        [ 11.0159]], grad_fn=<AddBackward0>)\n",
            "Epoch 869/1000, Loss: 0.07170248031616211\n",
            "tensor([[ -1.4742],\n",
            "        [  1.8255],\n",
            "        [  5.9498],\n",
            "        [-14.4084],\n",
            "        [ 11.0215]], grad_fn=<AddBackward0>)\n",
            "Epoch 870/1000, Loss: 0.07163811475038528\n",
            "tensor([[ -1.4752],\n",
            "        [  1.8265],\n",
            "        [  5.9508],\n",
            "        [-14.4154],\n",
            "        [ 11.0271]], grad_fn=<AddBackward0>)\n",
            "Epoch 871/1000, Loss: 0.07157379388809204\n",
            "tensor([[ -1.4763],\n",
            "        [  1.8274],\n",
            "        [  5.9519],\n",
            "        [-14.4223],\n",
            "        [ 11.0327]], grad_fn=<AddBackward0>)\n",
            "Epoch 872/1000, Loss: 0.0715097114443779\n",
            "tensor([[ -1.4773],\n",
            "        [  1.8283],\n",
            "        [  5.9529],\n",
            "        [-14.4292],\n",
            "        [ 11.0382]], grad_fn=<AddBackward0>)\n",
            "Epoch 873/1000, Loss: 0.07144557684659958\n",
            "tensor([[ -1.4783],\n",
            "        [  1.8293],\n",
            "        [  5.9540],\n",
            "        [-14.4361],\n",
            "        [ 11.0438]], grad_fn=<AddBackward0>)\n",
            "Epoch 874/1000, Loss: 0.07138156145811081\n",
            "tensor([[ -1.4793],\n",
            "        [  1.8302],\n",
            "        [  5.9550],\n",
            "        [-14.4430],\n",
            "        [ 11.0493]], grad_fn=<AddBackward0>)\n",
            "Epoch 875/1000, Loss: 0.07131771743297577\n",
            "tensor([[ -1.4803],\n",
            "        [  1.8312],\n",
            "        [  5.9561],\n",
            "        [-14.4499],\n",
            "        [ 11.0549]], grad_fn=<AddBackward0>)\n",
            "Epoch 876/1000, Loss: 0.07125391811132431\n",
            "tensor([[ -1.4813],\n",
            "        [  1.8321],\n",
            "        [  5.9571],\n",
            "        [-14.4568],\n",
            "        [ 11.0604]], grad_fn=<AddBackward0>)\n",
            "Epoch 877/1000, Loss: 0.07119028270244598\n",
            "tensor([[ -1.4823],\n",
            "        [  1.8331],\n",
            "        [  5.9582],\n",
            "        [-14.4637],\n",
            "        [ 11.0660]], grad_fn=<AddBackward0>)\n",
            "Epoch 878/1000, Loss: 0.07112669199705124\n",
            "tensor([[ -1.4833],\n",
            "        [  1.8340],\n",
            "        [  5.9592],\n",
            "        [-14.4706],\n",
            "        [ 11.0715]], grad_fn=<AddBackward0>)\n",
            "Epoch 879/1000, Loss: 0.07106330990791321\n",
            "tensor([[ -1.4843],\n",
            "        [  1.8349],\n",
            "        [  5.9602],\n",
            "        [-14.4775],\n",
            "        [ 11.0771]], grad_fn=<AddBackward0>)\n",
            "Epoch 880/1000, Loss: 0.07099996507167816\n",
            "tensor([[ -1.4853],\n",
            "        [  1.8359],\n",
            "        [  5.9613],\n",
            "        [-14.4844],\n",
            "        [ 11.0826]], grad_fn=<AddBackward0>)\n",
            "Epoch 881/1000, Loss: 0.0709366500377655\n",
            "tensor([[ -1.4863],\n",
            "        [  1.8368],\n",
            "        [  5.9623],\n",
            "        [-14.4912],\n",
            "        [ 11.0881]], grad_fn=<AddBackward0>)\n",
            "Epoch 882/1000, Loss: 0.07087354362010956\n",
            "tensor([[ -1.4873],\n",
            "        [  1.8377],\n",
            "        [  5.9634],\n",
            "        [-14.4981],\n",
            "        [ 11.0936]], grad_fn=<AddBackward0>)\n",
            "Epoch 883/1000, Loss: 0.07081045210361481\n",
            "tensor([[ -1.4883],\n",
            "        [  1.8387],\n",
            "        [  5.9644],\n",
            "        [-14.5050],\n",
            "        [ 11.0991]], grad_fn=<AddBackward0>)\n",
            "Epoch 884/1000, Loss: 0.07074759155511856\n",
            "tensor([[ -1.4893],\n",
            "        [  1.8396],\n",
            "        [  5.9654],\n",
            "        [-14.5118],\n",
            "        [ 11.1046]], grad_fn=<AddBackward0>)\n",
            "Epoch 885/1000, Loss: 0.07068470865488052\n",
            "tensor([[ -1.4902],\n",
            "        [  1.8405],\n",
            "        [  5.9665],\n",
            "        [-14.5187],\n",
            "        [ 11.1102]], grad_fn=<AddBackward0>)\n",
            "Epoch 886/1000, Loss: 0.0706220269203186\n",
            "tensor([[ -1.4912],\n",
            "        [  1.8415],\n",
            "        [  5.9675],\n",
            "        [-14.5255],\n",
            "        [ 11.1157]], grad_fn=<AddBackward0>)\n",
            "Epoch 887/1000, Loss: 0.07055941969156265\n",
            "tensor([[ -1.4922],\n",
            "        [  1.8424],\n",
            "        [  5.9685],\n",
            "        [-14.5323],\n",
            "        [ 11.1212]], grad_fn=<AddBackward0>)\n",
            "Epoch 888/1000, Loss: 0.07049686461687088\n",
            "tensor([[ -1.4932],\n",
            "        [  1.8433],\n",
            "        [  5.9696],\n",
            "        [-14.5392],\n",
            "        [ 11.1266]], grad_fn=<AddBackward0>)\n",
            "Epoch 889/1000, Loss: 0.07043444365262985\n",
            "tensor([[ -1.4942],\n",
            "        [  1.8443],\n",
            "        [  5.9706],\n",
            "        [-14.5460],\n",
            "        [ 11.1321]], grad_fn=<AddBackward0>)\n",
            "Epoch 890/1000, Loss: 0.07037217170000076\n",
            "tensor([[ -1.4952],\n",
            "        [  1.8452],\n",
            "        [  5.9716],\n",
            "        [-14.5528],\n",
            "        [ 11.1376]], grad_fn=<AddBackward0>)\n",
            "Epoch 891/1000, Loss: 0.07030986994504929\n",
            "tensor([[ -1.4962],\n",
            "        [  1.8461],\n",
            "        [  5.9726],\n",
            "        [-14.5596],\n",
            "        [ 11.1431]], grad_fn=<AddBackward0>)\n",
            "Epoch 892/1000, Loss: 0.07024776190519333\n",
            "tensor([[ -1.4972],\n",
            "        [  1.8471],\n",
            "        [  5.9737],\n",
            "        [-14.5664],\n",
            "        [ 11.1486]], grad_fn=<AddBackward0>)\n",
            "Epoch 893/1000, Loss: 0.07018576562404633\n",
            "tensor([[ -1.4982],\n",
            "        [  1.8480],\n",
            "        [  5.9747],\n",
            "        [-14.5732],\n",
            "        [ 11.1540]], grad_fn=<AddBackward0>)\n",
            "Epoch 894/1000, Loss: 0.07012386620044708\n",
            "tensor([[ -1.4992],\n",
            "        [  1.8489],\n",
            "        [  5.9757],\n",
            "        [-14.5800],\n",
            "        [ 11.1595]], grad_fn=<AddBackward0>)\n",
            "Epoch 895/1000, Loss: 0.07006194442510605\n",
            "tensor([[ -1.5001],\n",
            "        [  1.8498],\n",
            "        [  5.9767],\n",
            "        [-14.5868],\n",
            "        [ 11.1650]], grad_fn=<AddBackward0>)\n",
            "Epoch 896/1000, Loss: 0.07000026106834412\n",
            "tensor([[ -1.5011],\n",
            "        [  1.8508],\n",
            "        [  5.9778],\n",
            "        [-14.5936],\n",
            "        [ 11.1704]], grad_fn=<AddBackward0>)\n",
            "Epoch 897/1000, Loss: 0.06993864476680756\n",
            "tensor([[ -1.5021],\n",
            "        [  1.8517],\n",
            "        [  5.9788],\n",
            "        [-14.6004],\n",
            "        [ 11.1759]], grad_fn=<AddBackward0>)\n",
            "Epoch 898/1000, Loss: 0.06987719237804413\n",
            "tensor([[ -1.5031],\n",
            "        [  1.8526],\n",
            "        [  5.9798],\n",
            "        [-14.6072],\n",
            "        [ 11.1813]], grad_fn=<AddBackward0>)\n",
            "Epoch 899/1000, Loss: 0.06981565058231354\n",
            "tensor([[ -1.5041],\n",
            "        [  1.8535],\n",
            "        [  5.9808],\n",
            "        [-14.6139],\n",
            "        [ 11.1868]], grad_fn=<AddBackward0>)\n",
            "Epoch 900/1000, Loss: 0.06975442171096802\n",
            "tensor([[ -1.5051],\n",
            "        [  1.8545],\n",
            "        [  5.9819],\n",
            "        [-14.6207],\n",
            "        [ 11.1922]], grad_fn=<AddBackward0>)\n",
            "Epoch 901/1000, Loss: 0.06969311833381653\n",
            "tensor([[ -1.5060],\n",
            "        [  1.8554],\n",
            "        [  5.9829],\n",
            "        [-14.6274],\n",
            "        [ 11.1976]], grad_fn=<AddBackward0>)\n",
            "Epoch 902/1000, Loss: 0.06963203847408295\n",
            "tensor([[ -1.5070],\n",
            "        [  1.8563],\n",
            "        [  5.9839],\n",
            "        [-14.6342],\n",
            "        [ 11.2031]], grad_fn=<AddBackward0>)\n",
            "Epoch 903/1000, Loss: 0.06957098096609116\n",
            "tensor([[ -1.5080],\n",
            "        [  1.8572],\n",
            "        [  5.9849],\n",
            "        [-14.6409],\n",
            "        [ 11.2085]], grad_fn=<AddBackward0>)\n",
            "Epoch 904/1000, Loss: 0.06951005756855011\n",
            "tensor([[ -1.5090],\n",
            "        [  1.8582],\n",
            "        [  5.9859],\n",
            "        [-14.6477],\n",
            "        [ 11.2139]], grad_fn=<AddBackward0>)\n",
            "Epoch 905/1000, Loss: 0.06944926083087921\n",
            "tensor([[ -1.5100],\n",
            "        [  1.8591],\n",
            "        [  5.9869],\n",
            "        [-14.6544],\n",
            "        [ 11.2193]], grad_fn=<AddBackward0>)\n",
            "Epoch 906/1000, Loss: 0.0693885087966919\n",
            "tensor([[ -1.5109],\n",
            "        [  1.8600],\n",
            "        [  5.9879],\n",
            "        [-14.6611],\n",
            "        [ 11.2247]], grad_fn=<AddBackward0>)\n",
            "Epoch 907/1000, Loss: 0.06932786107063293\n",
            "tensor([[ -1.5119],\n",
            "        [  1.8609],\n",
            "        [  5.9890],\n",
            "        [-14.6679],\n",
            "        [ 11.2302]], grad_fn=<AddBackward0>)\n",
            "Epoch 908/1000, Loss: 0.06926731765270233\n",
            "tensor([[ -1.5129],\n",
            "        [  1.8618],\n",
            "        [  5.9900],\n",
            "        [-14.6746],\n",
            "        [ 11.2356]], grad_fn=<AddBackward0>)\n",
            "Epoch 909/1000, Loss: 0.0692068487405777\n",
            "tensor([[ -1.5139],\n",
            "        [  1.8627],\n",
            "        [  5.9910],\n",
            "        [-14.6813],\n",
            "        [ 11.2410]], grad_fn=<AddBackward0>)\n",
            "Epoch 910/1000, Loss: 0.06914650648832321\n",
            "tensor([[ -1.5148],\n",
            "        [  1.8637],\n",
            "        [  5.9920],\n",
            "        [-14.6880],\n",
            "        [ 11.2464]], grad_fn=<AddBackward0>)\n",
            "Epoch 911/1000, Loss: 0.06908629834651947\n",
            "tensor([[ -1.5158],\n",
            "        [  1.8646],\n",
            "        [  5.9930],\n",
            "        [-14.6947],\n",
            "        [ 11.2517]], grad_fn=<AddBackward0>)\n",
            "Epoch 912/1000, Loss: 0.0690261498093605\n",
            "tensor([[ -1.5168],\n",
            "        [  1.8655],\n",
            "        [  5.9940],\n",
            "        [-14.7014],\n",
            "        [ 11.2571]], grad_fn=<AddBackward0>)\n",
            "Epoch 913/1000, Loss: 0.0689660906791687\n",
            "tensor([[ -1.5178],\n",
            "        [  1.8664],\n",
            "        [  5.9950],\n",
            "        [-14.7081],\n",
            "        [ 11.2625]], grad_fn=<AddBackward0>)\n",
            "Epoch 914/1000, Loss: 0.06890605390071869\n",
            "tensor([[ -1.5187],\n",
            "        [  1.8673],\n",
            "        [  5.9960],\n",
            "        [-14.7148],\n",
            "        [ 11.2679]], grad_fn=<AddBackward0>)\n",
            "Epoch 915/1000, Loss: 0.06884624809026718\n",
            "tensor([[ -1.5197],\n",
            "        [  1.8682],\n",
            "        [  5.9970],\n",
            "        [-14.7214],\n",
            "        [ 11.2733]], grad_fn=<AddBackward0>)\n",
            "Epoch 916/1000, Loss: 0.0687863752245903\n",
            "tensor([[ -1.5207],\n",
            "        [  1.8691],\n",
            "        [  5.9980],\n",
            "        [-14.7281],\n",
            "        [ 11.2786]], grad_fn=<AddBackward0>)\n",
            "Epoch 917/1000, Loss: 0.06872672587633133\n",
            "tensor([[ -1.5216],\n",
            "        [  1.8701],\n",
            "        [  5.9990],\n",
            "        [-14.7348],\n",
            "        [ 11.2840]], grad_fn=<AddBackward0>)\n",
            "Epoch 918/1000, Loss: 0.06866706907749176\n",
            "tensor([[ -1.5226],\n",
            "        [  1.8710],\n",
            "        [  6.0000],\n",
            "        [-14.7414],\n",
            "        [ 11.2894]], grad_fn=<AddBackward0>)\n",
            "Epoch 919/1000, Loss: 0.06860774010419846\n",
            "tensor([[ -1.5236],\n",
            "        [  1.8719],\n",
            "        [  6.0010],\n",
            "        [-14.7481],\n",
            "        [ 11.2947]], grad_fn=<AddBackward0>)\n",
            "Epoch 920/1000, Loss: 0.06854821741580963\n",
            "tensor([[ -1.5245],\n",
            "        [  1.8728],\n",
            "        [  6.0020],\n",
            "        [-14.7548],\n",
            "        [ 11.3001]], grad_fn=<AddBackward0>)\n",
            "Epoch 921/1000, Loss: 0.0684889405965805\n",
            "tensor([[ -1.5255],\n",
            "        [  1.8737],\n",
            "        [  6.0030],\n",
            "        [-14.7614],\n",
            "        [ 11.3054]], grad_fn=<AddBackward0>)\n",
            "Epoch 922/1000, Loss: 0.06842978298664093\n",
            "tensor([[ -1.5265],\n",
            "        [  1.8746],\n",
            "        [  6.0040],\n",
            "        [-14.7680],\n",
            "        [ 11.3108]], grad_fn=<AddBackward0>)\n",
            "Epoch 923/1000, Loss: 0.06837067753076553\n",
            "tensor([[ -1.5274],\n",
            "        [  1.8755],\n",
            "        [  6.0050],\n",
            "        [-14.7747],\n",
            "        [ 11.3161]], grad_fn=<AddBackward0>)\n",
            "Epoch 924/1000, Loss: 0.06831170618534088\n",
            "tensor([[ -1.5284],\n",
            "        [  1.8764],\n",
            "        [  6.0060],\n",
            "        [-14.7813],\n",
            "        [ 11.3214]], grad_fn=<AddBackward0>)\n",
            "Epoch 925/1000, Loss: 0.06825270503759384\n",
            "tensor([[ -1.5294],\n",
            "        [  1.8773],\n",
            "        [  6.0070],\n",
            "        [-14.7879],\n",
            "        [ 11.3268]], grad_fn=<AddBackward0>)\n",
            "Epoch 926/1000, Loss: 0.06819391995668411\n",
            "tensor([[ -1.5303],\n",
            "        [  1.8782],\n",
            "        [  6.0080],\n",
            "        [-14.7945],\n",
            "        [ 11.3321]], grad_fn=<AddBackward0>)\n",
            "Epoch 927/1000, Loss: 0.06813515722751617\n",
            "tensor([[ -1.5313],\n",
            "        [  1.8791],\n",
            "        [  6.0090],\n",
            "        [-14.8012],\n",
            "        [ 11.3374]], grad_fn=<AddBackward0>)\n",
            "Epoch 928/1000, Loss: 0.06807649880647659\n",
            "tensor([[ -1.5322],\n",
            "        [  1.8800],\n",
            "        [  6.0100],\n",
            "        [-14.8078],\n",
            "        [ 11.3427]], grad_fn=<AddBackward0>)\n",
            "Epoch 929/1000, Loss: 0.0680178850889206\n",
            "tensor([[ -1.5332],\n",
            "        [  1.8809],\n",
            "        [  6.0110],\n",
            "        [-14.8144],\n",
            "        [ 11.3481]], grad_fn=<AddBackward0>)\n",
            "Epoch 930/1000, Loss: 0.0679594948887825\n",
            "tensor([[ -1.5341],\n",
            "        [  1.8818],\n",
            "        [  6.0120],\n",
            "        [-14.8210],\n",
            "        [ 11.3534]], grad_fn=<AddBackward0>)\n",
            "Epoch 931/1000, Loss: 0.06790115684270859\n",
            "tensor([[ -1.5351],\n",
            "        [  1.8827],\n",
            "        [  6.0130],\n",
            "        [-14.8276],\n",
            "        [ 11.3587]], grad_fn=<AddBackward0>)\n",
            "Epoch 932/1000, Loss: 0.06784281879663467\n",
            "tensor([[ -1.5361],\n",
            "        [  1.8836],\n",
            "        [  6.0140],\n",
            "        [-14.8341],\n",
            "        [ 11.3640]], grad_fn=<AddBackward0>)\n",
            "Epoch 933/1000, Loss: 0.0677846223115921\n",
            "tensor([[ -1.5370],\n",
            "        [  1.8846],\n",
            "        [  6.0150],\n",
            "        [-14.8407],\n",
            "        [ 11.3693]], grad_fn=<AddBackward0>)\n",
            "Epoch 934/1000, Loss: 0.0677265152335167\n",
            "tensor([[ -1.5380],\n",
            "        [  1.8854],\n",
            "        [  6.0159],\n",
            "        [-14.8473],\n",
            "        [ 11.3746]], grad_fn=<AddBackward0>)\n",
            "Epoch 935/1000, Loss: 0.06766849756240845\n",
            "tensor([[ -1.5389],\n",
            "        [  1.8863],\n",
            "        [  6.0169],\n",
            "        [-14.8539],\n",
            "        [ 11.3798]], grad_fn=<AddBackward0>)\n",
            "Epoch 936/1000, Loss: 0.06761059165000916\n",
            "tensor([[ -1.5399],\n",
            "        [  1.8872],\n",
            "        [  6.0179],\n",
            "        [-14.8604],\n",
            "        [ 11.3851]], grad_fn=<AddBackward0>)\n",
            "Epoch 937/1000, Loss: 0.06755281984806061\n",
            "tensor([[ -1.5408],\n",
            "        [  1.8881],\n",
            "        [  6.0189],\n",
            "        [-14.8670],\n",
            "        [ 11.3904]], grad_fn=<AddBackward0>)\n",
            "Epoch 938/1000, Loss: 0.06749506294727325\n",
            "tensor([[ -1.5418],\n",
            "        [  1.8890],\n",
            "        [  6.0199],\n",
            "        [-14.8736],\n",
            "        [ 11.3957]], grad_fn=<AddBackward0>)\n",
            "Epoch 939/1000, Loss: 0.06743747740983963\n",
            "tensor([[ -1.5427],\n",
            "        [  1.8899],\n",
            "        [  6.0209],\n",
            "        [-14.8801],\n",
            "        [ 11.4010]], grad_fn=<AddBackward0>)\n",
            "Epoch 940/1000, Loss: 0.06737986207008362\n",
            "tensor([[ -1.5437],\n",
            "        [  1.8908],\n",
            "        [  6.0219],\n",
            "        [-14.8867],\n",
            "        [ 11.4062]], grad_fn=<AddBackward0>)\n",
            "Epoch 941/1000, Loss: 0.06732239574193954\n",
            "tensor([[ -1.5446],\n",
            "        [  1.8917],\n",
            "        [  6.0228],\n",
            "        [-14.8932],\n",
            "        [ 11.4115]], grad_fn=<AddBackward0>)\n",
            "Epoch 942/1000, Loss: 0.06726514548063278\n",
            "tensor([[ -1.5456],\n",
            "        [  1.8926],\n",
            "        [  6.0238],\n",
            "        [-14.8997],\n",
            "        [ 11.4167]], grad_fn=<AddBackward0>)\n",
            "Epoch 943/1000, Loss: 0.06720777601003647\n",
            "tensor([[ -1.5465],\n",
            "        [  1.8935],\n",
            "        [  6.0248],\n",
            "        [-14.9063],\n",
            "        [ 11.4220]], grad_fn=<AddBackward0>)\n",
            "Epoch 944/1000, Loss: 0.0671505481004715\n",
            "tensor([[ -1.5475],\n",
            "        [  1.8944],\n",
            "        [  6.0258],\n",
            "        [-14.9128],\n",
            "        [ 11.4273]], grad_fn=<AddBackward0>)\n",
            "Epoch 945/1000, Loss: 0.06709349155426025\n",
            "tensor([[ -1.5484],\n",
            "        [  1.8953],\n",
            "        [  6.0268],\n",
            "        [-14.9193],\n",
            "        [ 11.4325]], grad_fn=<AddBackward0>)\n",
            "Epoch 946/1000, Loss: 0.06703649461269379\n",
            "tensor([[ -1.5494],\n",
            "        [  1.8962],\n",
            "        [  6.0277],\n",
            "        [-14.9258],\n",
            "        [ 11.4377]], grad_fn=<AddBackward0>)\n",
            "Epoch 947/1000, Loss: 0.06697951257228851\n",
            "tensor([[ -1.5503],\n",
            "        [  1.8971],\n",
            "        [  6.0287],\n",
            "        [-14.9323],\n",
            "        [ 11.4430]], grad_fn=<AddBackward0>)\n",
            "Epoch 948/1000, Loss: 0.06692269444465637\n",
            "tensor([[ -1.5513],\n",
            "        [  1.8980],\n",
            "        [  6.0297],\n",
            "        [-14.9388],\n",
            "        [ 11.4482]], grad_fn=<AddBackward0>)\n",
            "Epoch 949/1000, Loss: 0.06686599552631378\n",
            "tensor([[ -1.5522],\n",
            "        [  1.8989],\n",
            "        [  6.0307],\n",
            "        [-14.9453],\n",
            "        [ 11.4534]], grad_fn=<AddBackward0>)\n",
            "Epoch 950/1000, Loss: 0.06680938601493835\n",
            "tensor([[ -1.5531],\n",
            "        [  1.8998],\n",
            "        [  6.0316],\n",
            "        [-14.9518],\n",
            "        [ 11.4587]], grad_fn=<AddBackward0>)\n",
            "Epoch 951/1000, Loss: 0.06675280630588531\n",
            "tensor([[ -1.5541],\n",
            "        [  1.9006],\n",
            "        [  6.0326],\n",
            "        [-14.9583],\n",
            "        [ 11.4639]], grad_fn=<AddBackward0>)\n",
            "Epoch 952/1000, Loss: 0.06669624149799347\n",
            "tensor([[ -1.5550],\n",
            "        [  1.9015],\n",
            "        [  6.0336],\n",
            "        [-14.9648],\n",
            "        [ 11.4691]], grad_fn=<AddBackward0>)\n",
            "Epoch 953/1000, Loss: 0.06663987785577774\n",
            "tensor([[ -1.5560],\n",
            "        [  1.9024],\n",
            "        [  6.0346],\n",
            "        [-14.9713],\n",
            "        [ 11.4743]], grad_fn=<AddBackward0>)\n",
            "Epoch 954/1000, Loss: 0.06658364832401276\n",
            "tensor([[ -1.5569],\n",
            "        [  1.9033],\n",
            "        [  6.0355],\n",
            "        [-14.9777],\n",
            "        [ 11.4795]], grad_fn=<AddBackward0>)\n",
            "Epoch 955/1000, Loss: 0.06652732938528061\n",
            "tensor([[ -1.5579],\n",
            "        [  1.9042],\n",
            "        [  6.0365],\n",
            "        [-14.9842],\n",
            "        [ 11.4847]], grad_fn=<AddBackward0>)\n",
            "Epoch 956/1000, Loss: 0.06647127866744995\n",
            "tensor([[ -1.5588],\n",
            "        [  1.9051],\n",
            "        [  6.0375],\n",
            "        [-14.9907],\n",
            "        [ 11.4900]], grad_fn=<AddBackward0>)\n",
            "Epoch 957/1000, Loss: 0.06641515344381332\n",
            "tensor([[ -1.5597],\n",
            "        [  1.9060],\n",
            "        [  6.0384],\n",
            "        [-14.9971],\n",
            "        [ 11.4951]], grad_fn=<AddBackward0>)\n",
            "Epoch 958/1000, Loss: 0.06635920703411102\n",
            "tensor([[ -1.5607],\n",
            "        [  1.9068],\n",
            "        [  6.0394],\n",
            "        [-15.0036],\n",
            "        [ 11.5003]], grad_fn=<AddBackward0>)\n",
            "Epoch 959/1000, Loss: 0.06630337238311768\n",
            "tensor([[ -1.5616],\n",
            "        [  1.9077],\n",
            "        [  6.0404],\n",
            "        [-15.0100],\n",
            "        [ 11.5055]], grad_fn=<AddBackward0>)\n",
            "Epoch 960/1000, Loss: 0.0662475973367691\n",
            "tensor([[ -1.5625],\n",
            "        [  1.9086],\n",
            "        [  6.0413],\n",
            "        [-15.0164],\n",
            "        [ 11.5107]], grad_fn=<AddBackward0>)\n",
            "Epoch 961/1000, Loss: 0.06619186699390411\n",
            "tensor([[ -1.5635],\n",
            "        [  1.9095],\n",
            "        [  6.0423],\n",
            "        [-15.0229],\n",
            "        [ 11.5159]], grad_fn=<AddBackward0>)\n",
            "Epoch 962/1000, Loss: 0.06613630056381226\n",
            "tensor([[ -1.5644],\n",
            "        [  1.9104],\n",
            "        [  6.0432],\n",
            "        [-15.0293],\n",
            "        [ 11.5211]], grad_fn=<AddBackward0>)\n",
            "Epoch 963/1000, Loss: 0.0660807341337204\n",
            "tensor([[ -1.5654],\n",
            "        [  1.9112],\n",
            "        [  6.0442],\n",
            "        [-15.0358],\n",
            "        [ 11.5262]], grad_fn=<AddBackward0>)\n",
            "Epoch 964/1000, Loss: 0.06602535396814346\n",
            "tensor([[ -1.5663],\n",
            "        [  1.9121],\n",
            "        [  6.0451],\n",
            "        [-15.0422],\n",
            "        [ 11.5314]], grad_fn=<AddBackward0>)\n",
            "Epoch 965/1000, Loss: 0.06597007811069489\n",
            "tensor([[ -1.5672],\n",
            "        [  1.9130],\n",
            "        [  6.0461],\n",
            "        [-15.0486],\n",
            "        [ 11.5365]], grad_fn=<AddBackward0>)\n",
            "Epoch 966/1000, Loss: 0.0659148246049881\n",
            "tensor([[ -1.5682],\n",
            "        [  1.9138],\n",
            "        [  6.0470],\n",
            "        [-15.0550],\n",
            "        [ 11.5417]], grad_fn=<AddBackward0>)\n",
            "Epoch 967/1000, Loss: 0.0658596009016037\n",
            "tensor([[ -1.5691],\n",
            "        [  1.9147],\n",
            "        [  6.0480],\n",
            "        [-15.0614],\n",
            "        [ 11.5469]], grad_fn=<AddBackward0>)\n",
            "Epoch 968/1000, Loss: 0.06580456346273422\n",
            "tensor([[ -1.5700],\n",
            "        [  1.9156],\n",
            "        [  6.0489],\n",
            "        [-15.0678],\n",
            "        [ 11.5520]], grad_fn=<AddBackward0>)\n",
            "Epoch 969/1000, Loss: 0.06574955582618713\n",
            "tensor([[ -1.5710],\n",
            "        [  1.9165],\n",
            "        [  6.0499],\n",
            "        [-15.0742],\n",
            "        [ 11.5572]], grad_fn=<AddBackward0>)\n",
            "Epoch 970/1000, Loss: 0.06569462269544601\n",
            "tensor([[ -1.5719],\n",
            "        [  1.9174],\n",
            "        [  6.0509],\n",
            "        [-15.0806],\n",
            "        [ 11.5623]], grad_fn=<AddBackward0>)\n",
            "Epoch 971/1000, Loss: 0.06563975661993027\n",
            "tensor([[ -1.5728],\n",
            "        [  1.9182],\n",
            "        [  6.0518],\n",
            "        [-15.0870],\n",
            "        [ 11.5674]], grad_fn=<AddBackward0>)\n",
            "Epoch 972/1000, Loss: 0.06558506190776825\n",
            "tensor([[ -1.5737],\n",
            "        [  1.9191],\n",
            "        [  6.0528],\n",
            "        [-15.0934],\n",
            "        [ 11.5726]], grad_fn=<AddBackward0>)\n",
            "Epoch 973/1000, Loss: 0.06553041934967041\n",
            "tensor([[ -1.5747],\n",
            "        [  1.9200],\n",
            "        [  6.0537],\n",
            "        [-15.0997],\n",
            "        [ 11.5777]], grad_fn=<AddBackward0>)\n",
            "Epoch 974/1000, Loss: 0.06547581404447556\n",
            "tensor([[ -1.5756],\n",
            "        [  1.9209],\n",
            "        [  6.0547],\n",
            "        [-15.1061],\n",
            "        [ 11.5828]], grad_fn=<AddBackward0>)\n",
            "Epoch 975/1000, Loss: 0.06542129814624786\n",
            "tensor([[ -1.5765],\n",
            "        [  1.9217],\n",
            "        [  6.0556],\n",
            "        [-15.1125],\n",
            "        [ 11.5880]], grad_fn=<AddBackward0>)\n",
            "Epoch 976/1000, Loss: 0.0653669685125351\n",
            "tensor([[ -1.5774],\n",
            "        [  1.9226],\n",
            "        [  6.0566],\n",
            "        [-15.1188],\n",
            "        [ 11.5931]], grad_fn=<AddBackward0>)\n",
            "Epoch 977/1000, Loss: 0.06531260162591934\n",
            "tensor([[ -1.5784],\n",
            "        [  1.9235],\n",
            "        [  6.0575],\n",
            "        [-15.1252],\n",
            "        [ 11.5982]], grad_fn=<AddBackward0>)\n",
            "Epoch 978/1000, Loss: 0.06525833904743195\n",
            "tensor([[ -1.5793],\n",
            "        [  1.9243],\n",
            "        [  6.0585],\n",
            "        [-15.1315],\n",
            "        [ 11.6033]], grad_fn=<AddBackward0>)\n",
            "Epoch 979/1000, Loss: 0.0652041882276535\n",
            "tensor([[ -1.5802],\n",
            "        [  1.9252],\n",
            "        [  6.0594],\n",
            "        [-15.1379],\n",
            "        [ 11.6084]], grad_fn=<AddBackward0>)\n",
            "Epoch 980/1000, Loss: 0.06515014916658401\n",
            "tensor([[ -1.5811],\n",
            "        [  1.9261],\n",
            "        [  6.0604],\n",
            "        [-15.1442],\n",
            "        [ 11.6135]], grad_fn=<AddBackward0>)\n",
            "Epoch 981/1000, Loss: 0.06509615480899811\n",
            "tensor([[ -1.5820],\n",
            "        [  1.9270],\n",
            "        [  6.0613],\n",
            "        [-15.1505],\n",
            "        [ 11.6186]], grad_fn=<AddBackward0>)\n",
            "Epoch 982/1000, Loss: 0.06504227966070175\n",
            "tensor([[ -1.5830],\n",
            "        [  1.9278],\n",
            "        [  6.0623],\n",
            "        [-15.1569],\n",
            "        [ 11.6237]], grad_fn=<AddBackward0>)\n",
            "Epoch 983/1000, Loss: 0.0649884045124054\n",
            "tensor([[ -1.5839],\n",
            "        [  1.9287],\n",
            "        [  6.0632],\n",
            "        [-15.1632],\n",
            "        [ 11.6288]], grad_fn=<AddBackward0>)\n",
            "Epoch 984/1000, Loss: 0.0649346187710762\n",
            "tensor([[ -1.5848],\n",
            "        [  1.9296],\n",
            "        [  6.0641],\n",
            "        [-15.1695],\n",
            "        [ 11.6339]], grad_fn=<AddBackward0>)\n",
            "Epoch 985/1000, Loss: 0.06488101184368134\n",
            "tensor([[ -1.5857],\n",
            "        [  1.9304],\n",
            "        [  6.0651],\n",
            "        [-15.1758],\n",
            "        [ 11.6390]], grad_fn=<AddBackward0>)\n",
            "Epoch 986/1000, Loss: 0.06482741236686707\n",
            "tensor([[ -1.5866],\n",
            "        [  1.9313],\n",
            "        [  6.0660],\n",
            "        [-15.1821],\n",
            "        [ 11.6441]], grad_fn=<AddBackward0>)\n",
            "Epoch 987/1000, Loss: 0.06477387249469757\n",
            "tensor([[ -1.5875],\n",
            "        [  1.9322],\n",
            "        [  6.0670],\n",
            "        [-15.1884],\n",
            "        [ 11.6492]], grad_fn=<AddBackward0>)\n",
            "Epoch 988/1000, Loss: 0.06472055613994598\n",
            "tensor([[ -1.5884],\n",
            "        [  1.9330],\n",
            "        [  6.0679],\n",
            "        [-15.1947],\n",
            "        [ 11.6542]], grad_fn=<AddBackward0>)\n",
            "Epoch 989/1000, Loss: 0.06466714292764664\n",
            "tensor([[ -1.5894],\n",
            "        [  1.9339],\n",
            "        [  6.0689],\n",
            "        [-15.2010],\n",
            "        [ 11.6593]], grad_fn=<AddBackward0>)\n",
            "Epoch 990/1000, Loss: 0.0646139532327652\n",
            "tensor([[ -1.5903],\n",
            "        [  1.9347],\n",
            "        [  6.0698],\n",
            "        [-15.2073],\n",
            "        [ 11.6644]], grad_fn=<AddBackward0>)\n",
            "Epoch 991/1000, Loss: 0.06456077098846436\n",
            "tensor([[ -1.5912],\n",
            "        [  1.9356],\n",
            "        [  6.0707],\n",
            "        [-15.2136],\n",
            "        [ 11.6694]], grad_fn=<AddBackward0>)\n",
            "Epoch 992/1000, Loss: 0.06450769305229187\n",
            "tensor([[ -1.5921],\n",
            "        [  1.9365],\n",
            "        [  6.0717],\n",
            "        [-15.2199],\n",
            "        [ 11.6745]], grad_fn=<AddBackward0>)\n",
            "Epoch 993/1000, Loss: 0.064454585313797\n",
            "tensor([[ -1.5930],\n",
            "        [  1.9373],\n",
            "        [  6.0726],\n",
            "        [-15.2261],\n",
            "        [ 11.6795]], grad_fn=<AddBackward0>)\n",
            "Epoch 994/1000, Loss: 0.06440167129039764\n",
            "tensor([[ -1.5939],\n",
            "        [  1.9382],\n",
            "        [  6.0735],\n",
            "        [-15.2324],\n",
            "        [ 11.6846]], grad_fn=<AddBackward0>)\n",
            "Epoch 995/1000, Loss: 0.06434886157512665\n",
            "tensor([[ -1.5948],\n",
            "        [  1.9391],\n",
            "        [  6.0745],\n",
            "        [-15.2387],\n",
            "        [ 11.6896]], grad_fn=<AddBackward0>)\n",
            "Epoch 996/1000, Loss: 0.06429608166217804\n",
            "tensor([[ -1.5957],\n",
            "        [  1.9399],\n",
            "        [  6.0754],\n",
            "        [-15.2449],\n",
            "        [ 11.6947]], grad_fn=<AddBackward0>)\n",
            "Epoch 997/1000, Loss: 0.06424342095851898\n",
            "tensor([[ -1.5966],\n",
            "        [  1.9408],\n",
            "        [  6.0763],\n",
            "        [-15.2512],\n",
            "        [ 11.6997]], grad_fn=<AddBackward0>)\n",
            "Epoch 998/1000, Loss: 0.06419075280427933\n",
            "tensor([[ -1.5976],\n",
            "        [  1.9416],\n",
            "        [  6.0773],\n",
            "        [-15.2574],\n",
            "        [ 11.7048]], grad_fn=<AddBackward0>)\n",
            "Epoch 999/1000, Loss: 0.064138263463974\n",
            "tensor([[ -1.5985],\n",
            "        [  1.9425],\n",
            "        [  6.0782],\n",
            "        [-15.2637],\n",
            "        [ 11.7098]], grad_fn=<AddBackward0>)\n",
            "Epoch 1000/1000, Loss: 0.06408573687076569\n",
            "tensor([[ -1.5994],\n",
            "        [  1.9434],\n",
            "        [  6.0791],\n",
            "        [-15.2699],\n",
            "        [ 11.7148]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_tensor = torch.tensor(inputs)\n",
        "targets_tensor = torch.tensor(targets)\n",
        "\n",
        "# Створення TensorDataset з тензорів\n",
        "train_ds = TensorDataset(inputs_tensor, targets_tensor)"
      ],
      "metadata": {
        "id": "Kp7Wqt9tVRL_"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))\n",
        ""
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd21efe-aef4-48c2-a9e8-27d222edf128"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 87., 134.,  58.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.]]),\n",
              " tensor([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class LogReg(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(3, 1)\n",
        "    self.act = nn.Sigmoid()\n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    x = self.act(x)\n",
        "    return x\n",
        "\n",
        "model = LogReg()\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b028cd9b-0f01-498e-9cf1-acb1a6589b0f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogReg(\n",
            "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (act): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as funcy\n",
        "\n",
        "opt = torch.optim.SGD(model.parameters(), 1e-3)\n",
        "loss_fn = funcy.binary_cross_entropy\n"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses\n",
        "\n",
        "\n",
        "losses = fit_return_loss(1000, model, loss_fn, opt, train_dl)\n",
        "\n",
        "predictions = model(inputs_tensor)\n",
        "print(predictions)\n",
        "print(targets_tensor)"
      ],
      "metadata": {
        "id": "cEHQH9qE626k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec1c34f-9b5d-457b-f8f8-add18a0cfae1"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 0.6718\n",
            "Epoch [20/1000], Loss: 0.1076\n",
            "Epoch [30/1000], Loss: 0.5005\n",
            "Epoch [40/1000], Loss: 0.1168\n",
            "Epoch [50/1000], Loss: 0.1553\n",
            "Epoch [60/1000], Loss: 0.1301\n",
            "Epoch [70/1000], Loss: 0.2753\n",
            "Epoch [80/1000], Loss: 0.1384\n",
            "Epoch [90/1000], Loss: 0.6124\n",
            "Epoch [100/1000], Loss: 0.0689\n",
            "Epoch [110/1000], Loss: 0.0753\n",
            "Epoch [120/1000], Loss: 0.0860\n",
            "Epoch [130/1000], Loss: 0.1180\n",
            "Epoch [140/1000], Loss: 0.0702\n",
            "Epoch [150/1000], Loss: 0.1893\n",
            "Epoch [160/1000], Loss: 0.0568\n",
            "Epoch [170/1000], Loss: 0.0637\n",
            "Epoch [180/1000], Loss: 0.0675\n",
            "Epoch [190/1000], Loss: 0.0551\n",
            "Epoch [200/1000], Loss: 0.0680\n",
            "Epoch [210/1000], Loss: 0.0668\n",
            "Epoch [220/1000], Loss: 0.0510\n",
            "Epoch [230/1000], Loss: 0.0477\n",
            "Epoch [240/1000], Loss: 0.0575\n",
            "Epoch [250/1000], Loss: 0.0568\n",
            "Epoch [260/1000], Loss: 0.0543\n",
            "Epoch [270/1000], Loss: 0.0466\n",
            "Epoch [280/1000], Loss: 0.0549\n",
            "Epoch [290/1000], Loss: 0.0413\n",
            "Epoch [300/1000], Loss: 0.0441\n",
            "Epoch [310/1000], Loss: 0.0526\n",
            "Epoch [320/1000], Loss: 0.0435\n",
            "Epoch [330/1000], Loss: 0.0404\n",
            "Epoch [340/1000], Loss: 0.0559\n",
            "Epoch [350/1000], Loss: 0.0460\n",
            "Epoch [360/1000], Loss: 0.0451\n",
            "Epoch [370/1000], Loss: 0.0382\n",
            "Epoch [380/1000], Loss: 0.0411\n",
            "Epoch [390/1000], Loss: 0.0399\n",
            "Epoch [400/1000], Loss: 0.0358\n",
            "Epoch [410/1000], Loss: 0.0346\n",
            "Epoch [420/1000], Loss: 0.0395\n",
            "Epoch [430/1000], Loss: 0.0342\n",
            "Epoch [440/1000], Loss: 0.0380\n",
            "Epoch [450/1000], Loss: 0.0322\n",
            "Epoch [460/1000], Loss: 0.0353\n",
            "Epoch [470/1000], Loss: 0.0311\n",
            "Epoch [480/1000], Loss: 0.0341\n",
            "Epoch [490/1000], Loss: 0.0312\n",
            "Epoch [500/1000], Loss: 0.0303\n",
            "Epoch [510/1000], Loss: 0.0306\n",
            "Epoch [520/1000], Loss: 0.0286\n",
            "Epoch [530/1000], Loss: 0.0295\n",
            "Epoch [540/1000], Loss: 0.0279\n",
            "Epoch [550/1000], Loss: 0.0373\n",
            "Epoch [560/1000], Loss: 0.0294\n",
            "Epoch [570/1000], Loss: 0.0306\n",
            "Epoch [580/1000], Loss: 0.0277\n",
            "Epoch [590/1000], Loss: 0.0280\n",
            "Epoch [600/1000], Loss: 0.0271\n",
            "Epoch [610/1000], Loss: 0.0283\n",
            "Epoch [620/1000], Loss: 0.0262\n",
            "Epoch [630/1000], Loss: 0.0258\n",
            "Epoch [640/1000], Loss: 0.0256\n",
            "Epoch [650/1000], Loss: 0.0251\n",
            "Epoch [660/1000], Loss: 0.0260\n",
            "Epoch [670/1000], Loss: 0.0239\n",
            "Epoch [680/1000], Loss: 0.0244\n",
            "Epoch [690/1000], Loss: 0.0244\n",
            "Epoch [700/1000], Loss: 0.0238\n",
            "Epoch [710/1000], Loss: 0.0233\n",
            "Epoch [720/1000], Loss: 0.0255\n",
            "Epoch [730/1000], Loss: 0.0230\n",
            "Epoch [740/1000], Loss: 0.0232\n",
            "Epoch [750/1000], Loss: 0.0240\n",
            "Epoch [760/1000], Loss: 0.0235\n",
            "Epoch [770/1000], Loss: 0.0220\n",
            "Epoch [780/1000], Loss: 0.0220\n",
            "Epoch [790/1000], Loss: 0.0236\n",
            "Epoch [800/1000], Loss: 0.0218\n",
            "Epoch [810/1000], Loss: 0.0213\n",
            "Epoch [820/1000], Loss: 0.0227\n",
            "Epoch [830/1000], Loss: 0.0208\n",
            "Epoch [840/1000], Loss: 0.0206\n",
            "Epoch [850/1000], Loss: 0.0221\n",
            "Epoch [860/1000], Loss: 0.0211\n",
            "Epoch [870/1000], Loss: 0.0202\n",
            "Epoch [880/1000], Loss: 0.0208\n",
            "Epoch [890/1000], Loss: 0.0199\n",
            "Epoch [900/1000], Loss: 0.0207\n",
            "Epoch [910/1000], Loss: 0.0195\n",
            "Epoch [920/1000], Loss: 0.0227\n",
            "Epoch [930/1000], Loss: 0.0206\n",
            "Epoch [940/1000], Loss: 0.0200\n",
            "Epoch [950/1000], Loss: 0.0188\n",
            "Epoch [960/1000], Loss: 0.0186\n",
            "Epoch [970/1000], Loss: 0.0186\n",
            "Epoch [980/1000], Loss: 0.0179\n",
            "Epoch [990/1000], Loss: 0.0184\n",
            "Epoch [1000/1000], Loss: 0.0180\n",
            "tensor([[5.1988e-02],\n",
            "        [9.6576e-01],\n",
            "        [1.0000e+00],\n",
            "        [3.5491e-15],\n",
            "        [1.0000e+00],\n",
            "        [5.1988e-02],\n",
            "        [9.6576e-01],\n",
            "        [1.0000e+00],\n",
            "        [3.5491e-15],\n",
            "        [1.0000e+00],\n",
            "        [5.1988e-02],\n",
            "        [9.6576e-01],\n",
            "        [1.0000e+00],\n",
            "        [3.5491e-15],\n",
            "        [1.0000e+00]], grad_fn=<SigmoidBackward0>)\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Графік зміни втрат\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(losses, label='Значення втрат під час навчання', color='b')\n",
        "plt.xlabel('Ітерації')\n",
        "plt.ylabel('Значення втрат (loss)')\n",
        "plt.title('Зміна значень втрат під час навчання моделі')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "b5336emnWkcP",
        "outputId": "5b9c8bbc-5488-4e9d-c24e-8dd997db0b87"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ВИсновок\n",
        "\n",
        "Модель успішно навчилася:\n",
        "\n",
        "Швидке зменшення втрат: Значення втрат різко зменшуються на початкових етапах навчання, що свідчить про те, що модель швидко адаптується до даних і покращує свою точність.\n",
        "\n",
        "Стабілізація втрат: Після приблизно 200 ітерацій значення втрат стає стабільним і близьким до нуля. Це означає, що модель досягла мінімальних значень втрат і більше не покращується значно — ознака того, що модель добре підходить до тренувальних даних."
      ],
      "metadata": {
        "id": "3c_yApRHbxTu"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnidetsanna/python_for_ML_tasks/blob/main/%D0%92%D0%B8%D0%BA%D0%BE%D1%80%D0%B8%D1%81%D1%82%D0%B0%D0%BD%D0%BD%D1%8F_%D0%BF%D1%80%D0%BE%D0%BC%D0%BF%D1%82%D1%96%D0%B2_%D1%96_%D0%B0%D0%B3%D0%B5%D0%BD%D1%82%D1%96%D0%B2_%D0%B2_Langchain_Hnidets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### –ó–∞–≤–¥–∞–Ω–Ω—è 1: –í–∏–∫–ª–∏–∫ LLM –∑ –±–∞–∑–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º\n",
        "\n",
        "–°—Ç–≤–æ—Ä—ñ—Ç—å –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤–∏–∫–ª–∏–∫–∞—Ç–∏ LLM –∑—ñ –∑–≤–∏—á–∞–π–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º.\n",
        "\n",
        "–ü—Ä–æ–º–ø—Ç –º–∞—î –¥–æ–∑–≤–ª—è—Ç–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ—Å—Ç–æ—é –º–æ–≤–æ—é –Ω–∞ –ø–µ–≤–Ω—É —Ç–µ–º—É. –í —Ü—å–æ–º—É –∑–∞–≤–¥–∞–Ω–Ω—ñ –º–∏ —Ö–æ—á–µ–º–æ –¥—ñ–∑–Ω–∞—Ç–∏—Å—å –ø—Ä–æ —Ç–µ–º—É \"–ö–≤–∞–Ω—Ç–æ–≤—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è\".\n",
        "\n",
        "–í—ñ–¥–ø–æ–≤—ñ–¥—å –º–æ–¥–µ–ª—ñ –ø–æ–≤–∏–Ω–Ω–∞ –º—ñ—Å—Ç–∏—Ç–∏ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, –∫–ª—é—á–æ–≤—ñ –ø–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –ø–æ—Ç–æ—á–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤ —Ü—ñ–π –≥–∞–ª—É–∑—ñ.\n",
        "\n",
        "–û–±–º–µ–∂—Ç–µ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –¥–æ 200 —Å–∏–º–≤–æ–ª—ñ–≤ —ñ –ø—Ä–æ–ø–∏—à—ñ—Ç—å –≤ –ø—Ä–æ–º–ø—Ç—ñ –∞–±–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±—É–ª–∞ –∫–æ—Ä–æ—Ç–∫–æ—é (—Ü–µ –∑–µ–∫–æ–Ω–æ–º–∏—Ç—å –í–∞–º —á–∞—Å —ñ –≥—Ä–æ—à—ñ –Ω–∞ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ —Ç–æ–∫–µ–Ω–∏).\n",
        "\n",
        "–í —è–∫–æ—Å—Ç—ñ LLM –º–æ–∂–Ω–∞ —Å–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏—Å—å —è–∫ –º–æ–¥–µ–ª–ª—é –∑ HugginFace (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é Mistral), —Ç–∞–∫ —ñ ChatGPT4 –∞–±–æ ChatGPT3. –í –æ–±–æ—Ö –≤–∏–ø–∞–¥–∫–∞—Ö —Ç—Ä–µ–±–∞ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –ø–æ—Ç—Ä—ñ–±–Ω—É \"–æ–±–≥–æ—Ä—Ç–∫—É\" (—Ç–æ–±—Ç–æ –∫–ª–∞—Å, —è–∫–∏–π –¥–æ–∑–≤–æ–ª–∏—Ç—å —ñ–Ω—ñ—Ü—ñ—é–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å) –∑ LangChain –¥–ª—è –≤–∏–∫–ª–∏–∫—É LLM –∑–∞ API, –∞ —Ç–∞–∫–æ–∂ –∑—á–∏—Ç–∞—Ç–∏ –æ—Å–æ–±–∏—Å—Ç–∏–π —Ç–æ–∫–µ–Ω –∑ —Ñ–∞–π–ª–∞, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, `creds.json`, —è–∫–∏–π —Ä–æ–∑–º—ñ—â–µ–Ω–∏–π —É –í–∞—Å –ª–æ–∫–∞–ª—å–Ω–æ —ñ –í–∏ –ù–ï –∑–¥–∞—î—Ç–µ –π–æ–≥–æ –≤ –î–ó —ñ –ù–ï –∫–æ–º—ñ—Ç–∏—Ç–µ –≤ git üòè\n",
        "\n",
        "–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —Å–≤–æ—î –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏ –Ω–∞ —Å–≤—ñ–π —Ä–æ–∑—Å—É–¥ (—Ç—É—Ç –Ω–µ–º–∞—î –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —á–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è) —ñ –Ω–∞–ø–∏—à—ñ—Ç—å, —á–æ–º—É –≤–∏ –æ–±—Ä–∞–ª–∏ —Å–∞–º–µ —Ç–∞–∫–µ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è —Ü—å–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è.  \n",
        "\n",
        "–ó–∞–ø–∏—Ç–∏ –º–æ–∂–Ω–∞ —Ä–æ–±–∏—Ç–∏ —è–∫ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é, —Ç–∞–∫ —ñ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é - –æ—Ä—ñ—î–Ω—Ç—É–π—Ç–µ—Å—å –Ω–∞ —Ç–µ, –¥–µ —ñ —á–∏ —Ö–æ—á–µ—Ç–µ –≤–∏ –ø–æ—Ç—ñ–º –ª–∏—à–∏—Ç–∏ —Ü–µ–π –ø—Ä–æ—î–∫—Ç —ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∞ —è–∫–∞ –º–æ–≤–∞ –±—É–¥–µ –ø–∞—Å—É–≤–∞—Ç–∏ –±—ñ–ª—å—à–µ. –í —Ä–æ–∑–≤ º—è–∑–∫–∞—Ö –ø—Ä–æ–º–ø—Ç–∏ - —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é."
      ],
      "metadata": {
        "id": "RMalK_oYR-X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip -q install langchain langchain_openai huggingface_hub openai"
      ],
      "metadata": {
        "id": "vT1ICe74cAUS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "!pip install transformers\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clIF7SOdv26p",
        "outputId": "45ece7c6-64d7-4260-8fd6-ef2251a29d8d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-huggingface\n",
        "\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "P571ZnBT8U9k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ],
      "metadata": {
        "id": "nM5762Qx-UU0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mistral = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    temperature=0.4,\n",
        "    token = \"HF_TOKEN\",\n",
        "    max_new_tokens=200\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wSZ2ia98ptm",
        "outputId": "adf2c692-cf8d-4565-db41-b5e3279e28a4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! token is not default parameter.\n",
            "                    token was transferred to model_kwargs.\n",
            "                    Please make sure that token is what you intended.\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "request = \"What is the definition, key benefits and current research in the field of Quantum Computing? Make a short answer\"\n",
        "print(mistral.invoke(request))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6y1Tdzk8ys4",
        "outputId": "175838a6-aa23-4ced-99dc-a5eadc155239"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " for each of the following questions:\n",
            "\n",
            "1. What is Quantum Computing?\n",
            "Quantum Computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform calculations. Unlike classical computers that use bits to store and process information, quantum computers use quantum bits, or qubits, which can exist in multiple states at once, allowing for parallel processing and exponential speedup for certain algorithms.\n",
            "2. What are the key benefits of Quantum Computing?\n",
            "The key benefits of Quantum Computing include the potential for exponential speedup for certain algorithms, such as Shor's algorithm for factorization and Grover's algorithm for search. Quantum Computing also has the ability to solve problems that are intractable for classical computers, such as optimization problems and simulating quantum systems. Additionally, Quantum Computing has the potential to revolutionize industries, such as finance, materials science,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –ó–∞–≤–¥–∞–Ω–Ω—è 2: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É\n",
        "–¢–µ–ø–µ—Ä –º–∏ —Ö–æ—á–µ–º–æ –æ–Ω–æ–≤–∏—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Ñ—É–∫–Ω—Ü—ñ–æ–Ω–∞–ª —Ç–∞–∫, –∞–±–∏ –≤ –ø—Ä–æ–º–ø—Ç –º–∏ –º–æ–≥–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç–∏ —Ç–µ–º—É —è–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä. –î–ª—è —Ü—å–æ–≥–æ —Å–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ—Å—å `PromptTemplate` –∑ `langchain` —ñ —Ä–µ–∞–ª—ñ–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–∏–π –ø—Ä–æ–º–ø—Ç —Ç–∞ –≤–∏–∫–ª–∏–∫ –º–æ–¥–µ–ª—ñ –∑ –Ω–∏–º.\n",
        "\n",
        "–ó–∞–ø—É—Å—Ç—ñ—Ç—å –æ–Ω–æ–≤–ª–µ–Ω–∏–π —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª (–ø—Ä–æ–º–ø—Ç + –º–æ–¥–µ–ª) –¥–ª—è –ø–æ—è—Å–Ω–µ–Ω—å –ø—Ä–æ —Ç–µ–º–∏\n",
        "- \"–ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"\n",
        "- \"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"\n",
        "- \"Explainable AI\"\n",
        "\n",
        "–í–∏–≤–µ–¥—ñ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤—ñ–¥–ø—Ä–∞—Ü—é–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –µ–∫—Ä–∞–Ω."
      ],
      "metadata": {
        "id": "UiIzV0UIS0GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"Provide a definition, key benefits, and current research in the field of {topic}.\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
        "\n",
        "\n",
        "def generate_response(topic):\n",
        "    prompt = prompt_template.format(topic=topic)\n",
        "    response = mistral.invoke(prompt)\n",
        "    return response\n",
        "\n",
        "\n",
        "topics = [\n",
        "    \"–ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\",\n",
        "    \"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\",\n",
        "    \"Explainable AI\"\n",
        "]\n",
        "\n",
        "for topic in topics:\n",
        "    print(f\"–¢–µ–º–∞: {topic}\")\n",
        "    print(\"–í—ñ–¥–ø–æ–≤—ñ–¥—å:\", generate_response(topic))\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "kcg7EfdNcBDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2cd123-3161-43bc-9f45-79d5c6b6d138"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–¢–µ–º–∞: –ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\n",
            "–í—ñ–¥–ø–æ–≤—ñ–¥—å: \n",
            "\n",
            "Definition:\n",
            "Bayesian methods in machine learning refer to a class of statistical models and algorithms that apply Bayes' theorem to make probabilistic predictions or decisions based on data. Bayes' theorem is a fundamental concept in probability theory that describes how to update beliefs or probabilities based on new evidence or data. In machine learning, Bayesian methods are used to build probabilistic models that can reason about uncertainty and make predictions or decisions under uncertainty.\n",
            "\n",
            "Key Benefits:\n",
            "1. Handling Uncertainty: Bayesian methods are particularly useful in handling uncertainty, which is a common challenge in machine learning. By modeling the underlying probability distributions, Bayesian methods can provide probabilistic predictions or decisions, which can be more informative and robust than deterministic approaches.\n",
            "2. Flexibility: Bayesian methods are flexible and can be applied to a wide range of machine learning tasks, including regression, classification, clustering, and time series analysis.\n",
            "\n",
            "==================================================\n",
            "\n",
            "–¢–µ–º–∞: –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\n",
            "–í—ñ–¥–ø–æ–≤—ñ–¥—å: \n",
            "\n",
            "Transformers in Machine Learning: Definition and Key Benefits\n",
            "\n",
            "Transformers are a type of neural network architecture introduced by Vaswani et al. in 2017 in the paper \"Attention is All You Need.\" This architecture is designed for sequence-to-sequence modeling tasks, such as machine translation, text summarization, and speech recognition. The key innovation of transformers is the self-attention mechanism, which allows the model to focus on different parts of the input sequence when generating an output sequence.\n",
            "\n",
            "Self-attention mechanism:\n",
            "The attention mechanism in transformers allows the model to weight the importance of different input elements when generating an output element. It computes a weighted sum of input vectors based on their relevance to the current output element. The relevance is measured by a compatibility score, which is computed as the dot product of the input and query vectors, followed by a softmax function to ensure that the scores sum to one\n",
            "\n",
            "==================================================\n",
            "\n",
            "–¢–µ–º–∞: Explainable AI\n",
            "–í—ñ–¥–ø–æ–≤—ñ–¥—å: \n",
            "\n",
            "Definition:\n",
            "Explainable AI (XAI) or Explainable Machine Learning (EML) refers to the development of artificial intelligence (AI) and machine learning (ML) systems that can provide clear and understandable explanations for their decision-making processes. The goal of XAI is to create AI models that can communicate their reasoning and justification for the predictions or actions they make, thereby increasing transparency, trust, and accountability.\n",
            "\n",
            "Key Benefits:\n",
            "1. Increased Transparency: XAI can help organizations and individuals understand how AI systems make decisions, which is crucial for building trust and confidence in these technologies.\n",
            "2. Improved Explainability: XAI can help identify the factors that contribute to an AI model's predictions or actions, making it easier to diagnose and correct errors or biases.\n",
            "3. Enhanced Human-AI Collaboration: XAI can enable humans and AI systems to work together more effectively\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### –ó–∞–≤–¥–∞–Ω–Ω—è 3: –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –ø—Ä–æ—Ü–µ—Å—ñ–≤\n",
        "–°—Ç–≤–æ—Ä—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –¥–æ–ø–æ–º–æ–∂–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —à—É–∫–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –æ—Å—Ç–∞–Ω–Ω—ñ –Ω–∞—É–∫–æ–≤—ñ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –≤ —Ä—ñ–∑–Ω–∏—Ö –≥–∞–ª—É–∑—è—Ö. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –∞–≥–µ–Ω—Ç –º–∞—î –∑–Ω–∞–π—Ç–∏ 5 –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π –Ω–∞ —Ç–µ–º—É —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.\n",
        "\n",
        "**–ö—Ä–æ–∫–∏:**\n",
        "1. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ –∞–≥–µ–Ω—Ç–∞ —Ç–∏–ø—É ReAct –≤ LangChain –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤.\n",
        "2. –°—Ç–≤–æ—Ä—ñ—Ç—å –ø—Ä–æ–º–ø—Ç, —è–∫–∏–π —Å–ø—Ä—è–º–æ–≤—É—î –∞–≥–µ–Ω—Ç–∞ —à—É–∫–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—ñ –∞–±–æ –≤ –±–∞–∑–∞—Ö –¥–∞–Ω–∏—Ö –Ω–∞—É–∫–æ–≤–∏—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π.\n",
        "3. –ê–≥–µ–Ω—Ç –ø–æ–≤–∏–Ω–µ–Ω –≤–∏–¥–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π, –∫–æ–∂–Ω–∞ –∑ —è–∫–∏—Ö –º—ñ—Å—Ç–∏—Ç—å –Ω–∞–∑–≤—É, –∞–≤—Ç–æ—Ä—ñ–≤ —ñ –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å.\n",
        "\n",
        "–î–ª—è –≤–∑–∞—î–º–æ–¥—ñ—ó –∑ –ø–æ—à—É–∫–æ–º —Ç–∞–º –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ `Tool`. –í –ª–µ–∫—Ü—ñ—ó –º–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∏ `serpapi`. –ú–æ–∂–Ω–∞ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ç–∏—Å—å –Ω–∏–º, –∞–±–æ –æ–±—Ä–∞—Ç–∏ —ñ–Ω—à–µ –ê–†–Ü –¥–ª—è –ø–æ—à—É–∫—É (–≤–æ–Ω–∏ –≤ —Ç–æ–º—É —á–∏—Å–ª—ñ —î –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω—ñ). –ü–µ—Ä–µ–ª—ñ–∫ —Ä—ñ–∑–Ω–∏—Ö –ê–†–Ü, –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –≤ langchain, —ñ –æ—Ä—ñ—î–Ω—Ç–∏—Ä –ø–æ –≤–∞—Ä—Ç–æ—Å—Ç—ñ –∑–∞–ø–∏—Ç—ñ–≤ –º–æ–∂–Ω–∞ –∑–Ω–∞–π—Ç–∏ –≤ –æ–∫—Ä–µ–º–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—ñ [—Ç—É—Ç](https://hannapylieva.notion.site/API-12994835849480a69b2adf2b8441cbb3?pvs=4).\n",
        "\n",
        "–õ–∏—à–∞—é —Ç–∞–∫–æ–∂ –Ω–∏–∂—á–µ –ø—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –æ–¥–Ω–æ–≥–æ –∑ –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∏—Ö –ø–æ—à—É–∫–æ–≤–∏—Ö –ê–†–Ü - DuckDuckGo (–Ω–µ –ø–æ—Ç—Ä–µ–±—É—î —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω–∞!)  - –º–æ–∂–ª–∏–≤–æ –≤—ñ–Ω –≤–∞–º —Å–ø–æ–¥–æ–±–∞—î—Ç—å—Å—è :)\n"
      ],
      "metadata": {
        "id": "m9UsL2gXSe-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community duckduckgo_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzj4PHP9XwH-",
        "outputId": "f7ac31bf-0b91-454a-de45-eb40b5fd0d9a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain import PromptTemplate\n",
        "from langchain.agents import Tool, AgentExecutor, create_react_agent\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"\"\n",
        "\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    temperature=0.4,\n",
        "    max_new_tokens=200\n",
        ")\n",
        "\n",
        "\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "search_tool = Tool(\n",
        "    name=\"DuckDuckGoSearch\",\n",
        "    description=\"A search tool for answering questions about current events or information available on the internet.\",\n",
        "    func=search.run\n",
        ")\n",
        "\n",
        "\n",
        "tools = [search_tool]\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(template=\"\"\"\n",
        "Answer the following question as best as you can.\n",
        "If the question refers to scientific or social issues, try to find that information online.\n",
        "Return the answer as a list of publications with title, authors, and a short description that you found.\n",
        "You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: think about what action to take\n",
        "Action: the action to take, choose from [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\n",
        "\"\"\", input_variables=[\"input\", \"agent_scratchpad\", \"tools\", \"tool_names\"])\n",
        "\n",
        "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
        "\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "\n",
        "question = \"Is there a limit to the size of a transistor?\"\n",
        "response = agent_executor.invoke({\"input\": question})\n",
        "\n",
        "\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p56AZ_SnXvTs",
        "outputId": "da97e77e-0fec-40a0-b5e2-06c9eed9fcd7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer this question, I need to find out if there is a fundamental limit to the size of a transistor in terms of physical dimensions or performance. This information can be found in scientific literature or research articles.\n",
            "Action: DuckDuckGoSearch\n",
            "Action Input: \"limit to transistor size\" \"scientific literature\"\n",
            "Observation\u001b[0m\u001b[36;1m\u001b[1;3mNo good DuckDuckGo Search Result was found\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mI could not find a definitive answer from my initial search, so I will try to find more specific information by looking up recent research in the field of nanotechnology and transistor physics.\n",
            "Action: DuckDuckGoSearch\n",
            "Action Input: \"recent research on transistor size limit\" \"nanotechnology\" \"physics\"\n",
            "Observation\u001b[0m\u001b[36;1m\u001b[1;3mNo good DuckDuckGo Search Result was found\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mI will try to find more specific information by looking up recent research articles directly from scientific databases.\n",
            "Action: I will use the Crossref API to search for articles related to the topic of transistor size limits.\n",
            "Action Input: \"transistor size limit\"\n",
            "Observation\u001b[0mI will use the Crossref API to search for articles related to the topic of transistor size limits. is not a valid tool, try one of [DuckDuckGoSearch]."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mI cannot use the Crossref API directly, I will have to use a search engine like Google Scholar to find the articles and then use the Crossref API to get more information about them.\n",
            "Action: DuckDuckGoSearch\n",
            "Action Input: \"transistor size limit\" \"google scholar\"\n",
            "Observation\u001b[0m\u001b[36;1m\u001b[1;3mArticle ADS CAS Google Scholar Liu, Y. et al. Approaching the Schottky-Mott limit in van der Waals metal-semiconductor junctions. Nature 557 , 696-700 (2018).\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mThis article seems to be relevant to the topic of transistor size limits. I will use the Crossref API to get more information about the article.\n",
            "Action: CrossrefAPI\n",
            "Action Input: \"doi: 10.1038/s41586-018-0303-z\"\n",
            "Observation\u001b[0mCrossrefAPI is not a valid tool, try one of [DuckDuckGoSearch].\u001b[32;1m\u001b[1;3mI cannot use the Crossref API directly, I will have to manually search for the article on the publisher's website or use a library database to access the full text.\n",
            "Action: DuckDuckGoSearch\n",
            "Action Input: \"transistor size limit\" \"ADS CAS Liu et al. Approaching the Schottky-Mott limit in van der Waals metal-semiconductor junctions\"\n",
            "Observation\u001b[0m\u001b[36;1m\u001b[1;3mNo good DuckDuckGo Search Result was found\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mI will try to find more information by looking up other recent research articles related to transistor size limits and nanotechnology.\n",
            "Action: DuckDuckGoSearch\n",
            "Action Input: \"transistor size limit\" \"nanotechnology\" \"recent research\"\n",
            "Observation\u001b[0m\u001b[36;1m\u001b[1;3mNo good DuckDuckGo Search Result was found\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mI have searched extensively for information on the topic of transistor size limits, but I could not find a definitive answer. The current state of research suggests that there are physical limits to the size of transistors, but the exact size and the methods to overcome these limits are still under investigation.\n",
            "Final Answer: The size of transistors is a topic of ongoing research in the field of nanotechnology and physics. While there are physical limits to the size of transistors, the exact size and the methods to overcome these limits are still under investigation.</s>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'Is there a limit to the size of a transistor?', 'output': 'The size of transistors is a topic of ongoing research in the field of nanotechnology and physics. While there are physical limits to the size of transistors, the exact size and the methods to overcome these limits are still under investigation.</s>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### –ó–∞–≤–¥–∞–Ω–Ω—è 4: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–≥–µ–Ω—Ç–∞-–ø–æ–º—ñ—á–Ω–∏–∫–∞ –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –±—ñ–∑–Ω–µ—Å-–∑–∞–¥–∞—á\n",
        "\n",
        "–°—Ç–≤–æ—Ä—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –¥–æ–ø–æ–º–∞–≥–∞—î –≤–∏—Ä—ñ—à—É–≤–∞—Ç–∏ –∑–∞–¥–∞—á—ñ –±—ñ–∑–Ω–µ—Å-–∞–Ω–∞–ª—ñ—Ç–∏–∫–∏. –ê–≥–µ–Ω—Ç –º–∞—î –¥–æ–ø–æ–º–æ–≥—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ä—ñ–∫ –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ —Ä—ñ–≤–µ–Ω—å —ñ–Ω—Ñ–ª—è—Ü—ñ—ó —ñ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏. –ê–≥–µ–Ω—Ç –º–∞—î –≤–º—ñ—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Python —ñ —Ö–æ–¥–∏—Ç–∏ –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç –∞–±–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ.\n",
        "\n",
        "**–ö—Ä–æ–∫–∏:**\n",
        "1. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –ø—Ä–∞—Ü—é–≤–∞—Ç–∏–º–µ –∑ –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏, –∑–∞–¥–∞–Ω–∏–º–∏ —Ç–µ–∫—Å—Ç–æ–º. –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø–∏—à–µ\n",
        "\n",
        "```\n",
        "–ú–∏ –µ–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –∑ –ë—Ä–∞–∑–∏–ª—ñ—ó. –í 2021 –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞–ª–∏ 200—Ç, –≤ 2022 - 190—Ç, –≤ 2023 - 210—Ç, –≤ 2024 —è–∫–∏–π —â–µ –Ω–µ –∑–∞–∫—ñ–Ω—á–∏–≤—Å—è - 220—Ç. –ó—Ä–æ–±–∏ –æ—Ü—ñ–Ω–∫—É —Å–∫—ñ–ª—å–∫–∏ –º–∏ –∑–º–æ–∂–µ–º–æ –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –∞–ø–µ–ª—å—Å–∏–Ω—ñ–≤ –≤ 2025 –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏ –≤ –ë—Ä–∞–∑–∏–ª—ñ—ó —ñ –ø–æ–ø–∏—Ç –Ω–∞ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –≤ —Å–≤—ñ—Ç—ñ –≤–∏—Ö–æ–¥—è—á–∏ –∑ –µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ—ó —Å–∏—Ç—É–∞—Ü—ñ—ó.\n",
        "```\n",
        "\n",
        "2. –°—Ç–≤–æ—Ä—ñ—Ç—å –∑–∞–ø–∏—Ç –¥–æ –∞–≥–µ–Ω—Ç–∞, —â–æ –º—ñ—Å—Ç–∏—Ç—å —á—ñ—Ç–∫–µ –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äì –≤–∏–¥–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—ñ–∑–Ω–µ—Å –∞–Ω–∞–ª—ñ–∑—É –∞–±–æ –Ω–∞–ø–∏—Å–∞—Ç–∏, —â–æ –≤—ñ–Ω –Ω–µ –º–æ–∂–µ —Ü—å–æ–≥–æ –∑—Ä–æ–±–∏—Ç–∏ —ñ –∑–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (–ø—Ä–æ—Å—Ç–æ –º–æ–∂–µ –±—É—Ç–∏ –≤—Å–µ –æ–¥–Ω–∏–º –ø–æ–≤—ñ–¥–æ–º–ª–ª–µ–Ω–Ω—è–º).\n",
        "\n",
        "3. –ó–∞–ø—É—Å—Ç—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞ —ñ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏. –©–æ –º–æ–∂–Ω–∞ –ø–æ–∫—Ä–∞—â–∏—Ç–∏?\n"
      ],
      "metadata": {
        "id": "IOqujC6qY_NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain_experimental"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3_AiUs0Bz_4",
        "outputId": "2ce5d7b1-82d1-4959-aa0a-a4da71678971"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/209.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.6/209.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "UK6KA9PyRS0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4742a996-1cb1-41df-dadf-c680c8b4a411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI cannot directly answer the question as we do not have information about the weather conditions, inflation, and the current economic situation in Brazil that would affect the orange exports in 2025.\n",
            "\n",
            "Action: DuckDuckGoSearch\n",
            "Action Input:\n",
            "weather conditions, inflation, and current economical situation in Brazil for the year 2025\n",
            "\n",
            "Observation\u001b[0m\u001b[33;1m\u001b[1;3mBrazil's government forecast economic growth of 2.6% and inflation of 3.3% for 2025 in its draft budget proposal for the coming year, which was submitted to Congress late Friday. Forecast inflation rate for 2023 ... Key Economic Indicators. Brazil: monthly inflation rate 2016-2024. ... Basic Statistic Perception of economic situation in Brazil compared to last year 2024 Brazil's annual inflation is forecast to have reached a one-year high in October on steeper energy and meat prices caused by a severe drought, while a host of risks continue to muddy the outlook ... 1:33. Brazil's government raised its 2024 growth and inflation forecasts days ahead of a key central bank meeting. The economy is now expected to grow 3.2% this year, the Finance Ministry said ... (IMF) concluded the Article IV consultation1 with Brazil. Over the past two years, Brazil's economy has shown remarkable resilience as inflation has declined to within the target tolerance interval. Real GDP growth in 2023, at 2.9 percent, was significantly higher than projected at the start of the year, on the back of record agricultural\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n",
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mBased on the information obtained from the search, the inflation rate for 2025 is forecasted to be 3.3% and the economic growth rate is forecasted to be 2.6%. However, there is no specific information about the weather conditions or the orange production in Brazil for the year 2025.\n",
            "\n",
            "Action: python_repl\n",
            "Action Input: import math; import json; import requests; import datetime;\n",
            "              current_year = int(datetime.datetime.now().strftime(\"%Y\"));\n",
            "              url = \"https://api.openweathermap.org/data/2.5/weather?q=Brazil&appid=<your_api_key>&units=metric\";\n",
            "              weather_data = json.loads(requests.get(url).text);\n",
            "              temperature = weather_data[\"main\"][\"temp\"];\n",
            "              print(f\"Current temperature in Brazil is\u001b[0m\u001b[38;5;200m\u001b[1;3mIndentationError('unexpected indent', ('<string>', 2, 14, '              current_year = int(datetime.datetime.now().strftime(\"%Y\"));\\n', 2, -1))\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m              It seems that the python_repl command did not execute properly. I will try to use DuckDuckGoSearch again to find the current weather conditions in Brazil.\n",
            "\n",
            "Action: DuckDuckGoSearch\n",
            "Action Input: current weather conditions in Brazil\n",
            "\n",
            "Observation\u001b[0m\u001b[33;1m\u001b[1;3mTeresina. Add. Vitoria. Add. Back to top. This global web site presents OFFICIAL weather forecasts and climatological information for selected cities supplied by National Meteorological & Hydrological Services worldwide. 23 ¬∞ 16 ¬∞. Sunny intervals changing to partly cloudy by early evening. Most likely maximum day time temperature 23 degrees Celsius and highest possible is 25 degrees, Most likely and lowest possible maximum night time temperatures 16 degrees Celsius. Beta ‚Ä¢ Let us know what you think. As of 7 a.m. CDT, Major Hurricane Milton was located near latitude 22.5 N, longitude 88.8 W, or about 100 miles northeast of Progreso, Mexico, and 545 miles southwest of Tampa, Fla. Milton is traveling east-northeast at 9 mph, with maximum sustained winds of 145 mph, making it a Category 4 hurricane on the Saffir-Simpson Hurricane Wind Scale. S√£o Paulo ‚òÄ Weather forecast for 10 days, information from meteorological stations, webcams, sunrise and sunset, wind and precipitation maps for this place ... Brazil / Lat.: 23¬∞33'S / Lon.: 46¬∞38'W / Altitude: 779 m Timezone: America/Sao_Paulo (UTC-3) / Current time: 03:50 2024/11/13 . Current Weather ; Forecast ; Sun and Moon ; 17 ¬∞C ... Hourly weather forecast and temperature - Bras√≠lia, Brazil. Today, in, rainy weather is forecasted. Rainfall is anticipated for the rest of the day. The temperature will vary between a minimum of 20¬∞C (68¬∞F) and a maximum of 20¬∞C (68¬∞F). The maximum temperature will be comparable to November 's average highest temperature of 26.3¬∞C (79.3¬∞F).\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mBased on the information obtained from the search, the current temperature in Brazil is 17¬∞C. However, this information is not sufficient to determine the weather conditions for the entire year 2025.\n",
            "\n",
            "Action: None\n",
            "Thought: I cannot directly answer the question as we do not have enough information to determine the orange exports in 2025 based on weather conditions, inflation, and the current economic situation in Brazil.\n",
            "\n",
            "Final Answer: I cannot determine the number of oranges that can be exported from Brazil in 2025 based on the available information.</s>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': '\\n                        We are exporting bananas from Brazil.\\n                        In 2021 we exported 200t, in 2022 - 190t, in 2023 - 210t, in 2024 which is not finished yet - 220t.\\n                        How many oranges we can export in 2025 based on weather conditions, inflation and current economical situation in Brazil.\\n                       ',\n",
              " 'output': 'I cannot determine the number of oranges that can be exported from Brazil in 2025 based on the available information.</s>'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain import hub\n",
        "from langchain.agents import Tool, AgentExecutor, AgentType, create_react_agent, initialize_agent, load_tools\n",
        "from langchain.chains import LLMMathChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"\"\n",
        "\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    temperature=0.4,\n",
        "    max_new_tokens=200\n",
        ")\n",
        "\n",
        "\n",
        "search = DuckDuckGoSearchRun()\n",
        "python_repl = PythonREPL()\n",
        "\n",
        "search_tool = Tool(\n",
        "    name=\"DuckDuckGoSearch\",\n",
        "    description=\"A search Tool, useful for when you need to answer questions about current events, or other infromation that can be easely retrived from internet.\",\n",
        "    func=search.run\n",
        ")\n",
        "python_tool = Tool(\n",
        "    name=\"python_repl\",\n",
        "    description=\"\"\"\n",
        "        A Python shell.\n",
        "        Can be usefull to get current date, etc.\n",
        "        Use this to execute python commands.\n",
        "        Input should be a valid python command.\n",
        "        If you want to see the output of a value, you necessarily should print it out with `print(...)`.\n",
        "        Otherwise you won't see the result! It's very important.\n",
        "      \"\"\",\n",
        "    func=python_repl.run,\n",
        ")\n",
        "\n",
        "\n",
        "tools = load_tools([\"llm-math\"], llm=llm)\n",
        "\n",
        "tools.append(search_tool)\n",
        "tools.append(python_tool)\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(template=\"\"\"\n",
        "Answer the following question as best you can.\n",
        "The question will be buissiness realater, so the answer needs to be precise and reasonable.\n",
        "Use all avaliable data: weather, level of inflation, country GDP etc. to make the answer better.\n",
        "Before doing anything realted to date, check the today date with tools that you have.\n",
        "You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\n",
        "\"\"\", input_variables=[\"input\", \"agent_scratchpad\", \"tools\", \"tool_names\"],)\n",
        "\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "agent_executor.invoke({\"input\":\n",
        "                       \"\"\"\n",
        "                        We are exporting bananas from Brazil.\n",
        "                        In 2021 we exported 200t, in 2022 - 190t, in 2023 - 210t, in 2024 which is not finished yet - 220t.\n",
        "                        How many oranges we can export in 2025 based on weather conditions, inflation and current economical situation in Brazil.\n",
        "                       \"\"\"\n",
        "                      })"
      ]
    }
  ]
}